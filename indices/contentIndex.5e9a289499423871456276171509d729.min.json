{"/":{"title":"Digital Garden ü™¥","content":"\n# Welcome in my Digital Garden ü™¥\nI'm happy to welcome you to my garden!\n\nI love learning, testing and discovering new things, related to the world of data that is dear to me, or not. But it was quite frustrating because, over time, things would slip out of my mind replaced by other discoveries, not lost forever, but much harder to retrieve and share!\n\nAnd then I read Jacky's [Networked Thought](https://jzhao.xyz/posts/networked-thought) page sharing his [Quartz](https://github.com/jackyzha0/quartz) project, it gave me the tool I'd been missing, being able to document, create my garden full of seeds to grow and share with anyone interested üåª.\n\n\n## My garden ü•ï\nI'm just at the beginning of my garden, the land is still uncultivated, and I don't know where I will put the vegetable patch, how I will organize my plots, which trees and flowers I will be able to plant, and how they will grow! So, we will start by drawing inspiration from the neighbors.\n\n### #Seeds\n\nI tend to generally bookmark things for later then revisit them when I have time. For [ideas list](thoughts/bag%20of%20seeds.md), writing, and all sorts of reading. Even when reading books, I don‚Äôt like to take complex notes right away will only bookmark or highlight phrases. I will eventually come back to the bookmarks a second time to generate insights and actual thoughts. It feels like this weeds out unnecessary noise and provides a natural chance for spaced repetition.\n\nThese are the seeds that form the basis of ideas and thoughts.\n\n### #Saplings\n\nSaplings are single nodes or thoughts.\n\n### #Fruits\n\nOf course, a knowledge index isn‚Äôt much use if it doesn‚Äôt inform future thinking and output. Fruits are derivative or ‚Äônew‚Äô pieces of content.\n\nIt‚Äôs the act of creating ‚Äônewer‚Äô work from saplings, mostly longer form #posts, [projects](thoughts/projects.md), etc. At this stage, thoughts and ideas have matured enough to be able to share and collaborate.\n\n\n## Let's take a digital coffee ‚òï\n\n\u003e [!info]\n\u003e Feel free to join our [Slack channel](https://join.slack.com/t/konpytika/shared_invite/zt-14md072lv-Jr8mqYoeUrqzfZF~YGUpXA) for a chat ü§ó\n","lastmodified":"2023-08-03T11:22:06.675912611Z","tags":null},"/private/doctolib/code/.gitkeep":{"title":"","content":"","lastmodified":"2023-08-03T11:22:06.675912611Z","tags":null},"/private/doctolib/meets/.gitkeep":{"title":"","content":"","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/private/doctolib/scoping/.gitkeep":{"title":"","content":"","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/private/doctolib/term/.gitkeep":{"title":"","content":"","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/thoughts/Apache-NiFi":{"title":"Apache NiFi","content":"","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/thoughts/Kubernetes":{"title":"Kubernetes","content":"","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/thoughts/Kubernetes-CRD":{"title":"Kubernetes Custom Resource Definition","content":"","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/thoughts/Kubernetes-network-policies":{"title":"Network policies","content":"\n[Kubernetes](thoughts/Kubernetes.md) Network policies are an application-centric construct which allow you to specify how a pod is allowed to communicate with various network ‚Äúentities‚Äù (endpoints, services) over the network.\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: test-network-policy\n  namespace: default\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n    - Ingress\n    - Egress\n  ingress:\n    - from:\n        - ipBlock:\n            cidr: 172.17.0.0/16\n            except:\n              - 172.17.1.0/24\n        - namespaceSelector:\n            matchLabels:\n              project: myproject\n        - podSelector:\n            matchLabels:\n              role: frontend\n      ports:\n        - protocol: TCP\n          port: 6379\n  egress:\n    - to:\n        - ipBlock:\n            cidr: 10.0.0.0/24\n      ports:\n        - protocol: TCP\n          port: 5978\n\n```\n\n## Links\n- [Network Policies | Kubernetes](https://kubernetes.io/docs/concepts/services-networking/network-policies/)\n","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/thoughts/NiFiKop":{"title":"NiFiKop","content":"\nIt is is an open-source [Kubernetes operator](thoughts/kubernetes%20operator.md) that makes it¬†easy¬†to run [Apache NiFi](thoughts/Apache%20NiFi.md) on [Kubernetes](thoughts/Kubernetes.md).","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/thoughts/aws-vpc":{"title":"AWS VPC","content":"\nWith Amazon Virtual Private Cloud (Amazon VPC), you can launch AWS resources in a logically isolated virtual network that you've defined. This virtual network closely resembles a traditional network that you'd operate in your own data center, with the benefits of using the scalable infrastructure of AWS.\n\nThe following diagram shows an example VPC. The VPC has one subnet in each of the Availability Zones in the Region, EC2 instances in each subnet, and an internet gateway to allow communication between the resources in your VPC and the internet.\n\n![](thoughts/images/aws_vpc.png)","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/thoughts/aws-vpc-cni":{"title":"AWS VPC CNI","content":"[Container Network Inteface Plugin](thoughts/container%20network%20inteface%20plugin.md)¬†using [elastic network interfaces](thoughts/elastic%20network%20interfaces.md)¬†on AWS.\n#aws\n\n## Overlay network\nThe first one is the overlay network used for communication and encapsulating the packets over the network. In this case, the encapsulating is taking place at the hardware level using #aws [ENI](thoughts/elastic%20network%20interfaces.md) inside a [VPC](thoughts/aws%20vpc.md).\n\n![](thoughts/images/pod_networking_amazon_vpc_cni.png)\n\nThe plugin support two modes when it comes to manage IP addresses management for pods:\n\n- [Prefix mode](https://aws.github.io/aws-eks-best-practices/networking/prefix-mode/index_linux/ \"https://aws.github.io/aws-eks-best-practices/networking/prefix-mode/index_linux/\"): allows [Kubernetes](thoughts/Kubernetes.md) Pods to have the same IP address as they do on the [VPC](thoughts/aws%20vpc.md) network. More specifically, all containers inside the Pod share a network namespace, and they can communicate with each-other using local ports.\n- [Secondary IP mode](https://aws.github.io/aws-eks-best-practices/networking/vpc-cni/#overview \"https://aws.github.io/aws-eks-best-practices/networking/vpc-cni/#overview\"): when a worker node is provisioned, it has a default [ENI](thoughts/elastic%20network%20interfaces.md), called the primary [ENI](thoughts/elastic%20network%20interfaces.md), attached to it. The [CNI](thoughts/container%20network%20inteface%20plugin.md) allocates a warm pool of [ENIs](thoughts/elastic%20network%20interfaces.md) and secondary IP addresses from the subnet attached to the node‚Äôs primary [ENI](thoughts/elastic%20network%20interfaces.md). By default, IPAMD (a long-running node-local IP Address Management daemon and is responsible for) attempts to allocate an additional [ENI](thoughts/elastic%20network%20interfaces.md) to the node. The IPAMD allocates additional [ENI](thoughts/elastic%20network%20interfaces.md) when a single Pod is scheduled and assigned a secondary IP address from the primary [ENI](thoughts/elastic%20network%20interfaces.md). This \"warm\" [ENI](thoughts/elastic%20network%20interfaces.md) enables faster Pod networking. As the pool of secondary IP addresses runs out, the [CNI](thoughts/container%20network%20inteface%20plugin.md) adds another [ENI](thoughts/elastic%20network%20interfaces.md) to assign more.\n\n## Flow traffic control\nIn #aws a [Security Group (SG)](thoughts/security%20group.md) acts as a virtual firewall for EC2 instances to control inbound and outbound traffic. By default the Amazon [VPC](thoughts/aws%20vpc.md) CNI will use [SGs](thoughts/security%20group.md) associated with the primary[ENI](thoughts/elastic%20network%20interfaces.md)  on the node. Which means that every Pod on a node shares the same security groups as the node it runs on.\n![](thoughts/images/aws_vpc_cni_node_security_group.png)\nWith [Security Group for Pods](https://aws.github.io/aws-eks-best-practices/networking/sgpp/ \"https://aws.github.io/aws-eks-best-practices/networking/sgpp/\"), multiple types of security rules, such as Pod-to-Pod and Pod-to-External AWS services, can be defined in a single place with EC2 security groups and applied to workloads with Kubernetes native APIs.\n\n![](thoughts/images/aws_security_group_for_pods.png)\nTo enable the Security Groups for pod we have to set `ENABLE_POD_ENI = true` for VPC CNI. Once done, for each node in the cluster the add-on adds a label with the value `vpc.amazonaws.com/has-trunk-attached=true`. The [VPC Resource Controller](https://github.com/aws/amazon-vpc-resource-controller-k8s \"https://github.com/aws/amazon-vpc-resource-controller-k8s\") running on the control plane creates and attaches a trunk interface called ‚Äúaws-k8s-trunk-eni‚Äù to the node. It acts as a standard network interface attached to the instance.\n\nThe controller also creates branch interfaces named \"aws-k8s-branch-eni\" and associates them with the trunk interface. Pods are assigned a security group using the [SecurityGroupPolicy](https://github.com/aws/amazon-vpc-resource-controller-k8s/blob/master/config/crd/bases/vpcresources.k8s.aws_securitygrouppolicies.yaml \"https://github.com/aws/amazon-vpc-resource-controller-k8s/blob/master/config/crd/bases/vpcresources.k8s.aws_securitygrouppolicies.yaml\") custom resource and are associated with a branch interface. Since security groups are specified with network interfaces, we are now able to [schedule Pods requiring specific security groups](https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html#sg-pods-example-deployment \"https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html#sg-pods-example-deployment\") on these additional network interfaces.\n\n![](thoughts/images/aws_security_group_for_pods_branch_eni.png)\n\n\u003e [!warning] \n\u003e Branch interface capacity is _additive_ to existing instance type limits for secondary IP addresses. Pods that use security groups are not accounted for in the max-pods formula and when you use security group for pods you need to consider [raising the max-pods](https://docs.aws.amazon.com/eks/latest/userguide/cni-increase-ip-addresses.html \"https://docs.aws.amazon.com/eks/latest/userguide/cni-increase-ip-addresses.html\") value or be ok with running fewer pods than the node can actually support.\n\n#### Recommandation\n\nSome highlights on recommandations using this feature:\n\n\u003e[!info]   [Disable TCP Early Demux for Liveness Probe](https://aws.github.io/aws-eks-best-practices/networking/sgpp/#disable-tcp-early-demux-for-liveness-probe \"https://aws.github.io/aws-eks-best-practices/networking/sgpp/#disable-tcp-early-demux-for-liveness-probe\")\n\u003e\n\u003eIf are you using liveness or readiness probes, you also need to disable TCP early demux, so that the kubelet can connect to Pods on branch network interfaces via TCP\n\n\u003e [!info] [Supporting Kubernetes Network Policy](https://aws.github.io/aws-eks-best-practices/networking/sgpp/#enforcing-mode-use-standard-mode-in-the-following-situations \"https://aws.github.io/aws-eks-best-practices/networking/sgpp/#enforcing-mode-use-standard-mode-in-the-following-situations\")\n\u003e \n\u003e We recommend using standard enforcing mode when using network policy with Pods that have associated security groups.\n\u003e\n\u003eWe strongly recommend to utilize security groups for Pods to limit network-level access to AWS services that are not part of a cluster. Consider [Kubernetes network policies](thoughts/Kubernetes%20network%20policies.md) to restrict network traffic between Pods inside a cluster, often known as East/West traffic.\n\n\u003e [!info] [Deploy pods with Security Groups to Private Subnets](https://aws.github.io/aws-eks-best-practices/networking/sgpp/#deploy-pods-with-security-groups-to-private-subnets \"https://aws.github.io/aws-eks-best-practices/networking/sgpp/#deploy-pods-with-security-groups-to-private-subnets\")\n\u003e \n\u003e Pods that are assigned security groups must be run on nodes that are deployed on to private subnets. Note that Pods with assigned security groups deployed to public subnets will not able to access the internet.\n\n## Setup\nWhen you create a new EKS cluster the Amazon VPC CNI is now installed by default as a managed add-on. But on our cluster they have been installed in self-managed mode.\n\nThere is two different approaches to install it:\n- [Update the self-managed add-on](https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html#vpc-add-on-self-managed-update \"https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html#vpc-add-on-self-managed-update\")\n- [Switch to the Amazon EKS add-on](https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html#vpc-add-on-create \"https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html#vpc-add-on-create\")\n\n|**Pros**|**Cons**|\n|---|---|\n|VPC CNI uses Native VPC networking, which results in high performance and is also highly scalable|Need to add another CNI plugin to manage network policies|\n|Support to configure POD security groups, which allows or denies access to POD both from external and internal|No solution to control traffic flow based on fqdn|\n|Support of secondary CIDR IP ranges, so that PODs can have more IPs from secondary subnet||\n\n## Links","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/thoughts/aws-vpc-cni-x-cilium":{"title":"Amazon VPC CNI x Cilium","content":"\nIn this hybrid mode, the [Amazon VPC CNI](thoughts/aws%20vpc%20cni.md) is responsible for setting up the virtual network devices as well as for IP address management (IPAM) via [ENIs](thoughts/elastic%20network%20interfaces.md). After the initial networking is setup for a given pod, the Cilium CNI plugin is called to attach [eBPF](thoughts/eBPF.md) programs to the network devices set up by the [Amazon VPC CNI](thoughts/aws%20vpc%20cni.md) plugin in order to enforce [Kubernetes network policies](thoughts/Kubernetes%20network%20policies.md), perform load-balancing and provide encryption.\n\nThe steps to setup CNI chaining with AWS CNI:\n![](thoughts/images/cilium_cni_chaining_amazon_vpc_cni.png)\n\n- [Setting up Cilium with helm](https://docs.cilium.io/en/stable/installation/cni-chaining-aws-cni/#setting-up-a-cluster-on-aws \"https://docs.cilium.io/en/stable/installation/cni-chaining-aws-cni/#setting-up-a-cluster-on-aws\")\n- [Restart existing pods](https://docs.cilium.io/en/stable/installation/cni-chaining-aws-cni/#restart-existing-pods \"https://docs.cilium.io/en/stable/installation/cni-chaining-aws-cni/#restart-existing-pods\")\n- [Validate the installation](https://docs.cilium.io/en/stable/installation/cni-chaining-aws-cni/#validate-the-installation \"https://docs.cilium.io/en/stable/installation/cni-chaining-aws-cni/#validate-the-installation\")\n- [Enabling security groups for pods](https://docs.cilium.io/en/stable/installation/cni-chaining-aws-cni/#validate-the-installation \"https://docs.cilium.io/en/stable/installation/cni-chaining-aws-cni/#validate-the-installation\")\n\n\u003e [!warning]\n\u003e \n\u003e Some advanced [Cilium](thoughts/cilium.md) features may be limited when chaining with other CNI plugins, such as:\n\u003e- [Layer 7 Policy](https://docs.cilium.io/en/stable/security/policy/language/#l7-policy \"https://docs.cilium.io/en/stable/security/policy/language/#l7-policy\") (see [GitHub issue 12454](https://github.com/cilium/cilium/issues/12454 \"https://github.com/cilium/cilium/issues/12454\"))\n\u003e- [IPsec Transparent Encryption](https://docs.cilium.io/en/stable/security/network/encryption-ipsec/#encryption-ipsec \"https://docs.cilium.io/en/stable/security/network/encryption-ipsec/#encryption-ipsec\") (see [GitHub issue 15596](https://github.com/cilium/cilium/issues/15596 \"https://github.com/cilium/cilium/issues/15596\"))\n\n","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/thoughts/bag-of-seeds":{"title":"Bag of seeds üå±","content":"\n","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/thoughts/cilium":{"title":"Cilium","content":"\n[Cilium](https://cilium.io/get-started/ \"https://cilium.io/get-started/\") is an open source project to provide networking, security and observability for [Kubernetes](thoughts/Kubernetes.md) cluster an other container orchestration platforms. At the foundation of Cilium is a new Linux kernel technology called [eBPF](thoughts/eBPF.md), which enabled the dynamic insertion of powerful security, visibility, and networking control logic into the Linux kernel.\n\n## Global architecture\n![](thoughts/images/cilium_global_architecture.png)\n\n### Cilium\n- **Agent:** runs on each node in the cluster. It accepts configuration via [Kubernetes](thoughts/Kubernetes.md) or APIs that describes networking, service load-balancing, [Kubernetes network policies](thoughts/Kubernetes%20network%20policies.md), and visibility \u0026 monitoring requirements. It listens for events from orchestration systems such as [Kubernetes](thoughts/Kubernetes.md) to learn when containers or workloads are started and stopped. It manages the [eBPF](thoughts/eBPF.md) programs which the Linux kernel uses to control all network access in / out of those containers.\n- **Operator:** is responsible for managing duties in the cluster which should logically be handled once for the entire cluster, rather than once for each node in the cluster. The Cilium [kubernetes operator](thoughts/kubernetes%20operator.md) is not in the critical path for any forwarding or network policy decision.\n- **[CNI plugin](thoughts/container%20network%20inteface%20plugin.md):** is invoked by Kubernetes when a pod is scheduled or terminated on a node. It interacts with the Cilium API of the node to trigger the necessary datapath configuration to provide networking, load-balancing and [Kubernetes network policies](thoughts/Kubernetes%20network%20policies.md) for the pod.\n\n### Data Store\nCilium requires a data store to propagate state between agents. It supports the following data stores:\n\n- **Kubernetes CRDs (Default):** The default choice to store any data and propagate state is to use [Kubernetes Custom Resource Definition (CRD)](thoughts/Kubernetes%20CRD.md). CRDs are offered by  [Kubernetes](thoughts/Kubernetes.md) for cluster components to represent configurations and state via  [Kubernetes](thoughts/Kubernetes.md) resources.\n- **Key-Value Store** **(etcd):** A key-value store can optionally be used as an optimization to improve the scalability of a cluster as change notifications and storage requirements are more efficient with direct key-value store usage\n\n### Observability with Hubble\n- **Server:** runs on each node and retrieves the eBPF-based visibility from Cilium. It is embedded into the Cilium agent in order to achieve high performance and low-overhead. It offers a gRPC service to retrieve flows and Prometheus metrics.\n- **Relay:** standalone component which is aware of all running Hubble servers and offers cluster-wide visibility by connecting to their respective gRPC APIs and providing an API that represents all servers in the cluster.\n- **Graphical UI:** utilizes relay-based visibility to provide a graphical service dependency and connectivity map.\n\n## eBPF datapath: Linux Kernel\n[eBPF](thoughts/eBPF.md) is used to provide high-performance networking, multi-cluster and multi-cloud capabilities, advanced load balancing, transparent encryption, extensive network security capabilities, transparent observability, and much more.\n\nHere is an example how the life of a packet, with [eBPF](thoughts/eBPF.md).\n\n![](thoughts/images/cilium_ebpf_packer_life.png)\n- **Network endpoint (lxc)**: Defines in Kubernetes as [Cilium Endpoint](https://docs.cilium.io/en/stable/network/kubernetes/ciliumendpoint/ \"https://docs.cilium.io/en/stable/network/kubernetes/ciliumendpoint/\") CR. It defines the network interface couple (one in the pod, another one on the host) that link the pod network to the host network.\n\u003e [!note] \n\u003e\n\u003e Even if an endpoint is running, it doesn't mean it's ready. [Here](https://docs.cilium.io/en/stable/security/policy/lifecycle/ \"https://docs.cilium.io/en/stable/security/policy/lifecycle/\") is the graph states and their meanings.\n- **Barckley Packet Filter BPF**: a technology used in certain computer operating systems for programs that need to, among other things, analyze network traffic. It provides a raw interface to data link layers, permitting raw link-layer packets to be sent and received.\n\u003e [!note] \n\u003e \n\u003e I highly recommend anyone working with Cilium to watch this video: [Episode 51: Life of a Packet with Cilium](https://www.youtube.com/watch?v=0BKU6avwS98\u0026t=494s). It explains the basics of this scheme, and gives the commands and methodology for gaining visibility of the network flow, which is very useful for debugging.\n\n## Flow traffic control\nThe configuration of the Cilium agent and the Cilium [network policy](thoughts/Kubernetes%20network%20policies.md) determines whether an endpoint accepts traffic from a source or not. The agent can be put into the following three policy enforcement modes:\n- **default:** endpoints will start without any restrictions and as soon as a rule restricts their ability to receive traffic on ingress or to transmit traffic on egress, then the endpoint goes into whitelisting mode and all traffic must be explicitly allowed.\n- **always:** policy enforcement is enabled on all endpoints even if no rules select specific endpoints.\n- **never:** policy enforcement is disabled on all endpoints, even if rules do select specific endpoints.\n\n### Layer 3 policy\nCan be specified using the following methods:\n- [Labels Based](https://docs.cilium.io/en/stable/security/policy/language/#labels-based \"https://docs.cilium.io/en/stable/security/policy/language/#labels-based\"): used to describe the relationship if both endpoints are managed by Cilium and are thus assigned labels.\n- [Services based](https://docs.cilium.io/en/stable/security/policy/language/#services-based \"https://docs.cilium.io/en/stable/security/policy/language/#services-based\"): Intermediate form between Labels and CIDR and makes use of the services concept in the orchestration system.\n- [Entities Based](https://docs.cilium.io/en/stable/security/policy/language/#entities-based \"https://docs.cilium.io/en/stable/security/policy/language/#entities-based\"): Entities are used to describe remote peers which can be categorized without knowing their IP addresses.\n- [IP/CIDR based](https://docs.cilium.io/en/stable/security/policy/language/#cidr-based \"https://docs.cilium.io/en/stable/security/policy/language/#cidr-based\"): used to describe the relationship to or from external services if the remote peer is not an endpoint.\n- [DNS based](https://docs.cilium.io/en/stable/security/policy/language/#dns-based \"https://docs.cilium.io/en/stable/security/policy/language/#dns-based\"): Selects remote, non-cluster, peers using DNS names converted to IPs via DNS lookups. It shares all limitations of the [IP/CIDR based](https://docs.cilium.io/en/stable/security/policy/language/#cidr-based \"https://docs.cilium.io/en/stable/security/policy/language/#cidr-based\") rules above. DNS information is acquired by routing DNS traffic via a proxy, or polling for listed DNS targets. DNS TTLs are respected.\n\n### Layer 4 policy\nLayer 4 policy can be specified in addition to layer 3 policies or independently. It restricts the ability of an endpoint to emit and/or receive packets on a particular port using a particular protocol.\n\n### Layer 7 policy\nLayer 7 policy rules are embedded into Layer 4 rules and can be specified for ingress and egress.\n- **path:** extended POSIX regex matched against the path of a request\n- **method:** extended POSIX regex matched against the method of a request, e.g. `GET`, `POST`, `PUT`, `PATCH`, `DELETE`, ‚Ä¶\n- **host:** extended POSIX regex matched against the host header of a request, e.g. `foo.com`\n- **headers:** list of HTTP headers which must be present in the request\n\n\u003e [!warning]  \n\u003e \n\u003e DNS Proxy intercepts egress DNS traffic and records IPs seen in the responses. This interception is, itself, a [separate policy rule governing the DNS requests](https://docs.cilium.io/en/stable/security/policy/language/#dns-proxy \"https://docs.cilium.io/en/stable/security/policy/language/#dns-proxy\"), and must be specified separately\n\n\u003e [!note]\n\u003e \n\u003e Unlike layer 3 and layer 4 policies, violation of layer 7 rules does not result in packet drops. Instead, if possible, an application protocol specific access denied message is crafted and returned, e.g. an _HTTP 403 access denied_ is sent back for HTTP requests which violate the policy, or a _DNS REFUSED_ response for DNS requests.\n\n\u003e [!note]\n\u003e \n\u003e Layer 7 rules are not currently supported in [Host Policies](https://docs.cilium.io/en/stable/security/policy/language/#hostpolicies \"https://docs.cilium.io/en/stable/security/policy/language/#hostpolicies\"), i.e., policies that use [Node Selector](https://docs.cilium.io/en/stable/security/policy/intro/#nodeselector \"https://docs.cilium.io/en/stable/security/policy/intro/#nodeselector\").\n\n### Egress gateway\n\n[Egress Gateway ‚Äî Cilium 1.14.0 documentation](https://docs.cilium.io/en/stable/network/egress-gateway/\n\nEgress gateway feature routes all IPv4 connections originating from pods and destined to specific cluster-external CIDRs through particular nodes, from now on called ‚Äúgateway nodes‚Äù. When enabled, matching packets that leave the cluster are masqueraded with selected, predictable IPs associated with the gateway nodes.\n\n\u003e [!info]\n\u003e \n\u003e As an example, this feature can be used in combination with legacy firewalls to allow traffic to legacy infrastructure only from specific pods within a given namespace. The pods typically have ever-changing IP addresses, and even if masquerading was to be used as a way to mitigate this, the IP addresses of nodes can also change frequently over time.\n\n## Installation concerns\n\nThere is two installation modes for Cilium:\n\n- [Using the CLI](https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/ \"https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/\")\n- [Using an helm chart](https://docs.cilium.io/en/stable/installation/k8s-install-helm/ \"https://docs.cilium.io/en/stable/installation/k8s-install-helm/\")\n\nFor both configurations, the steps are as follows:\n- Patch managed node groups taints\n- Install cilium with helm chart\n- Restart unmanaged pods\n- Validate installation\n\nSome steps, such as patching the [aws vpc cni](thoughts/aws%20vpc%20cni.md), are not explicit in the CLI documentation and may be hidden, so we need to compare the two implementations to see if any differences exist and what they imply. Same for [ENI integration as a requirement or not](https://docs.cilium.io/en/stable/network/concepts/routing/#aws-eni \"https://docs.cilium.io/en/stable/network/concepts/routing/#aws-eni\").\n\nFor the Helm chart, this means that some steps are manual. We can check out a way to manage our own Helm chart that includes Cilium's and add a Job to perform these steps.\n\nFor the observability part with Hubble, this is the same as for Cilium, we can install with the [CLI or an Helm chart](https://docs.cilium.io/en/stable/gettingstarted/hubble_setup/ \"https://docs.cilium.io/en/stable/gettingstarted/hubble_setup/\"), with some configuration:\n- [TLS certificates](https://docs.cilium.io/en/stable/gettingstarted/hubble-configuration/#tls-certificates \"https://docs.cilium.io/en/stable/gettingstarted/hubble-configuration/#tls-certificates\") (with [cert-manager](https://docs.cilium.io/en/stable/gettingstarted/hubble-configuration/#auto-generated-certificates-via-cert-manager \"https://docs.cilium.io/en/stable/gettingstarted/hubble-configuration/#auto-generated-certificates-via-cert-manager\")? or [auto-generated via Helm](https://docs.cilium.io/en/stable/gettingstarted/hubble-configuration/#auto-generated-certificates-via-helm \"https://docs.cilium.io/en/stable/gettingstarted/hubble-configuration/#auto-generated-certificates-via-helm\"))\n- [Enable UI](https://docs.cilium.io/en/stable/gettingstarted/hubble/#enable-the-hubble-ui \"https://docs.cilium.io/en/stable/gettingstarted/hubble/#enable-the-hubble-ui\")\n\n### Specific Configurations\n#### Tunnel mode\n- **Overlay:** Encapsulation-based virtual network spanning all hosts. Currently [VXLAN](https://en.wikipedia.org/wiki/Virtual_Extensible_LAN \"https://en.wikipedia.org/wiki/Virtual_Extensible_LAN\") (default) and [Geneve](https://www.redhat.com/en/blog/what-geneve \"https://www.redhat.com/en/blog/what-geneve\") are baked in but all encapsulation formats supported by Linux can be enabled.\n- **Native routing:** (see integration with Amazon VPC CNI section)\n\n#### Bandwith manager\n[Bandwidth Manager ‚Äî Cilium 1.14.0 documentation](https://docs.cilium.io/en/stable/network/kubernetes/bandwidth-manager/)\n\n#### Kube-proxy replacement\n[eCHO Episode 53: Life of a Packet in Cilium Continued - YouTube](https://www.youtube.com/watch?v=SGfMEpjq07Q\u0026list=PLDg_GiBbAx-mY3VFLPbLHcxo6wUjejAOC\u0026index=47)\n[Kubernetes Without kube-proxy ‚Äî Cilium 1.9.18 documentation](https://docs.cilium.io/en/v1.9/gettingstarted/kubeproxy-free/#kube-proxy-hybrid-modes)\n\n|**Pros**|**Cons**|\n|---|---|\n|One of the most, if not the most advanced CNI plugin|Not integrated with Amazon VPC CNI as it ([pods with Security Group will not be impacted by the network policies](https://aws.github.io/aws-eks-best-practices/networking/sgpp/#enforcing-mode-use-standard-mode-in-the-following-situations \"https://aws.github.io/aws-eks-best-practices/networking/sgpp/#enforcing-mode-use-standard-mode-in-the-following-situations\") as they are on a branch ENI)|\n|Open source|Network skills will be needed to understand what's going on and to be able to debug.|\n|Manage toFQDN egress endpoints||\n|User friendly observability with Hubble||\n","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/thoughts/container-network-inteface-plugin":{"title":"Container Network Interface plugin","content":"\n[Kubernetes](thoughts/Kubernetes.md) does not implement such things as mulit-host networking (i.e. Pod-to-Pod communication across nodes). The Container Network Interface (CNI) is a framework for dynamically configuring network resources. \n\nThe plugin specification defines an interface for configuring network, provisioning IP addreses, and maintaining connectivity with multiple hosts:\n- Pods have their own IP addresses\n- Nodes manages pod subnets and allocates pod IPs locally\n- The communication/access from one pod to another pod is through CNI (L2 bridge)\n- The CNI plugin creates interface, used as interface between container runtime and network, and to configure network routes.\n- The interface communicates to another node through an overlay network ([VXLAN](https://en.wikipedia.org/wiki/Virtual_Extensible_LAN \"https://en.wikipedia.org/wiki/Virtual_Extensible_LAN\"), [BGP](https://en.wikipedia.org/wiki/Border_Gateway_Protocol \"https://en.wikipedia.org/wiki/Border_Gateway_Protocol\") etc.) \n\n![Pod networking and communication](thoughts/images/pod_networking_communication.png)\n\n\n## Links\n\n\n","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/thoughts/eBPF":{"title":"eBPF","content":"\nA Linux kernel bytecode interpreter originally introduced to filter network packets, e.g. tcpdump and socket filters. It has since been extended with additional data structures such as hashtable and arrays as well as additional actions to support packet mangling, forwarding, encapsulation, etc\n\n\n","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/thoughts/elastic-network-interfaces":{"title":"Elastic Network interfaces","content":"\nAn¬†_elastic network interface_¬†is a logical networking component in a [VPC](thoughts/aws%20vpc.md) that represents a virtual network card. It can include the following attributes:\n- A primary private IPv4 address from the IPv4 address range of your [VPC](thoughts/aws%20vpc.md)\n- One or more secondary private IPv4 addresses from the IPv4 address range of your VPC\n- One Elastic IP address (IPv4) per private IPv4 [VPC](thoughts/aws%20vpc.md)\n- One public IPv4 address\n- One or more IPv6 addresses\n- One or more security groups\n- A MAC address\n- A source/destination check flag\n- A description\n\nWe can create and configure network interfaces and attach them to instances in the same Availability Zone. The account might also have¬†_requester-managed_¬†network interfaces, which are created and managed by AWS services to enable us to use other resources and services. We cannot manage these network interfaces ourself.","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/thoughts/github-actions":{"title":"Github Actions","content":"[how to access private repositories from GA](thoughts/how%20to%20access%20private%20repositories%20from%20GA.md)","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/thoughts/how-to-access-private-repositories-from-GA":{"title":"How to access private repositories from Github Actions?","content":"\nWhen running in [Github Actions](thoughts/github%20actions.md), the private repositories can not be accessed since the runners is not configured to access the private repositories.\n\n## The solution\n\nSince the private repositories that my app depending on are mine as well, therefore I can add the required configuration into those private repositories with the action [ssh-agent](https://github.com/webfactory/ssh-agent)\n\nThe following steps are required to setup the SSH keys for the runner during runtime to access the private repositories:\n\n- Create SSH key for the private repositories by running¬† `ssh-keygen -C \"git@github.com:erdrix/digital-garden-git.git\"`¬†.\n- Put the public key as deployment key into the private repository.\n- Put the private key as a secret into the main repository.\n- Change your action a bit\n\n```yaml\n# Make sure the @v0.7.0 matches the current version of the action\n- uses: webfactory/ssh-agent@v0.7.0\n  with:\n    ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY_CONTENT }}\n\n- uses: actions/checkout@v3\n  with:\n    submodules: recursive # this is important not to forget\n```\n\n","lastmodified":"2023-08-03T11:22:06.679912774Z","tags":null},"/thoughts/kubernetes-operator":{"title":"Kubernetes Operator","content":"","lastmodified":"2023-08-03T11:22:06.683912936Z","tags":null},"/thoughts/osi-model-layers":{"title":"OSI model layers","content":"\nOSI stands for Open Systems Interconnection. It is a 7-layer architecture with each layer having specific functionality to perform. All these 7 layers work collaboratively to transmit the data from one perso to another across the globe.\n\n![OSI model layers](thoughts/images/osi_model_layers.png)\n\n- **\\[L3\\] Network Layer:** handles addressing, routing, and packet transmission between different networks. It assigns IP addresses, routes data packets, and ensures efficient communication across networks. _For Kubernetes it is translated by the fact that we assign unique IP addresses to pods and enables communication between them within a cluster.\n- **\\[L4\\] Transport Layer:** responsible for managing reliable and efficient communication between devices. It uses protocols like TCP for reliable and ordered data transfer, and UDP for fast, connectionless communication. It handles data segmentation, encapsulation, and ensures reliable delivery when needed. _For Kubernetes it manages communication between pods using protocols and ports to each pod.\n- **\\[L7\\] Application Layer:** focuses on application-level interactions. It handles application-specific protocols, performs tasks like protocol identification, message segmentation, encryption, routing, load balancing, caching, and content delivery. _For Kubernetes it provides service discovery, load balancing, routing, API management._\n\n## Links\n\n\n","lastmodified":"2023-08-03T11:22:06.683912936Z","tags":null},"/thoughts/projects":{"title":"Projects","content":"\n# NiFiKop - A Kubernetes operator for Apache NiFi\n[NiFiKop](thoughts/NiFiKop.md), is an open-source Kubernetes operator that makes it¬†easy¬†to run Apache NiFi on Kubernetes. \n\n[Github](https://github.com/konpyutaika/nifikop), [Documentation](https://konpyutaika.github.io/nifikop/)","lastmodified":"2023-08-03T11:22:06.683912936Z","tags":null},"/thoughts/security-group":{"title":"Security group","content":"\nA security group acts as a firewall that controls the traffic allowed to and from the resources in your [virtual private cloud (VPC)](thoughts/aws%20vpc.md). You can choose the ports and protocols to allow for inbound traffic and for outbound traffic.\n\nFor each security group, you add separate sets of rules for inbound traffic and outbound traffic.","lastmodified":"2023-08-03T11:22:06.683912936Z","tags":null}}