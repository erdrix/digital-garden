{"/":{"title":"Data Glossary üß†","content":"\n# A Single Place for All Data Knowledge\nWelcome to the Data Glossary, a one-stop-shop for data-related concepts. Inspired by the [Digital Garden](https://jzhao.xyz/posts/networked-thought/) analogy, this interactive platform offers a comprehensive collection of data terms, covering various topics. The [Data Glossary](term/about%20this%20glossary.md) aims to help you expand your data knowledge and uncover new insights. Happy learning!\n\n# Navigation\nThere are multiple ways to navigate my Second Brain:\n\n1. Use the search bar on the top right or press `cmd+k` (`ctrl+k` on Windows) or click on the Search button (top right) to search for any term.\n2. Click on a note to explore its content, and follow the links and backlinks to dive deeper into related topics.\n3. Interact with the graph at the bottom of the page to visualize connections between notes and click on any node to navigate directly to that note.\n4. Click on the [Hashtags](tags) to explore the topics by tags. \n\n## Map of Content\nThe Data Glossary is continuously growing, and while I have some essential¬†Map of Content¬†starting points listed below, there are many more topics to discover as you explore. Feel free to dive into any of the following areas:\n\n| Category              | Topics                                                                                        |\n|-----------------------|-----------------------------------------------------------------------------------------------|\n| Data Engineering      | [Data Engineering Concepts](term/data%20engineering%20concepts), [Data Engineering Guides](term/data%20engineering%20guides), [Data Engineering Lifecycle](term/data%20engineering%20lifecycle) |\n| Data Architectures          | [Data Warehouse](term/data%20warehouse), [Data Lake](term/data%20lake), [Data Lakehouse](term/data%20lakehouse) |\n| Data Processing       | [ELT](term/elt), [ETL](term/etl), [EtLT](term/etlt.md), [Reverse ETL](term/reverse%20etl), [Data Integration](term/data%20integration) |\n| Data Formats          | [Apache Avro](term/apache%20avro), [Apache Parquet](term/apache%20parquet), [Apache ORC](term/orc) |\n| Data Analysis, BI \u0026 ML    | [Analytics](term/analytics), [Business Intelligence](term/business%20intelligence), [Business Intelligence Tools](term/business%20intelligence%20tools.md), [Machine Learning](term/machine%20learning) |\n| Programming Languages | [Python](term/python), [Rust](term/rust), [SQL](term/sql) |\n| Programming| [Functional Programming](term/functional%20programming), [Functional Data Engineering](term/functional%20data%20engineering) |\n\n## Contribute\n\n\u003e [!info] How to Contribute?\n\u003e \n\u003e 1.  ‚≠ê Star our¬†[GitHub](https://github.com/airbytehq/glossary)¬†repo\n\u003e 3.  ‚úçÔ∏è¬†Missing a Term or want to fix a typo? [Contribute to Glossary](term/contribute%20to%20glossary.md) \n\u003e 4. üëÄ Want to discuss or need help, talk to us on [Slack](https://slack.airbyte.com)\n","lastmodified":"2023-07-25T13:01:03.597827001Z","tags":null},"/term/about-this-glossary":{"title":"About this Glossary","content":"The Airbyte Glossary is built on top of the [Digital Garden](https://jzhao.xyz/posts/networked-thought/) analogy. Instead of aligning all glossary terms in a single level, the digital garden approach lets you go inwards. You can learn about each term and go deeper into each of its connections. The Glossary will show you each link that is related to the above interactive graph to it and all backlinks.\n\nThese will allow you to see connections in a visual way, that you would not otherwise.\n\nThis Glossary is forked from [Quartz](https://github.com/jackyzha0/quartz) and we thank Jacky for open-sourcing this gem.\n\n### Navigation\nYou can simply hit `ctrl/cmd+k` and **search** the whole Data Brain. Or you can click on the links and navigation through our content.\n\n### Interactive Graph\nUse the `Interactive Graph` on the bottom. It will appear every term. You can zoom and click on different nodes to navigate through the content.\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/acid-transactions":{"title":"What are ACID Transactions?","content":"An¬†ACID transaction¬†secures that either all changes are successfully committed or rollbacked. It makes sure you never end in an inconsistent state. There is different concurrency control that, for example, guarantees consistency between reads and writes. Each¬†[Data Lake Table Format](term/data%20lake%20table%20format.md)¬†has other implementations and features here.","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/airbyte":{"title":"What is Airbyte?","content":"\n[Airbyte](https://airbyte.com/) is the modern open-source [ELT](term/elt.md) standard and a [data integration](term/data%20integration.md) platform that syncs data from APIs, databases \u0026¬†files to [data warehouses](term/data%20warehouse.md), [lakes](term/data%20lake.md) and other destinations. In addition to covering the long tail of connectors with the involvement of its community, [[Airbyte Cloud]] differentiates itself with its transparent and predictable volume-based pricing. Airbyte addresses all connector needs through its open-source extensibility. Its ambition is to make [data integrations](term/data%20integration.md) a commodity.  \n\nRead more on [Why Airbyte](https://airbyte.com/why-airbyte), check out our [Connector Catalog](https://docs.airbyte.com/integrations/), or read about the [Company Updates](https://airbyte.com/blog-categories/company-updates).\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/airbyte-catalog":{"title":"Airbyte Catalog","content":"\n\u003e [!info] Info\n\u003e \n\u003e This is only relevant for individuals who want to create a connector.\n\nThis refers to how you define the data that you can retrieve from a Source. For example, if you want to retrieve information from an API, the data that you can receive needs to be defined clearly so that Airbyte can have a clear expectation of what endpoints are supported and what the objects that the streams return look like. This is represented as a sort of schema that Airbyte can interpret. \n\nLearn more on¬†[Beginners Guide to Catalog](https://docs.airbyte.com/understanding-airbyte/beginners-guide-to-catalog).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/airbyte-cdk":{"title":"Airbyte CDK","content":"\nThe Airbyte CDK (Connector Development Kit) allows you to create connectors for Sources or Destinations. If your source or destination doesn't exist, you can use the CDK to make the building process a lot easier. It generates all the tests and files you need and all you need to do is write the connector-specific code for your source or destination. \n\nAn extensive [Step-by-Step Example](https://airbyte.com/tutorials/extract-data-from-the-webflow-api) of how to create a custom Airbyte source connector with the [Python CDK](https://docs.airbyte.com/connector-development/cdk-python/). Another example by the Faros AI team created with [Javascript/Typescript](https://docs.airbyte.com/connector-development/cdk-faros-js). ","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/airbyte-cloud":{"title":"What is Airbyte Cloud?","content":"\n[Airbyte Cloud](https://cloud.airbyte.io) is the fastest, most reliable way to address all your [ELT](term/elt.md) needs that lets you get started in 10 minutes with hundreds of out-of-the-box connectors.\n\nAirbyte Cloud offers simple, predictable, scalable [pricing](https://airbyte.com/pricing).  [Try for free](https://cloud.airbyte.io/signup) or read more on [Airbyte¬†Cloud](https://airbyte.com/offer-cloud).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/airbyte-glossary-of-terms":{"title":"Glossary of Terms (Airbyte Specific)","content":"\nThis is the start of the Glossary relevant for [Airbyte](https://airbyte.com) specific, which are related to [docs.airbyte.com](https://docs.airbyte.com/) or when using Airbyte.\n\nYou'll find all terms at [#airbyte](/tags/airbyte/), or you can get inspired with the following terms:\n- [Airbyte CDK](term/airbyte%20cdk.md)\n- Related to [Incremental Sync](term/incremental%20synchronization.md) and [Full Refresh Sync](term/full%20refresh%20synchronization.md):\n\t- [Cursor](term/cursor.md), [Soft Delete](term/soft%20delete.md), [Partial Success](term/partial%20success.md), and ¬†[Raw Tables](term/raw%20tables.md)\n- [Airbyte Normalization](term/normalization.md)\n- [ETL and ELT](term/etl%20elt%20airbyte.md)\n\n## Advanced Terms\n- [Airbyte Catalog](term/airbyte%20catalog.md)\n- [Airbyte Specification](term/airbyte%20specification.md)\n- [Temporal](term/temporal.md)","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/airbyte-specification":{"title":"Airbyte Specification","content":"\u003e [!info] Info\n\u003e \n\u003e This is only relevant for individuals who want to create a connector.\n\nThis refers to the functions that a Source or Destination must implement to successfully retrieve data and load it, respectively. Implementing these functions using the Airbyte Specification makes a Source or Destination work correctly. \n\nLearn more¬†on [[Airbyte Protocol]].","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/airbyte-streams":{"title":"What are Airbyte Streams?","content":"In order to understand¬†**AirbyteStreams**, let‚Äôs first talk about the¬†**AirbyteCatalog**. An¬†**AirbyteCatalog**¬†describes the structure of data in a data source. It has a single field called streams that contains a list of¬†**AirbyteStreams**. Each¬†**AirbyteStream**¬†contains a¬†_name_¬†and¬†_json_schema_¬†field. The¬†_json_schema_¬†field describes the structure of a stream. This data model is intentionally flexible.\n\nIf we are using a data source that is a traditional relational database, each table in that database would map to an¬†**AirbyteStream**. Each column in the table would be a key in the¬†_properties_¬†field of the _json_schema_¬†field.\n\nIf we are using a data source that wraps an API with multiple different resources (e.g.¬†_api/customers_¬†and¬†_api/products_) each route would correspond to a stream.","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/analytics":{"title":"What is Analytics?","content":"Analytics is the systematic computational analysis of [[data]] or statistics. It is used for the discovery, interpretation, and communication of meaningful patterns in data. It also entails applying data patterns toward effective decision-making.\n\nIt's highly related to [Business Intelligence](term/business%20intelligence.md).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/apache-airflow":{"title":"What is Apache Airflow?","content":"\n[Airflow](https://airflow.apache.org/) is a [data orchestrator](term/data%20orchestrator.md) and the first that made task scheduling popular with [Python](term/python.md). Originally created by [Maxime Beauchemin](term/maxime%20beauchemin.md) working at Airbnb.\n\nAirflow programmatically author, schedule, and monitor workflows. It follows the [imperative](term/imperative.md) paradigm of schedule as *how* a DAG is run has to be defined within the Airflow jobs. Airflow calls its *Workflow as code* with the main characteristics\n- **Dynamic**: Airflow pipelines are configured as Python code, allowing for dynamic pipeline generation.\n- **Extensible**: The Airflow framework contains operators to connect with numerous technologies. All Airflow components are extensible to easily adjust to your environment.\n- **Flexible**: Workflow parameterization is built-in leveraging the¬†[Jinja Templating](term/jinja%20template.md)¬†engine.\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/apache-arrow":{"title":"What is Apache Arrow?","content":"Apache Arrow is a development platform for in-memory analytics. It contains a set of technologies that enable big data systems to process and move data fast.\n\nRead more on [Data Lake File Format](term/data%20lake%20file%20format.md).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/apache-avro":{"title":"What is Apache Avro?","content":"Avro is an open-source data serialization system that helps with data exchange between systems, [programming languages](term/programming%20languages.md), and processing frameworks. Avro helps define a binary format for your data, as well as map it to the programming language of your choice.\n\nAvro has a JSON-like data model, but can be represented as either JSON or in a compact binary form. It comes with a¬†**very sophisticated schema description language**¬†that describes data. Avro is another¬†[Data Lake File Format](term/data%20lake%20file%20format.md).\n\nRead more about how to build a Data Lake on top of it on our¬†[Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/apache-druid":{"title":"What is Apache Druid?","content":"Druid is an open-source, column-oriented, distributed data store written in Java. It's designed to quickly ingest massive quantities of event data, and provide low-latency queries on top of the data.\n\nAn analytics database designed for fast slice-and-dice analytics ([OLAP](term/olap%20(online%20analytical%20processing).md) queries) on large data sets. Most often, Druid powers use cases where real-time ingestion, fast query performance, and high uptime are important.\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/apache-hadoop":{"title":"What is Apache Hadoop?","content":"Apache Hadoop is a collection of open-source software utilities that facilitates using a network of many computers to solve problems involving massive amounts of data and computation. It provides a software framework for distributed storage and processing of big data using the [MapReduce](term/map%20reduce.md) programming model.\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/apache-hive":{"title":"What is Apache Hive?","content":"Apache Hive is a¬†[Data Warehouse](term/data%20warehouse.md)¬†software project built on top of¬†[Apache Hadoop](term/apache%20hadoop.md)¬†for providing data queries and analysis. Hive gives an SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop. Traditional SQL queries must be implemented in the¬†[MapReduce](term/map%20reduce.md)¬†Java API to execute SQL applications and queries over distributed data. Hive provides the necessary SQL abstraction to integrate SQL-like queries ([HiveQL](https://en.wikipedia.org/wiki/Apache_Hive#HiveQL)) into the underlying Java without the need to implement queries in the low-level Java API.","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/apache-hudi":{"title":"What is Apache Hudi?","content":"Apache Hudi is a¬†[Data Lake Table Format](term/data%20lake%20table%20format.md)¬†and was originally developed at Uber in 2016 (code-named and pronounced \"Hoodie\"), open-sourced end of 2016 ([first commit](https://github.com/apache/hudi/commit/0512da094bad2f3bcd2ddddc29e8abfec175dcfe)¬†in 2016-12-16), and submitted to the Apache Incubator in January 2019. More about the back story on¬†[The Apache Software Foundation Announces Apache¬Æ Hudi‚Ñ¢ as a Top-Level Project](https://www.globenewswire.com/news-release/2020/06/04/2043732/0/en/The-Apache-Software-Foundation-Announces-Apache-Hudi-as-a-Top-Level-Project.html).\n\nRead more about how to build a Data Lake on top of it on our¬†[Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/apache-iceberg":{"title":"What is Apache Iceberg?","content":"Apache Iceberg is a¬†[Data Lake Table Format](term/data%20lake%20table%20format.md)¬†and was¬†[initially developed](https://github.com/Netflix/iceberg)¬†at Netflix to solve long-standing issues using huge, petabyte-scale tables. It was open-sourced in 2018 as an Apache Incubator project and graduated from the incubator on the 19th of May 2020. Their¬†[first public commit](https://github.com/apache/iceberg/commit/a5eb3f6ba171ecfc517a4f09ae9654e7d8ae0291)¬†was 2017-12-19‚Äîmore insights about the story on¬†[A Short Introduction to Apache Iceberg](https://medium.com/expedia-group-tech/a-short-introduction-to-apache-iceberg-d34f628b6799).\n\nRead more about how to build a Data Lake on top of it on our¬†[Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/apache-parquet":{"title":"What is Apache Parquet?","content":"Apache Parquet is a free and open-source column-oriented¬†[Data Lake File Format](term/data%20lake%20file%20format.md)¬†in the Apache Hadoop ecosystem. It is similar to RCFile and¬†[ORC](term/orc.md), the other columnar-storage file formats in Hadoop, and is compatible with most of the data processing frameworks around Hadoop.\n\nRead more about how to build a Data Lake on top of it on our¬†[Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/apache-spark":{"title":"What is Apache Spark?","content":"Apache Spark‚Ñ¢¬†is an open-source multi-language engine for executing [Data Engineering](term/data%20engineering.md)  and [Machine Learning](term/machine%20learning.md) on single-node machines or clusters. It's optimized for large-scale data processing.\n\nSpark runs well with [Kubernetes](term/kubernetes.md).\n\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/behavioral-data":{"title":"What is Behavioral Data?","content":"Behavioral data are a result of users performing actions or events while interacting with a product and are therefore also referred to as event data or product-usage data. \n\n## Why collect Behavioral Data?\nBehavioral data serves two main purposes for teams ‚Äî understanding how the product is being used or not used (user behavior) and building personalized customer experiences across various touchpoints to influence user behavior.\n\nFind more in-depth on [How to Collect Behavioral Data? A Guide for Data Engineers and Analysts](https://airbyte.com/blog/collect-behavioral-data-guide).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/big-o-notation":{"title":"What is the Big-O Notation?","content":"\nBig-O Notation is an analysis of the algorithm using¬†[Big ‚Äì O asymptotic notation](https://www.geeksforgeeks.org/analysis-of-algorithms-set-3asymptotic-notations/).¬† Mostly related to computing rather than storage, but having in mind that doing things not exponentially, such as copying the same data many times, will save lots of performance and money.\n\nWe can express algorithmic complexity using the big-O notation. For a problem of size N:\n-   A constant-time function/method is ‚Äúorder 1‚Äù : O(1)\n-   A linear-time function/method is ‚Äúorder N‚Äù : O(N)\n-   A quadratic-time function/method is ‚Äúorder N squared‚Äù : O(N^2) \n\nCheck out more on [Analysis of Algorithms | Big-O analysis](https://www.geeksforgeeks.org/analysis-algorithms-big-o-analysis/).\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/bus-matrix":{"title":"What is Bus Matrix?","content":"A Bus Matrix was traditionally used in [OLAP](term/olap%20(online%20analytical%20processing).md) cubes such as Microsoft SSAS and co. They let you visually see what [[Measure]] can be queried with which [dimensions](term/dimensions.md).\n\nThey look something like this, example of [SSAS](https://blog.exsilio.com/all/ssas-dimensions-and-cube-basics/):\n![](https://lh3.googleusercontent.com/rtcy8qRao3f_3dXYyFRRRhE2Q21stS9gITYq4YJh2Y3iYf4QUYJgPGWehqwZmryWLfZARniGvboL_aeLwAblhxmClk4rj418Jof1ijdjocu61shPJzu1KdTk4UWxZWAToqgz8aVIiQXcHXWTc9I7yQoVtASNC3GQjcOTkKxAPehuFSHQdR1wOhbUPA)\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/business-intelligence":{"title":"What is Business Intelligence?","content":"Business intelligence (BI) leverages software and services to [transform data](term/data%20transformation.md) into actionable insights that inform an organization‚Äôs business decisions. The new term is [Data Engineering](term/data%20engineering.md). The language of a BI engineer is [SQL](term/sql.md).\n\n## Goals of BI\nBI should produce a simple overview of your business, boost efficiency, and automate repetitive tasks across your organization. In more detail:\n  * **Roll-up capability** - (data) [Visualization](term/analytics.md) over the most important [KPIs][2] (aggregations) - like a cockpit in an airplane which gives you the important information at one glance.\n  * **Drill-down possibilities** - from the above high-level overview drill down the very details to figure out why something is not performing as planned. **Slice-and-dice or¬†pivot your data from different angles.\n  * **Single source of truth** - instead of multiple spreadsheets or other tools with different numbers, the process is automated and done for all unified. Employees can talk about the business problem instead of the various numbers everyone has. Reporting, budgeting, and forecasting are automatically updated and consistent, accurate, and in timely manner.\n  * **Empower users**: With the so-called self-service BI, every user can analyze their data instead of only BI or IT persons.\n\nRead more on [Business Intelligence meets Data Engineering with Emerging Technologies](https://www.sspaeti.com/blog/business-intelligence-meets-data-engineering/).\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/business-intelligence-tools":{"title":"What are Business Intelligence Tools?","content":"\n[Business Intelligence](term/business%20intelligence.md) tools visualizing your data across the organizations. See a curated list of tools including the referenced image below on [Catalog of BI tools](https://notion.castordoc.com/catalog-of-bi-tools).\n\n![](images/business-intelligence-tools-landscape.png)\n\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/cdp-customer-data-platform":{"title":"What is a CDP (Customer Data Platform)?","content":"\nA customer Data Platform (CDP) is a system that collects large quantities of customer data (i.e. information about your customers) from a variety of channels and devices, helping to make this data more accessible to the people who need it. CDPs are responsible for sorting and categorizing data, as well as data cleansing to remove inaccurate or out-of-date information.\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/cloud-provider":{"title":"What are the top Cloud Providers?","content":"\nAmong the biggest cloud providers are [AWS](https://aws.amazon.com/), [Microsoft Azure](https://azure.microsoft.com/), [Google Cloud](https://cloud.google.com/). Whereas [Databrick](https://www.databricks.com/) and [Snowflake](https://www.snowflake.com/) provide dedicated [Data Warehouse](term/data%20warehouse.md) and [Lakehouse](term/data%20lakehouse.md) solutions.\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/contribute-to-glossary":{"title":"How to Contribute to the Glossary","content":"\n\u003e [!info] General Infos\n\u003e \n\u003e If you want to know more general how this glossary works, see [General Info](term/general%20infos.md) with a description of folder structure, how to create a link, etc.\n\nDeployment wise you have three options, which are explained in the chapters below:\n * Web edits through GitHub\n* Creating an Issue and we'll do the rest\n* Or cloning and running it locally\n\n## Web Edit with GitHub\nYou can either click on `Edit Source` on each page and directly edit on GitHub or you can create a [New Issue](https://github.com/airbytehq/glossary/issues).\n\n## Create an Issue\nIf you are unsure, you can always [create an issue](https://github.com/airbytehq/glossary/issues) on our GitHub repo and we will make sure to add the new entry.\n\n## Changing a lot? Clone locally\n### Clone it locally with git\nClone the [repo](https://github.com/airbytehq/glossary) with:\n```sh\ngit clone https://github.com/airbytehq/glossary.git\n```\n\n### Editors\n#### Obsidian as an Editor (Recommended)\nIf you want to use [Obsidian](https://obsidian.md/), which I recommend as it will handle all links when renaming terms, adding a nice Markdown view with lots of features (even if you don't need them) and showing backlinks and [graph](term/about%20this%20glossary.md#interactive-graph). Just open the Obsidian in the folder `content/`, there is a hidden folder called `.obsidian` which does the rest.\n\n1. ![](images/setup-obsidian-vault.png)\n2. ![](images/setup-folder-structure.png)\n\nMore details and step-by-step manual you see on [Quartz Setup](https://quartz.jzhao.xyz/notes/setup/), how to [Edit Notes ](https://quartz.jzhao.xyz/notes/editing/) and [How to set up Obsidian](https://quartz.jzhao.xyz/notes/obsidian/) (although the settings are already done when you open the `.obsidian` folder as described above).\n\n#### Use any other Editor\nOf course, as everything is Markdown, you can edit each file under `content/term` as a normal markdown file and publish (see below) changes to GitHub. Keep in mind, ethat very new term you create will automatically be created as a page in dthe eployment process or when you run it locally.\n\n### Preview Locally\n#### Setup\nQuartz runs on top of¬†[Hugo](https://gohugo.io/)¬†so all notes are written in¬†[Markdown](https://www.markdownguide.org/getting-started/).\n\nWe need to install golang, hugo-obsidian and hugo. Follow the instructions on [Preview Changes on Quartz](https://quartz.jzhao.xyz/notes/preview-changes/).\n\n\u003e [!info]\n\u003e \n\u003e If you are running into an error saying that¬†`command not found: hugo-obsidian`, make sure you set your¬†`GOPATH`¬†correctly! This will allow your terminal to correctly recognize hugo-obsidian as an executable.\n\nI added to my `~/.zshrc` (or `~/.bashrc`):\n ```sh\n#go path\nexport GOPATH=$HOME/go\nexport PATH=$PATH:$GOROOT/bin:$GOPATH/bin\n```\n#### Run it!\nAll you need to do it goint to your root directory of you cloned repo and start `make serve`:\n```sh\n# Navigate to your local Quartz folder\ncd \u003clocation-of-your-local-quartz\u003e\n\n# Start local server\nmake serve\n\n# View your site in a browser at http://localhost:1313/\n```\nThat's it, from now on that's how you run it. All changes you make will be automatically published, you do not need to stop and restart when you add terms, etc. (only the graph view will only be updated after stopping and serving again).\n\n## How to Publish\nCommit and Push to branch `hugo` and wait a couple of minutes until [GitHub Actions](https://github.com/airbytehq/glossary/actions) will deploy it automatically. At the moment we do not need to create PR's to make the updates as easy as possible. \n\nIf we encounter problems in the future, we might change that. If you are unsure, you can always [create an issue](https://github.com/airbytehq/glossary/issues) and we will make sure to add the new entry.\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/cte-common-table-expression":{"title":"What is a CTE (Common Table Expression)?","content":"\nA Common Table Expression (CTE) is a temporary named result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement. The CTE can also be used in a View.\n\n```sql\nWITH cte_query AS\n(SELECT ‚Ä¶ subquery ...)\nSELECT main query ... FROM/JOIN with cte_query ...\n```\n\n## Types: Recursive and Non-Recursive\n### Non-Recursive CTE\nThere are two types of CTEs: Recursive and Non-Recursive.\n\nThe non-recursive are simple where CTE is used to avoid SQL duplication by referencing a name instead of the actual SQL statement.\n\nE.g.\n```sql\nWITH avg_per_store AS\n  (SELECT store, AVG(amount) AS average_order\n   FROM orders\n   GROUP BY store)\nSELECT o.id, o.store, o.amount, avg.average_order AS avg_for_store\nFROM orders o\nJOIN avg_per_store avg\nON o.store = avg.store;\n```\n\n### Recursive CTE\n\nRecursive CTEs use repeated procedural loops therefore the recursion. The recursive query calls itself until the query satisfied the condition. In a recursive CTE, we should provide a where condition to terminate the recursion.\n\nA recursive CTE is useful in querying hierarchical data such as organization charts where one employee reports to a manager or multi-level bill of materials when a product consists of many components, and each component itself also consists of many other components.\n\n```sql\nWITH levels AS (\n  SELECT\n    id,\n    first_name,\n    last_name,\n    superior_id,\n    1 AS level\n  FROM employees\n  WHERE superior_id IS NULL\n  UNION ALL\n  SELECT\n    employees.id,\n    employees.first_name,\n    employees.last_name,\n    employees.superior_id,\n    levels.level + 1\n  FROM employees, levels\n  WHERE employees.superior_id = levels.id\n)\n \nSELECT *\nFROM levels;\n```\n\nSee more on[5 Practical SQL CTE Examples | LearnSQL.com](https://learnsql.com/blog/practical-sql-cte-examples/).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/cursor":{"title":"What is a Cursor?","content":"At a conceptual level, a cursor is a tracker that is used during [incremental synchronization](term/incremental%20synchronization.md) to ensure that only newly updated or inserted records are sent from a data source to a destination in any given synchronization iteration.\n\nAirbyte‚Äôs incremental synchronization can be conceptually thought of as a loop which periodically executes synchronization operations. Each iteration of this loop only replicates records that have been inserted or updated in the source system since the previous execution of this synchronization loop ‚Äì in other words, each synchronization operation will copy only records that have not previously been replicated by previous synchronizations. This is much more efficient than copying an entire dataset on each iteration, which is the behavior of full refresh synchronization.\n\nSending only updated or newly inserted documents requires tracking which records have already been replicated in previous synchronizations. This is done by a cursor, which can be thought of as a pointer to the most recent record that has been replicated by a given synchronization. When selecting documents for synchronization, Airbyte includes the most recent cursor value as part of the query on the source system to ensure that only new/updated records will be replicated.\n\nFor example, a source database could contain records which include a field called `updated_at`, which stores the most recent time that a record is inserted or updated. If `updated_at` is selected as the cursor field, then after a given synchronization operation the cursor will remember the largest `updated_at` value that has been seen in the records that have been replicated to the destination in that synchronization. In the subsequent synchronization operation, records that have been inserted or updated on the source are retrieved by including the cursor value as part of the query, so that it only selects records where the `updated_at` value is greater than (and in some edge cases greater than or equal to) the largest `updated_at` value seen in the previous synchronization.\n\nNote that while it is not strictly necessary to choose a time field for a cursor field, the field that is chosen should be monotonically increasing over time.\n\nRead more on [Incremental data synchronization between Postgres databases](https://airbyte.com/tutorials/incremental-data-synchronization). ","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/dag-directed-acyclic-graph":{"title":"What is a Directed Acyclic Graph (DAG)?","content":"DAG stands for **Directed Acyclic Graph**. A DAG is a graph where information must travel along with a finite set of nodes connected by vertices. There is no particular start or node and also no way for data to travel through the graph in a loop that circles back to the starting point.\n\nIt's a popular way of building data pipelines in tools like [[Airflow]], [[Dagster]], [[Prefect]]. It clearly defines the¬†[Data Lineage](term/data%20lineage.md). As well, it's made for a functional approach where you have the¬†[idempotency](term/idempotency.md)¬†to restart pipelines without side-effects.\n\n![](images/dag.png)","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/dagster":{"title":"What is Dagster?","content":"[Dagster](https://dagster.io/) is a [data orchestrator](term/data%20orchestrator.md) focusing on data-aware scheduling that supports the whole development lifecycle, with integrated lineage and observability, a [declarative](term/declarative.md) programming model, and best-in-class testability.\n\nKey features are: \n- Manage your data assets with code\n- A single pane of glass for your data platform ","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-asset":{"title":"What is a Data Asset?","content":"A data asset is typically a database table, a machine learning model, or a report. A persistent object that captures some understanding of the world. It's more a technical term where¬†[Data Product](term/data%20product.md)¬†is more used in general or in¬†[Data Mesh](term/data%20mesh.md).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-catalog":{"title":"What is a Data Catalog?","content":"A Data Catalog¬†is a centralized store where all metadata data about your data is made searchable.\n\n**Think about a Google Search for your internal Metadata**. This is vital, as with¬†[Data Lake](term/data%20lake.md)¬†and other data stores, and you want the ability to search for your data. Data is growing exponentially, with 90% of the world‚Äôs data being generated alone in the last two years. It's hard to keep this amount over time. A data catalog solves the problem of the fast-growing handling of data internally.\n\nAn interesting read about the beginning of the Data Catalog is explained in the 2017 published paper about a¬†[Data Context Service](http://cidrdb.org/cidr2017/papers/p111-hellerstein-cidr17.pdf).  \n\nSee a High-Level Feature Comparison by the¬†[Awesome Data Discovery and Observability](https://github.com/opendatadiscovery/awesome-data-catalogs)¬†list on GitHub (check out the link for more):\n![](images/data-catalog-feature-comparison2.png)\n\nOr a great overview by Sarah Krasnik on [Choosing a Data Catalog](https://sarahsnewsletter.substack.com/p/choosing-a-data-catalog):\n![](images/data-catalog-overview-sarah.png)\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-contract":{"title":"What is a Data Contract?","content":"\nData Contracts are API-like agreements between software/data engineers who own services and data consumers that understand how the business works. The goal is to generate well-modeled, high-quality, trusted, real-time data.\n\nIt's an **abstraction** that allows engineers to decouple their databases and services from analytics and ML requirements. It will avoid production-breaking incidents when modifying the schema as they are validated and enforced.\n\n![](images/data-contract.png)\nIllustration by Chad Sanderson on [The Rise of Data Contracts - by Chad Sanderson](https://dataproducts.substack.com/p/the-rise-of-data-contracts)\n\n[Chad Sanderson](https://www.linkedin.com/in/chad-sanderson/) said that at Convoy, they use [[Protobuf]] and [[Apache Kafka]] to abstract the CRUD transactions. They define the schema based on what they *need*, not what they get from the source. Same as [[Software-Defined Assets]] describe the [Data Asset](term/data%20asset.md) in a declarative manner and set [expectations](https://github.com/dagster-io/dagster/discussions/9543).\n\nConfluent also built similar functions on top of Kafka with their [Schema Registry](https://docs.confluent.io/platform/current/schema-registry/), and terms such as [Semantic Layer](term/metrics%20layer.md) and [Analytics API](https://www.sspaeti.com/blog/analytics-api-with-graphql-the-next-level-of-data-engineering/#what-is-an-analytics-api) (with [[GraphQL]]) are trying to achieve similar things.\n\nData Contracts are not meant to replace data pipelines and [Modern Data Stack](term/modern%20data%20stack.md), a more batch approach. These are good for fast prototyping. You could start defining data contracts when you have some knowledge about data.\n\nInterestingly, the differentiation to [Data Mesh](term/data%20mesh.md) is an organizational framework with a micro-service approach to data. Data Mesh doesn't inform which data should be emitted or validate the data being emitted from production is correct or conforms to a consumer's expectations.\n\nAlso, data contracts are a form of [Data Governance](term/data%20governance.md). This term is very vague and gets more concrete with explicit contracts. You can also use [Great Expectations](https://greatexpectations.io/) to set expectations for your data, which I believe is a great way to start.\n\n## From the Discussion on YouTube w/ Chad Sanderson vs Ethan Aaron\n\nChad Sanderson says in [Data Contract Battle Royale w/ Chad Sanderson vs Ethan Aaron - YouTube](https://youtu.be/4BEpYAp3Qu4) :\n- It's just a database version of a real-world contract. \n- A real-world contract is just an agreement between two parties where:\n\t- There's some mechanism for enforcing that it happens. \n\t- A data contract is a similar agreement, but it's **between someone that produces data and consumes data** to vend a particular data set which usually includes a schema and some enforcement mechanism. \n- Differentiation between data contract and data product:\n\t- **Data contract**, which is *what* is the data and *how* do we enforce this quality \n\t- **[Data Product](term/data%20product.md)** which is *why* do we need this data \n\nEthan Aaron is saying his problem with data contracts is that you focus on defining the interface/contract too early. E.g., if you have a big task done by several teams or people, you have a contract to agree on an interface. I'd argue that's precisely what the data products are, and instead of agreeing on some artificial contract, decide on the product, so the tools and teams can be distinct.\n\n## Summary Blog Posts\nAn excellent summary by [Mehdi Ouazza](https://www.linkedin.com/in/mehd-io) about data contracts [From Zero To Hero](https://towardsdatascience.com/data-contracts-from-zero-to-hero-343717ac4d5e). He is illustrating how [[Apache Kafka]] could also be the interface that defines the contract.\n\n![](images/data-contract-example.png)\nIllustration from [Data Contracts ‚Äî From Zero To Hero](https://towardsdatascience.com/data-contracts-from-zero-to-hero-343717ac4d5e)\n\nSee also [Semantic Warehouse](term/semantic%20warehouse.md).\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-engineering":{"title":"What is Data Engineering?","content":"The definition from the¬†[Fundamentals of Data Engineering](https://www.oreilly.com/library/view/fundamentals-of-data/9781098108298/), as it‚Äôs one of the most recent and complete: \n\u003e Data engineering is the development, implementation, and maintenance of systems and processes that take in raw data and produce high-quality, consistent information that supports downstream use cases, such as analysis and machine learning. Data engineering intersects security, data management, DataOps, data architecture, orchestration, and software engineering.\n\nA data engineer today oversees the whole data engineering process, from collecting data from various sources to making it available for downstream processes. The role requires familiarity with the multiple stages of the [Data Engineering Lifecycle](term/data%20engineering%20lifecycle.md) and an aptitude for evaluating data tools for optimal performance across several dimensions, including price, speed, flexibility, scalability, simplicity, reusability, and interoperability.\n\nData Engineering helps also overcome the bottlenecks of [Business Intelligence](term/business%20intelligence.md):\n- More transparency as tools are open-source mostly\n- More frequent data loads\n- Supporting [Machine Learning](term/machine%20learning.md) capabilities \n\nCompared to existing roles it would be a **software engineering plus business intelligence engineer including big data abilities** as the [Hadoop](term/apache%20hadoop.md) ecosystem, streaming, and computation at scale. Business creates more reporting artifacts themselves but with more data that needs to be collected, cleaned, and updated near real-time and complexity is expanding every day.\n\nWith that said more programmatic skills are needed similar to software engineering. **The emerging language at the moment is [Python](term/python.md)** which is used in engineering with tools alike [[Apache Airflow]], [Dagster](Dagster), [[Prefect]] as well as data science with powerful libraries.\n\nAs a data engineer, you use mainly [SQL](term/sql.md) for almost everything except when using external data from an API. Here you'd use [ELT](term/elt.md) tools or write some [[data pipelines]] with the tools mentioned above.\n\nWant to know more about [The Evolution of The Data Engineer: A Look at The Past, Present \u0026 Future](https://airbyte.com/blog/data-engineering-past-present-and-future), check out the linked article or watch the video form of it:\n{{\u003c youtube Si14Hgj4Lok \u003e}}\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-engineering-concepts":{"title":"Data Engineering Concepts","content":"Some core concepts we are going to explore:\n\n| Data Engineering Topics                                                                                                 |\n|--------------------------------------------------------------------------------------------------------|\n| [Data Warehouse](term/data%20warehouse.md), [Data Lake](term/data%20lake.md), [Data Lakehouse](term/data%20lakehouse.md) |\n| [Storage Layer](term/storage%20layer%20object%20store.md), [Data Lake File Format](term/data%20lake%20file%20format.md), [Data Lake Table Format](term/data%20lake%20table%20format.md) |\n| [Data Catalog](term/data%20catalog.md)                                                                |\n| [Modern Data Stack](term/modern%20data%20stack.md), [Open Data Stack](term/open%20data%20stack.md)    |\n| [Data Engineering Lifecycle](term/data%20engineering%20lifecycle.md)                                  |\n| [ELT](term/elt.md), [ETL](term/etl.md), [EtLT](term/etlt.md)                                          |\n| [Functional Data Engineering](term/functional%20data%20engineering.md), [Software-Defined Assets](term/software-defined%20assets.md) |\n| [Metrics Layer](term/metrics%20layer.md), [Semantic Warehouse](term/semantic%20warehouse.md), [Data Virtualization](term/data%20virtualization.md) |\n| [Metrics](term/metric.md), [Key Performance Indicator (KPI)](term/key%20performance%20indicator%20(kpi).md) |\n| [Push-Downs](term/push-down.md), [Rollup](term/rollup.md)                                             |\n| [Data Modeling](term/data%20modeling.md), [Dimensional Modeling](term/dimensional%20modeling.md)      |\n| [Data Contract](term/data%20contract.md)                                                              |\n| [OLAP](term/olap%20(online%20analytical%20processing).md), [OLTP](term/oltp%20(online%20transactional%20processing).md) |\n| [MapReduce](term/map%20reduce.md), [Apache Hadoop](term/apache%20hadoop.md)                           |\n| [Declarative vs Imperative](term/declarative.md)                                                      |\n| [Notebooks](term/notebooks.md)                                                                        |\n\n\n\n\n\n\u003c!--\n- [[Batch processing]] vs [[Streaming Processing]]\n- [[Indexing]]\n- [[Relational Database]] vs [[NoSQL Database]]\n--\u003e\n\nSee also [What is Data Engineering](term/data%20engineering.md).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-engineering-guides":{"title":"Data Engineering Guides","content":"\nSome Data Engineering Guides that will help you learn [data engineering](term/data%20engineering.md):\n\n- **[Data Quality](https://airbyte.com/blog/data-quality-issues)**\n\t- How to handle [[term/data quality]] issues by detecting, understanding, fixing, and reduce\n- **[Data Lake / Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi)**\n\t- The what \u0026 why of a [Data Lake](term/data%20lake.md)\n\t- Differences between [Lakehouse](term/data%20lakehouse.md) \u0026 [Data Warehouse](term/data%20warehouse.md)\n\t- Components of a data lake\n\t\t1. [Storage Layer](term/storage%20layer%20object%20store.md)\n\t\t2. [Data Lake File Format](term/data%20lake%20file%20format.md)\n\t\t3. [Data Lake Table Format](term/data%20lake%20table%20format.md) with [Apache Parquet](term/apache%20parquet.md), [Apache Iceberg](term/apache%20iceberg.md), and [Apache Hudi](term/apache%20hudi.md)\n\t- Trends in the market\n\t- We answer questions such as:\n\t\t- How to build an open-source data lake offloading data for analytics?\n\t\t- How to [govern](term/data%20governance.md) your hundreds to thousands of files and have more database-like features?\n- **[Reverse ETL Explained](https://airbyte.com/blog/reverse-etl)**\n\t- A Brief Story of Data Integration: [ETL](term/etl.md) vs. [ELT](term/elt.md)\n\t- So, What is a [Reverse ETL](term/reverse%20etl.md)?\n\t- Technical Differences Between ETL and Reverse ETL\n\t- Typical Reverse ETL Use Cases\n\t- Reverse ETL and the [Data Hierarchy of Needs](term/data%20hierarchy%20of%20needs.md)\n- [Data Orchestration Trends](https://airbyte.com/blog/data-orchestration-trends)\n- [Data Integration Guide](https://airbyte.com/blog/data-integration)\n- [Understanding Change Data Capture (CDC)](https://airbyte.com/blog/change-data-capture-definition-methods-and-benefits)\n- [Using an ETL Framework vs Writing Yet Another ETL Script](https://airbyte.com/blog/etl-framework-vs-etl-script)\n\nSee more on [Data Insights Blog Posts](https://airbyte.com/blog-categories/data-insights).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-engineering-lifecycle":{"title":"What is the Data Engineering Lifecycle?","content":"\nA data engineer today oversees the whole data engineering process, from collecting data from various sources to making it available for downstream processes. The role requires familiarity with the multiple stages of the data engineering lifecycle and an aptitude for evaluating data tools for optimal performance across several dimensions, including price, speed, flexibility, scalability, simplicity, reusability, and interoperability.\n\n![](images/data-engineering-lifecycle.png)\nThe data engineering lifecycle, inspired by [Fundamentals of Data Engineering](https://www.oreilly.com/library/view/fundamentals-of-data/9781098108298/)\n\n\u003e [!example] Example Open Data Stack Project\n\u003e\n\u003e With the [Open Data Stack](term/open%20data%20stack.md) project, we are implementing a hands-on example with the core components of the lifecycle, such as ingestion, [transformation](term/data%20transformation.md), [analytics](term/analytics.md), and [machine learning](term/machine%20learning.md).\n\nRead more on [The Evolution of The Data Engineer: A Look at The Past, Present \u0026 Future](https://airbyte.com/blog/data-engineering-past-present-and-future).\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-enrichment":{"title":"What is data enrichment","content":"\nData enrichment is a kind of [data transformation](term/data%20transformation.md) which adds additional information to the data in order to makes new kinds of queries possible and/or more efficient.\n\n## Example of data enrichment\nImagine that you have a ‚ÄúSystem A‚Äù that contains an IP address to country mapping, and a ‚ÄúSystem B‚Äù that contains a data set with records that include an IP address (but no country). If you would like to execute queries on ‚ÄúSystem B‚Äù by country, it would be beneficial to transform records in ‚ÄúSystem B‚Äù to include a country field. This can be achieved by running a transformation job that reads the IP address from each record on ‚ÄúSystem B‚Äù, performs a lookup on ‚ÄúSystem A‚Äù to get the associated country name, and that writes the country name back into an ‚Äúenriched‚Äù data set on ‚ÄúSystem B‚Äù. Future queries which break down the data by country can then be efficiently executed against this enriched data set on ‚ÄúSystem B‚Äù.","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-federation":{"title":"What is Data Federation","content":"\nData Federation is a virtual layer very similar to [Data Virtualization](term/data%20virtualization.md). The slight difference is that data federations include federated query engines such as [[Trino]], [[Presto]], [[Spark]], and alike. ","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-governance":{"title":"What is Data Governance?","content":"[**Data governance**](https://www.talend.com/resources/what-is-data-governance/)¬†**is a collection of processes, roles, policies, standards, and metrics that ensure the effective and efficient use of information in enabling an organization to achieve its goals.**¬†It establishes the processes and responsibilities that ensure the [data quality](term/data%20quality.md) and security of the data used across a business or organization. Data governance defines who can take what action, upon what data, in what situations, and using what methods.\n\nRead more on our¬†[Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-hierarchy-of-needs":{"title":"The Data Hierarchy of Needs","content":"\nThe data hierarchy of needs in this image is inspired by¬†[Grouparoo's blog post](https://www.grouparoo.com/blog/data-hierarchy-of-needs):\n![](images/data-hierarchy-of-needs.png)\n\nMore on [Reverse ETL Explained](https://airbyte.com/blog/reverse-etl#so-what-is-a-reverse-etl).\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-integration":{"title":"What is Data Integration?","content":"\nData integration is the process of combining data from disparate source systems into a single unified view. This can be accomplished via manual integration, data virtualization, application integration, or by moving data from multiple sources into a unified destination.¬†These data integration methods are discussed below.\n\n## Manual integration¬†\nBefore implementing a systematic approach to data integration, organizations may initially make use of manual integration when trying to make sense of data that is spread across multiple systems. This involves analysts manually logging into source systems, analyzing and/or exporting data on these systems, and creating reports based on their findings.¬†\n\nManual integration as a data integration strategy has several disadvantages. In addition to being time-consuming, analysts require access to multiple operational systems which creates security risks. Furthermore, analysts may run expensive analytics operations on systems that are not optimized for such workloads, which may interfere with the functioning of these systems. Finally, data in the source systems may frequently change which means that manually generated reports will quickly become outdated.¬†\n\n## Data virtualization\nOrganizations may also consider adopting a data virtualization solution to integrate their data. In this type of data integration, data from multiple sources is left in place and is accessed via a virtualization layer so that it¬†_appears_¬†as a single data store. This virtualization layer makes use of adapters that translate queries executed on the virtualization layer into a format that each connected source system can execute. The virtualization layer then combines the responses from these source systems into a single result. This data integration strategy is sometimes used when a BI tool like Tableau needs to access data from multiple data sources.\n\nOne disadvantage of data virtualization is that analytics workloads are executed on operational systems, which could interfere with their functioning. Another disadvantage is that the virtualization layer may act as a bottleneck on the performance of analytics operations.¬†¬†\n\n## Application integration\nAnother alternative data integration solution is to directly link multiple applications to each other and move data directly between them. This is known as application integration, and linking can be done via point-to-point communications, via a middleware layer such as an enterprise service bus (ESB), or through an application integration tool.¬†\n\nApplication integration may result in many copies of the same data across multiple source systems, which may increase cost, and may cause a large amount of point-to-point traffic between various systems. Furthermore, as with the previous data integration types, executing analytics workloads directly on operational systems could interfere with their functioning.\n\n## Moving data to a unified destination\nSending data from across an enterprise into a centralized system such as a database, a data warehouse, a data lake, or a data lakehouse results in a¬†**single unified location for accessing and analyzing all the information that is flowing through an organization**. At Airbyte we are advocates of this data integration methodology, and the next section of this article is dedicated to discussing its benefits in more detail.¬†\n\nBelow is a high-level representation of¬†[data replication](https://airbyte.com/blog/what-is-data-replication)¬†from multiple sources into Google BigQuery.¬†\n\n![data-integration](images/data-integration.jpg)\nData replication into a central destination\n\nRead more on [Data Integration Guide: Techniques, Technologies, and Tools | Airbyte](https://airbyte.com/blog/data-integration).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-lake":{"title":"What is a Data Lake?","content":"A Data Lake is a storage system with vast amounts of unstructured and structured data, stored as-is, without a specific purpose in mind, that can be built on multiple technologies such as Hadoop, NoSQL, Amazon Simple Storage Service, a relational database, or various combinations and different formats (e.g. Excel, CSV, Text, Logs, etc.).\n\nAccording to¬†[Hortonworks Data Lake Whitepaper](http://hortonworks.com/wp-content/uploads/2014/05/TeradataHortonworks_Datalake_White-Paper_20140410.pdf), the data lake arose because new types of data needed to be captured and exploited by the enterprise. As this data became increasingly available, early adopters discovered that they could extract insight through new applications built to serve the business. The data lake supports the following capabilities:\n-   To capture and store raw data at scale for a low cost\n-   To store many types of data in the same repository\n-   To perform [data transformation](term/data%20transformation.md) on the data where the purpose may not be defined\n-   To perform new types of data processing\n-   To perform single-subject analytics based on particular use cases\n\nThe initial concept was created by Databricks in the¬†[CIDR Paper](http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf)¬†in 2021. Read more on our¬†[Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-lake-file-format":{"title":"What is a Data Lake File Format?","content":"Data lake file formats are the new CSVs on the cloud. They are more column-oriented and compress large files with added features. The main players here are [Apache Parquet](term/apache%20parquet.md), [Apache Avro](term/apache%20avro.md), and [Apache Arrow](term/apache%20arrow.md). It‚Äôs the physical store with the actual files distributed around different buckets on your¬†[Object Store](term/storage%20layer%20object%20store.md).\n\nYou can build more features with [Data Lake Table Format](term/data%20lake%20table%20format.md) on top. Read more on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-lake-table-format":{"title":"What is a Data Lake Table Format?","content":"Data lake table formats are very attractive as they are databases on¬†[Data Lake](term/data%20lake.md). Same as a table, one¬†**data lake table format bundles distributed files into one table that is otherwise hard to manage**. You can think of it as an abstraction layer between your physical data files and how they are structured to form a table.\n\n\n\nIt is built on top o the [Storage Layer](term/storage%20layer%20object%20store.md) and [Data Lake File Format](term/data%20lake%20file%20format.md). Table Formats are [Delta Lake](term/delta%20lake.md), [Apache Iceberg](term/apache%20iceberg.md) or [Apache Hudi](term/apache%20hudi.md). Read more on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-lake-transaction-log":{"title":"What is a Data Lake Transaction Log?","content":"The transaction log is the ordered record of every transaction, with a configurable duration that can optionally be set to retain all transactions (i.e. data infinite). A transaction log is a common component used through many of its above-mentioned features, including¬†[ACID Transactions](term/acid%20transactions.md), scalable metadata handling, and¬†[Time Travel](term/time%20travel.md). For example,¬†[Delta Lake](term/delta%20lake.md)¬†creates a single¬†[folder called `_delta_log`](https://airbyte.com/tutorials/load-data-into-delta-lake-on-databricks-lakehouse#step-5).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-lakehouse":{"title":"What is a Data Lakehouse?","content":"\nA Data Lakehouse open data management architecture that combines the flexibility, cost-efficiency, and scale of¬†[Data Lake](term/data%20lake.md)¬†with the data management and ACID transactions of¬†[Data Warehouse](term/data%20warehouse.md)¬†with Data Lake Table Formats¬†([Delta Lake](term/delta%20lake.md), [Apache Iceberg](term/apache%20iceberg.md) \u0026 [Apache Hudi](term/apache%20hudi.md)) that enable Business Intelligence¬†(BI) and Machine Learning¬†(ML) on all data.\n\nThe initial concept was created by Databricks in the¬†[CIDR Paper](http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf)¬†in 2021. Read more on our¬†[Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-lineage":{"title":"What is Data Lineage?","content":"Data lineage uncovers the life cycle of data. It aims to show the complete data flow from start to finish. Data lineage is the process of understanding, recording, and visualizing data as it flows from data sources to consumption. This includes all [data transformation](term/data%20transformation.md) (what changed and why).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-literacy":{"title":"What is Data Literacy?","content":"\nData literacy is the ability to read, work with, analyze, and argue with data in order to extract meaningful information and make informed decisions. This skill set is crucial for employees across various levels of an organization, especially as data-driven decision-making becomes increasingly important.\n\nOrganizations should invest in data literacy training programs to empower their employees with the necessary skills to effectively engage with data. A data-literate employee can read charts, draw correct conclusions, recognize when data is being used inappropriately or misleadingly, and gain a deeper understanding of the business domain. This enables them to communicate more effectively using a common language of data, spot unexpected operational issues, identify root causes, and prevent poor decision-making due to data misinterpretation.\n\nExamples of data literacy in action include:\n\n* Implementing the Adoptive Framework to create a Data Literacy Program.\n* Employees working with spreadsheets to understand the rationale behind data-driven decisions and advocating for alternative courses of action.\n* Work teams identifying areas where data needs clarification for a project.\n\nBy nurturing a data-literate workforce, businesses can improve their ability to make informed decisions, drive innovation, and achieve better outcomes.\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-mesh":{"title":"What is Data Mesh?","content":"The¬†[Data Mesh Paper](https://martinfowler.com/articles/data-monolith-to-mesh.html)¬†tries to eliminate silos between data teams, ensuring that the experience and knowledge about data are shared among all data consumers in the company. Data Mesh sees¬†[Data as a Product](term/data%20product.md). Data meshes are also about connecting platforms that those teams are using so data can be easily moved around for the organization's benefit. Companies will try to find better ways of unifying and connecting the tools so that data professionals don‚Äôt have to switch and work in a silo.\n\nData meshes try to eliminate the tensions between decentralizing and centralizing data resources, with some common infrastructure but otherwise mostly decentralized. It empowers data teams and gives ownership to domain experts.\n\nMore valuable resources such as a¬†[short version](https://cnr.sh/essays/what-the-heck-data-mesh), a¬†[visually appealing one](https://www.datamesh-architecture.com/)), or¬†[applied in practice](https://youtu.be/eiUhV56uVUc).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-modeling":{"title":"What is Data Modeling?","content":"\nData modeling in software engineering is the process of creating a data model for an information system by applying certain formal techniques.¬†One specific example of data modeling is [Dimensional Modeling](term/dimensional%20modeling.md) which has a high state even in modern data architecture.\n\nRead more on [Wikipedia](https://en.wikipedia.org/wiki/Data_modeling).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-observability":{"title":"What is Data Observability?","content":"\nData observability, also known as monitoring, continuously collects metrics about your data. You can collect data about the number of rows, columns, and properties for each dataset. You can also manage metadata about the dataset, such as when it was last updated.\n\nFrom the great article [Choosing a Data Quality Tool - by Sarah Krasnik](https://sarahsnewsletter.substack.com/p/choosing-a-data-quality-tool?s=r), there are also different categories for observability:\n- **Auto-profiling data**\n\t- [Bigeye](https://www.bigeye.com/): unique in a wide range of ML-driven automatic threshold tests and alerts\n\t- [Datafold](https://www.datafold.com/): unique Github integration presenting Data Diff between environments with custom tests\n\t- [Monte Carlo](https://www.montecarlodata.com/): unique in being the most enterprise-ready enterprise-ready with many data lake integrations\n\t- [Lightup](https://www.lightup.ai/): unique self-hosted deployment option, appealing to highly regulated industries\n\t- [Metaplane](https://www.metaplane.dev/): unique in a high level of configuration for a hosted tool with both out-of-the-box and custom tests\n- **Pipeline Testing**\n\t- [Great Expectations](https://greatexpectations.io/): unique in its data quality specific community and automatic documentation of tests\n\t- [Soda](https://www.soda.io/): unique in its self-hosted cloud option\n\t- [dbt tests](https://docs.getdbt.com/docs/building-a-dbt-project/tests): unique in integration with dbt core and dbt Cloud builds (naturally), but not as versatile outside of the dbt ecosystem\n- **Infrastructure monitoring**\n\t- [DataDog](https://www.datadoghq.com/): unique agent implementation that can be deployed anywhere for monitoring, even at the container level, with custom Airflow metric reporting\n\t- [New Relic](https://newrelic.com/): unique one-step integration with the big three cloud \t\n- **A little bit of everything**\n\t- [Databand](https://databand.ai/): unique integration with Airflow and specific Airflow metric monitoring\n\t- [Unravel](https://www.unraveldata.com/): unique support for other data sources like Spark, data lake, and NoSQL databases\n\t- [Data Catalogs](term/data%20catalog.md): Helping observe existing data\n\nRelated terms are [Data Governance](term/data%20governance.md) and [Data Quality](term/data%20quality.md).\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-ops":{"title":"What is DataOps?","content":"Similar to how¬†[DevOps](term/dev%20ops.md)¬†changed the way software is developed, DataOps is changing the way data products are created. With DataOps, data engineers and data scientists can work together, bringing a level of collaboration and communication, with a common goal of producing valuable insight for the business.\n\n![](images/data-ops.png)\nRead more on [The Rise of DataOps](https://medium.com/towards-data-science/the-rise-of-dataops-2788958034ee).\n","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-orchestrator":{"title":"What is a Data Orchestrator?","content":"A Data Orchestrator¬†models dependencies between different tasks in¬†[complex heterogeneous cloud environments](https://mattturck.com/data2021/)¬†end-to-end. It handles integrations with legacy systems, new cloud-based tools, and your data lakes and data warehouses. It¬†invokes¬†[computation](https://en.wikipedia.org/wiki/Orchestration_(computing)), such as wrangling your business logic in [SQL](term/sql.md) and [Python](term/python.md) and applying ML models¬†at the right time based on a time-based trigger or by custom-defined logic.\n\nMore Insights in¬†[Data Orchestration Trends: The Shift from Data Pipelines to Data Products](https://airbyte.com/blog/data-orchestration-trends).","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-processing-techniques":{"title":"What are Data Processing Techniques (row-based, columnar, vectorized)?","content":"\nA collection of concepts and technologies that encompass various methods and optimizations for storing, retrieving, and processing data in database systems. This glossary page includes definitions for **columnar storage, row-based storage, and vectorized engines**, which are all techniques that aim to improve the efficiency and performance of different types of workloads, such as transactional, analytical, and large-scale data processing tasks. \n\nBy understanding these techniques, database users and developers can make informed decisions about which approach best fits their specific use cases and requirements.\n\n## Columnar Storage\nA database storage technique that stores data by columns rather than rows, allowing for more efficient compression and faster querying for analytical workloads. Columnar storage is particularly useful for read-heavy operations and large-scale data analytics, as it enables the retrieval of specific columns without the need to access the entire row. This contrasts with traditional row-based storage, where data is stored row by row, making it more suited for transactional workloads and operations that involve frequent updates and inserts.\n\n## Row-based Storage\nA traditional database storage technique where data is stored in consecutive rows, allows for efficient processing of operations that involve entire records, such as inserts, updates, and deletions. Row-based storage is well-suited for transactional systems (OLTP) that require fast access to individual records. However, it can be less efficient for analytical workloads and large-scale data processing, where columnar storage offers better performance due to its ability to selectively retrieve specific columns without accessing the entire row.\n\n## Vectorized Engine\nA modern database query execution engine designed to optimize data processing by leveraging vectorized operations and SIMD (Single Instruction, Multiple Data) capabilities of modern CPUs. Vectorized engines, such as Databricks' Photon Engine or [DuckDB](term/duckdb.md), process data in large blocks or batches, allowing for improved parallelism, cache locality, and reduced overhead compared to traditional row-at-a-time processing engines. This results in significantly faster query performance, particularly for [analytical](term/analytics.md) and large-scale data processing workloads, making vectorized engines a popular choice in the era of big data and real-time analytics.","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-product":{"title":"What is a Data Product?","content":"[DJ Patil](https://twitter.com/dpatil), the former Chief Data Scientist of the United States, defined a data product as \"a product that facilitates an end goal through data.\" Also,¬†[Data Mesh](term/data%20mesh.md)¬†talks about \"data as a product.\" It applies more product thinking, whereas the \"Data Product\" essentially is a dashboard, report, and table in a¬†[Data Warehouse](term/data%20warehouse.md)¬†or a Machine Learning model. Sometimes Data Products are also called¬†[Data Asset](term/data%20asset.md)s.","lastmodified":"2023-07-25T13:01:03.633830409Z","tags":null},"/term/data-quality":{"title":"What is Data Quality?","content":"Data quality is the process of ensuring data meets expectations.\n\nThere are three main ways to detect a data quality issue:¬†\n-   A business user reports an issue.\n-   A data test fails.\n-   Data monitoring raises an alert.\n\n![](images/data-quality.png)\n\nRead more on [Why is data quality harder than code quality?](https://airbyte.com/blog/data-quality-issues).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/data-swamp":{"title":"What is a Data Swamp?","content":"Data swamps start to arise when there is a lack of responsibilities, data ownership, availability, and data governance. ¬†It's when a¬†[Data Lake](term/data%20lake.md)¬†is unmanaged or unable to provide value. Sometimes a Data Swamp can also arise from a¬†[Data Warehouse](term/data%20warehouse.md)¬†due to existing hybrid models.","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/data-transformation":{"title":"What is data transformation?","content":"\nData transformation is the process of converting data from one format into a different format. Reasons for doing this could be to optimize the data for a different use case than it was originally intended for, or to meet the requirements for storing data in a different system. Data transformation may involve steps such as cleansing, normalizing, [structuring](term/structured%20data.md), validation, sorting, joining, or [enriching](term/data%20enrichment.md) data. \n\n## How is data transformation done\nData is often transformed as part of an [ETL (Extract, Transform, Load)](term/etl.md) or [ELT (Extract, Load, Transform)](term/elt.md) approach to [data integration](term/data%20integration.md). \n\nSee [ETL vs. ELT](term/etl%20vs%20elt.md) for a comparison of these two approaches.  \n\nAdditionally, a hybrid approach has recently emerged which is known as [EtLT (Extract, ‚Äútweak‚Äù, Load, Transform)](term/etlt.md). This combines aspects of both ETL and ELT. \n\n## Benefits of data transformation\nWhen used correctly, data transformation can provide the following benefits:\n\n- Improved query-time efficiency and speed. \n- Conversion of data into a format that is required by a target system.\n- Enrichment of data with additional information that allows insights to be more easily extracted.\n- Improved data quality by validating and fixing data, and removal of duplicates. \n\n## Examples of data transformation\nBelow are some examples of how data may be transformed to achieve some of the benefits mentioned above.\n\n### Improved efficiency and speed\nOne kind of transformation could be the extraction of structured data from data that is stored in a string. Imagine data that looks as follows: \n\n```\ninput_string: \"Bob is 29\"\n```\n\nIn order to efficiently process this data in the future, it may preferable to transform this data into additional/new fields, and store it as:\n\n```\nname: \"Bob\"\nage: 29\n```\n\nStoring the data in this manner makes it much more efficient to analyze with operations such as:\n\n```sql\nSELECT * FROM X where Age=29\n```\n\n### Enriching data\n[Data enrichment](data%20enrichment.md) is a data transformation that adds additional information to the data that makes new kinds of queries possible.  \n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/data-virtualization":{"title":"What is Data Virtualization?","content":"Data Virtualization helps you when you have many source systems from different technologies, but all of them are rather fast in response time, and if you don't run a lot of operational applications. In that way, you don't move and copy data around and pre-aggregate, but you have a [Semantic Layer](term/metrics%20layer.md) where you create your business models (like cubes), and only if you query this data virtualization layer does it query the data source. If you use, e.g.¬†[Dremio](https://www.dremio.com/), there you use¬†[Apache Arrow](term/apache%20arrow.md)¬†technology which will cache and optimize a lot in-memory for you that you have as well as stonishing fast response times.\n\nIt's tightly connected to [Data Federation](term/data%20federation.md) and [Push-Downs](term/push-down.md).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/data-warehouse":{"title":"What is a Data Warehouse?","content":"A Data Warehouse, in short DWH, also known as an¬†Enterprise Data Warehouse¬†(EDW), is the traditional way of collecting data as we do¬†[since 30+ years](https://tdwi.org/articles/2016/02/01/data-warehousing-30.aspx). The DWH serves to be the [data integration](term/data%20integration.md) from many different sources, the single point of truth and the data management, meaning cleaning, historizing, and data joined together. It provides greater executive insight into corporate performance with management Dashboards, Reports, or Ad-Hoc Analyses.\n\nVarious types of¬†business data are analyzed¬†with Data Warehouses. The need for it often becomes evident when analytic requirements run afoul of the ongoing performance of operational databases. Running a complex query on a database requires the database to enter a temporarily fixed state. It is often untenable for transactional databases. A data warehouse is employed to do the analytical work, leaving the transactional database free to focus on transactions.\n\nThe other characteristic is analyzing data from multiple origins (e.g., your Google Analytics with your CRM data). It is highly [transformed](term/data%20transformation.md) and structured due to the¬†[ETL (Extract Transform Load)](term/etl.md) process.\n\nIf you wonder about the difference between a Data Warehouse, Data Lake, and a Lakehouse, read more on our [Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/database-normalization":{"title":"What is Normalization?","content":"\nNormalization is used in relational database design to reduce data redundancy and improve data integrity. Developed by British computer scientist [Edgar F. Codd ](https://en.wikipedia.org/wiki/Edgar_F._Codd) in the 1970s as part of his relational model, normalization involves organizing the columns (attributes) and tables (relations) in a database to ensure proper enforcement of dependencies through database integrity constraints. \n\nThis is achieved by applying formal rules during the synthesis (creation of a new database design) or decomposition (improvement of an existing database design) process.\n\n1.  **First Normal Form (1NF)**:\n    - Eliminate duplicate data by ensuring each attribute contains only atomic values and each table has a unique primary key.\n2.  **Second Normal Form (2NF)**:\n    - Meet all requirements of 1NF and remove partial dependencies by ensuring that every non-prime attribute (attribute not part of any candidate key) entirely depends on the primary key.\n3.  **Third Normal Form (3NF)**:\n    - Meet all requirements of 2NF and remove transitive dependencies by ensuring that no non-prime attribute is transitively dependent on the primary key.\n\n## Denormalization\n**Denormalization**, on the other hand, is the process of intentionally introducing redundancy into a database design by combining tables or adding redundant data, aiming to improve query performance or simplify the database structure. Denormalization is the **opposite of normalization**. Please consider the trade-offs between data integrity and query performance. This technique is used with [Dimensional Modeling](term/dimensional%20modeling.md) in [OLAP](term/olap%20(online%20analytical%20processing).md) cubes, for example.\n\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/declarative":{"title":"What is declarative?","content":"A¬†**[declarative](term/declarative.md)**¬†data pipeline does not tell the order it needs to be executed but instead allows each step/task to find the best time and way to run. The declarative approach describes *what* the program does without explicitly specifying its control flow. [Functional Data Engineering](term/functional%20data%20engineering.md) and [Functional Programming](term/functional%20programming.md) is a¬†**declarative**¬†programming paradigm, in contrast to¬†**[imperative](term/imperative.md)**¬†programming paradigms.\n\n## Declarative vs Imperative\nDeclarative approaches appeal because they make systems easier to debug and automate. It's done by explicitly showing intention and offering a simple way to manage and apply changes. By explicitly declaring how the pipeline should look, for example,¬†**defining the data products that should exist**, it becomes much easier to discover when it does not look like that, the reason why, and reconcile. It's the foundation layer for your entire platform's lineage, observability, and¬†[data quality](https://airbyte.com/blog/data-quality-issues)¬†monitoring.\n\n![](images/declarative-vs-imperative.png)\n\nRead more on [Data Orchestration Trends: The Shift From Data Pipelines to Data Products](https://airbyte.com/blog/data-orchestration-trends).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/delta-lake":{"title":"What is Delta Lake?","content":"Delta Lake is an open-source¬†[Data Lake Table Format](term/data%20lake%20table%20format.md)¬†project created by Databricks and kindly open-sourced with its¬†[first public GitHub Commit](https://github.com/delta-io/delta/commit/14cb4e0267cc188e0fdd47e5b4f0235baf87874e)¬†on 2019-04-22. Recently announced¬†[Delta Lake 2.0](https://www.databricks.com/blog/2022/06/30/open-sourcing-all-of-delta-lake.html).\n\nRead more about how to build a Data Lake on top of it on our¬†[Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi) or if you are curious to build a Delta Lake destination with Airbyte [Load Data into Delta Lake on Databricks Lakehouse](https://airbyte.com/tutorials/load-data-into-delta-lake-on-databricks-lakehouse).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/dev-ops":{"title":"What is DevOps?","content":"DevOps¬†is a combination of software developers (dev) and operations (ops). It is defined as a software engineering methodology that aims to integrate the work of software development and software operations teams by facilitating a culture of collaboration and shared responsibility.\n\nIs also related to [DataOps](term/data%20ops.md)","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/dimensional-modeling":{"title":"What is Dimensional Modeling?","content":"Dimensional modeling (DM) is part of the Business Dimensional Lifecycle methodology developed by [[Ralph Kimball]], which includes a set of methods, techniques, and concepts for use in [Data Warehouse](term/data%20warehouse.md) design.\n\nAs a bottom-up approach, the approach focuses on identifying the critical business processes within a business and modeling and implementing these before adding additional business processes.‚Ää An alternative approach from [[Bill Inmon]] advocates a top-down design of the model of all the enterprise data using tools such as Entity-Relationship Modeling (ER).\n\nRead more on [Data Modeling with SQL and dbt](https://airbyte.com/blog/sql-data-modeling-with-dbt).\n\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/dimensions":{"title":"What are Dimensions?","content":"\nDimensions are the categorical buckets that can be used to segment, filter, or group‚Äîsuch as sales amount region, city, product, color, and distribution channel. Traditionally known from [OLAP](term/olap%20(online%20analytical%20processing).md) cubes with [Bus Matrixes](term/bus%20matrix.md).\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/duckdb":{"title":"What is DuckDB?","content":"\n[DuckDB](https://duckdb.org/) is an in-process SQL [OLAP](term/olap%20(online%20analytical%20processing).md) database management system. It has strong support for SQL. DuckDB is borrowing the SQLite shell implementation. Each database is a single file on disk. It's analogous to \"[SQLite](https://www.sqlite.org) for **analytical (OLAP) workloads**\" (direct comparison on the [SQLite vs DuckDB paper](https://simonwillison.net/2022/Sep/1/sqlite-duckdb-paper/) here), whereas SQLite is for OLTP ones. But it can handle vast amounts of data locally. It's the smaller, lighter version of [Apache Druid](Apache%20Druid) and other OLAP technologies.\n\nIt's designed to work as an embedded library, eliminating the network latency you usually get when talking to a database.\n\n\u003e [!note] Skip working with error-prone Excels or CSVs directly\n\u003e\n\u003e With DuckDB, we no longer need to use plain files (CSV, Excel, Parquet). DuckDB supports schema, types, and SQL interface and is super fast. \n\n## Use-Cases\n- Ultra-fast analytical use-case locally. E.g., the Taxi example includes a 10 Year, 1.5 Billion row Taxi data example that still works on a laptop. See benchmarks below. \n- It can be used as an SQL wrapper with zero copies (on top of parquets in S3). \n- Bring your **data to the users** instead of having big roundtrips and latency by doing REST calls. Instead, you can put data inside the client. You can do 60 frames per second as data is where the query is.\n- DuckDB on Kubernetes for a zero-copy layer to read S3 in the [Data Lake](https://digital-garden.erdrix.konpyutaika.com/term/data-lake)! Inspired by [this](https://twitter.com/Ubunta/status/1584907743391272961) Tweet. The cheapest and fastest option to get started.\n\nCheck out [Rill Data](https://www.rilldata.com/), a [BI tool](term/business%20intelligence%20tools.md) that delivers sub-second interactivity because it‚Äôs backed by¬†DuckDB (and¬†[Druid](Apache%20Druid)¬†for enterprise-grade cloud services).\n\n[MotherDuck](https://motherduck.com/) is the managed service around DuckDB that lets you scale from a local DB to a cloud DB and hybrid‚Äîdone by one of [Google BigQuery](Google%20BigQuery) creators or developers such as [[Jordan Tigani]]. Check his discussion on the [Analytics Engineering Podcast about The Personal Data Warehouse](https://open.spotify.com/episode/3CmeFOuIOg91xApdjbWqey?si=CmelGaxBTZ-Z-BR3fvMjmg\u0026utm_source=copy-link\u0026nd=1). The stimulating conversation around connected WebAssembly, e.g., Is an application compiled to C code, which is super fast. E.g., Figma is using that, which would otherwise never work in a browser. \n\n## Benchmarks\n- [Fast Analysis With DuckDB + Pyarrow](https://tech.gerardbentley.com/python/data/intermediate/2022/04/26/holy-duck.html) - 2022\n- [SQL on Python, part 1: The simplicity of DuckDB](https://www.orchest.io/blog/sql-on-python-part-1-the-simplicity-of-duckdb): How to use DuckDB to analyze 4.6+ million mentions of climate change on Reddit\n- [Tweet](https://mobile.twitter.com/medriscoll/status/1554698141789614081): Impressively fast, collaborative exploratory data analytics over a 20+ million row data set, hosted in the cloud with Drifting‚Äôs Jamsocket + Rill Data + DuckDB - 2022\n- [Taking DuckDB for a spin | Uwe‚Äôs Blog](https://uwekorn.com/2019/10/19/taking-duckdb-for-a-spin.html) - 2019\n- [SQLite vs DuckDB paper](https://simonwillison.net/2022/Sep/1/sqlite-duckdb-paper/): \n  - SQLite out-performs DuckDB on a write transactions benchmark by 10x-500x on a powerful cloud server and 2x-60x on a Raspberry Pi, for small to large databases.\n  - For analytical benchmarks using the SSB (Star Schema Benchmark) DuckDB out-performs SQLite by 30-50x at the highest margin and 3-8x at the lowest.\n\n## Example Projects\n- [Modern Data Stack in a Box with DuckDB](https://duckdb.org/2022/10/12/modern-data-stack-in-a-box.html): A fast, free, and open-source Modern Data Stack (MDS) can now be fully deployed on your laptop or to a single machine using the combination of DuckDB, Meltano, dbt, and Apache Superset. \n- [Fully Featured Project Example](https://github.com/dagster-io/dagster/blob/master/examples/project_fully_featured/) (s3, dbt, parquet, and snowflake) reading from Hackernews orchestrated with dagster.\n- [Data Engineering in 2022: Exploring dbt with DuckDB](https://rmoff.net/2022/10/20/data-engineering-in-2022-exploring-dbt-with-duckdb/): ¬†Step-by-step guide on using dbt and DuckDB.\n- [Build a poor man‚Äôs data lake from scratch with DuckDB](https://dagster.io/blog/duckdb-data-lake): Full fleshed example by Dagster, also in [video](https://youtu.be/33sxkrt6eYk).\n- [Using Polars on results from DuckDB's Arrow interface in Rust](https://vikramoberoi.com/using-polars-on-results-from-duckdbs-arrow-interface-in-rust/)\n\n## Tech and Papers\nIt ships as an [amalgamation](https://www.sqlite.org/amalgamation.html) build - a single giant C++ file (SQLite is a single giant C file). And it's also backed up by some strong computer science. It's by the academic researchers behind MonetDB and includes implementations of a bunch of interesting papers:\n-   [Data Management for Data Science - Towards Embedded Analytics](https://www.duckdb.org/pdf/CIDR2020-raasveldt-muehleisen-duckdb.pdf)¬†(CIDR 2020)\n-   [DuckDB: an Embeddable Analytical Database](https://www.duckdb.org/pdf/SIGMOD2019-demo-duckdb.pdf)¬†(SIGMOD 2019 Demo)\n\nFrom [Why DuckDB](https://duckdb.org/why_duckdb):\n\u003e DuckDB contains a¬†**columnar-vectorized query execution engine**, where queries are still interpreted, but a large batch of values (a ‚Äúvector‚Äù) is processed in one operation. This dramatically reduces overhead in traditional systems such as PostgreSQL, MySQL, or SQLite, which process each row sequentially. Vectorized query execution leads to far better performance in OLAP queries.\n\n\n## Python API and Handling Data\nFrom [DuckDB Docs - Python API](https://duckdb.org/docs/api/python)\n### Fetching Data with SQL\n```python\n# fetch as pandas data frame\ndf = con.execute(\"SELECT * FROM items\").fetchdf()\nprint(df)\n#        item   value  count\n# 0     jeans    20.0      1\n# 1    hammer    42.2      2\n# 2    laptop  2000.0      1\n# 3  chainsaw   500.0     10\n# 4    iphone   300.0      2\n\n# fetch as dictionary of numpy arrays\narr = con.execute(\"SELECT * FROM items\").fetchnumpy()\nprint(arr)\n# {'item': masked_array(data=['jeans', 'hammer', 'laptop', 'chainsaw', 'iphone'],\n#              mask=[False, False, False, False, False],\n#        fill_value='?',\n#             dtype=object), 'value': masked_array(data=[20.0, 42.2, 2000.0, 500.0, 300.0],\n#              mask=[False, False, False, False, False],\n#        fill_value=1e+20), 'count': masked_array(data=[1, 2, 1, 10, 2],\n#              mask=[False, False, False, False, False],\n#        fill_value=999999,\n#             dtype=int32)}\n```\n### Create Table\n```python\n# create a table\ncon.execute(\"CREATE TABLE items(item VARCHAR, value DECIMAL(10,2), count INTEGER)\")\n# insert two items into the table\ncon.execute(\"INSERT INTO items VALUES ('jeans', 20.0, 1), ('hammer', 42.2, 2)\")\n\n# retrieve the items again\ncon.execute(\"SELECT * FROM items\")\nprint(con.fetchall())\n# [('jeans', 20.0, 1), ('hammer', 42.2, 2)]\n```\n### Insert Data\n```python\n# insert a row using prepared statements\ncon.execute(\"INSERT INTO items VALUES (?, ?, ?)\", ['laptop', 2000, 1])\n\n# insert several rows using prepared statements\ncon.executemany(\"INSERT INTO items VALUES (?, ?, ?)\", [['chainsaw', 500, 10], ['iphone', 300, 2]] )\n\n# query the database using a prepared statement\ncon.execute(\"SELECT item FROM items WHERE value \u003e ?\", [400])\nprint(con.fetchall())\n# [('laptop',), ('chainsaw',)]\n```\n\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/elt":{"title":"What is ELT?","content":"ELT (Extract, Load, and [Transform](term/data%20transformation.md)) is a [data integration](term/data%20integration.md) approach that extracts (E) data from a source system, and loads (L) raw data into a destination system before it transforms (T) the data. In other words, in the ELT approach, transformation (T) of the data is done _within_ the destination [Data Warehouse](term/data%20warehouse.md) after data has been loaded. \n\nELT is in contrast to the more traditional [ETL](term/etl.md) data integration approach, in which data is transformed before it arrives at the destination. See [ETL vs ELT](term/etl%20vs%20elt.md) for a more detailed comparision of these approaches.\n\nThe shift from the ETL paradigm to the ELT paradigm has been made possible thanks to the plummeting cost of cloud-based computation and storage, and the appearance of cloud-based data warehouses like Redshift, BigQuery, or Snowflake. \n\nThe following image demonstrates the ELT approach to data integration -- in this diagram [dbt](https://docs.getdbt.com/docs/introduction) creates and manages the SQL that is used for transforming the data in the destination:\n\n![](images/elt-tool.png)\n\nELT is also related to [Reverse ETL](term/reverse%20etl.md) which you can find more information about at: [Reverse ETL Explained](https://airbyte.com/blog/reverse-etl#so-what-is-a-reverse-etl) or [Airbyte.com](https://airbyte.com). \n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/etl":{"title":"What is ETL?","content":"ETL (Extract, [Transform](term/data%20transformation.md), and Load) is a paradigm for moving data from one system to another. It involves reading data (Extract) from one system, modifying the data (Transform), and then sending it (Load) to a destination system. The ETL paradigm emerged in the 1970s. \n\nA key feature of ETL is that data is transformed before being sent to the destination, as demonstrated in the following image:\n\n![](images/etl-tool.png)\n\nHowever in recent years, the preferred data movement paradigm has shifted to [ELT](term/elt.md) (Extract, Load, and Transform). The ELT philosophy dictates that data should be untouched ‚Äì apart from minor cleaning and filtering ‚Äì as it moves through the extraction and loading stages so that the raw data is always accessible in the destination [Data Warehouse](term/data%20warehouse.md). See [ETL vs ELT](term/etl%20vs%20elt.md) for a comparison of these approaches.\n\n\n## ETL is Changing\nThe way we do ETL is changing. For a long time ETL was done with tools such as Informatica, IBM Datastage, Cognos, AbInitio, or Microsoft SSIS. Today we use more programmatic or configuration-driven platforms like [[Airflow]], [[Dagster]], and [Temporal](term/temporal.md). \n\nHistorically¬†**ETL was once preferred**¬†over ELT for the following¬†**no-longer-valid reasons**:¬†\n- ETL could achieve cost savings by removing unwanted data before sending it to the destination ‚Äì¬† however, with the plummeting cost of cloud-based computation and storage the value of this proposition is greatly reduced.¬†\n- Because ETL transforms data before it is stored, it avoids the complexity of transforming data¬†_after_¬†sending it to the destination ‚Äì however, new tools such as¬†[[dbt]]¬†(data build tool) make it preferable and easy to transform data in the destination.\n\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/etl-elt-airbyte":{"title":"ETL and ELT with Airbyte","content":"[ELT](term/elt.md) and [ETL](term/etl.md) specific to Airbyte mean:\n- **Extract**: Retrieve data from a¬†[source](https://docs.airbyte.com/integrations/#Sources), which can be an application, database, or anything really.\n- **Load**: Move data to your¬†[destination](https://docs.airbyte.com/integrations/#Destinations).\n- **Transform**: Clean up the data. This is referred to as¬†[normalization](term/normalization.md)¬†in Airbyte and involves [incremental synchronization](term/incremental%20synchronization.md) and¬†[deduplication](https://docs.airbyte.com/understanding-airbyte/connections/incremental-deduped-history), changing data types, formats, and more.","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/etl-vs-elt":{"title":"ETL vs. ELT","content":"[ETL](term/etl.md)¬†(Extract, Transform, and Load) and [ELT](term/elt.md) (Extract, Load, and Transform) are two paradigms for moving data from one system to another.¬†The main difference between them is that when an ETL approach is used, data is transformed before it is loaded into a destination system. On the other hand, in the case of ELT, any required transformations are done after the data has been written to the destination and are _then_ done _inside_ the destination -- often by executing SQL commands. The difference between these approaches is easier to understand by a visual comparison of the two approaches. \n\nThe image below demonstrates the ETL approach to [data integration](term/data%20integration.md):\n\n![](images/etl-tool.png)\n\nWhile the following image demonstrates the ELT approach to data integration:\n\n![](images/elt-tool.png)\n\nETL was originally used for [Data Warehousing](term/data%20warehouse.md) and ELT for creating a [Data Lake](term/data%20lake.md). \n\n## Disadvantages of ETL compared to ELT\n\n**ETL**¬†has several¬†**disadvantages compared to ELT**, including the following:\n\n- Generally, only transformed data is stored in the destination system, and so analysts must know beforehand every way they are going to use the data, and every report they are going to produce.¬†¬†\n- Modifications to requirements can be costly, and often require re-ingesting data from source systems.\n- Every transformation that is performed on the data may obscure some of the underlying information, and analysts only see what was kept during the transformation phase.¬†\n- Building an ETL-based data pipeline is often beyond the technical capabilities of analysts.\n\nFind more on [An overview of Airbyte‚Äôs replication modes](https://airbyte.com/blog/understanding-data-replication-modes).\n\n## ELT/ETL Tool Comparision\nNeed to find the best data integration tool for your business? Which platform integrates with hour data sources and destinations? Which one provides the features you‚Äôre looking for?  \n  \nWe made it simple for you. Here‚Äôs a¬†[spreadsheet](https://docs.google.com/spreadsheets/d/1QKrtBpg6PliPMpcndpmkZpDVIz_o6_Y-LWTTvQ6CfHA/edit?usp=sharing)¬†with a comparison of all those actors. Or an extensive detailed comparison between the tools on [Top ETL tools compared in details](https://airbyte.com/etl-tools-comparison).\n\nSee also more on [Airbyte.com](https://airbyte.com) or [Reverse ETL Explained](https://airbyte.com/blog/reverse-etl#so-what-is-a-reverse-etl).\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/etlt":{"title":"What is EtLT?","content":"EtLT refers to Extract, ‚Äútweak‚Äù, Load, [Transform](term/data%20transformation.md), and can be thought of an extension to the [ELT](term/elt.md) approach to [data integration](term/data%20integration.md). \n\nWhen compared to ELT, the EtLT approach incorporates an additional light ‚Äútweak‚Äù (small ‚Äút‚Äù) transformation, which is done on the data after it is extracted from the source and before it is loaded into the destination. This is demonstrated in the following image:\n\n![](images/etlt-extract-tweak-load-transform.png)\n\nFor a more detailed explanation of EtLT see: [EtLT for improved GDPR compliance](https://airbyte.com/blog/etlt-gdpr-compliance).\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/full-refresh-synchronization":{"title":"What is Full Refresh Synchronization?","content":"A Full Refresh Sync will attempt to retrieve all data from the source every time a sync is run. Then there are two choices, *Overwrite* and *Append*. Overwrite deletes the data in the destination before running the sync and Append doesn't.\n\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/functional-data-engineering":{"title":"What is Functional Data Engineering?","content":"Functional Data Engineering brings¬†_clarity_. When functions are \"pure,\" they do not have side effects. They can be written, tested, reasoned about, and debugged in isolation without understanding the external context or history of events surrounding their execution. Its¬†[Functional Programming](term/functional%20programming.md) applied to the field of data engineering initiated by [Maxime Beauchemin](term/maxime%20beauchemin.md) with¬†[Functional Data Engineering ‚Äî a modern paradigm for batch data processing](https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a).\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/functional-programming":{"title":"What is Functional Programming?","content":"\nFunctional Programming is a style of building functions that threaten computation as a mathematical function that avoids changing state and mutable data. It is a declarative programming paradigm, which means programming expressive and [declarative](term/declarative.md) as opposed to imperative. It's getting more popular with the rise of [Functional Data Engineering](term/functional%20data%20engineering.md).\n\nSee also [Programming Languages](term/programming%20languages.md).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/general-infos":{"title":"General Infos (Folder Structure, Links)","content":"[Quartz](https://quartz.jzhao.xyz) runs on top of¬†[Hugo](https://gohugo.io/)¬†so all notes are written in¬†[Markdown](https://www.markdownguide.org/getting-started/) and can be edited through any editor.\n\n## Folder Structure\nThe content of the Glossary is in `content/term` folder. The rest outside of `content/` is the website/framework.\n\nTo edit the main home page, open¬†`/content/_index.md`.\n## Links\nTo create a link between terms in the glossary, just create a normal link using Markdown pointing to the document in question. Please note that¬†**all links should be relative to the root¬†`/content`¬†path**.\n```markdown\nFor example, I want to link this current document to `term/config.md`.\n[A link to the config page](term/config.md)\n```\n\nSimilarly, you can put local images anywhere in the¬†`/content`¬†folder.\n```markdown\nExample image (source is in content/images/example.png)\n![Example Image](/content/images/example.png)\n```\n\n## Lower Case\nTerms are lower case that links are also lowercase. When we create a link to a term, I usually capitalize the beginning of each word to make it look nice. E.g `[Apache Arrow](term/apache%20arrow.md)`. Other such as YAML I write all in capitals.\n\nWe didn't activate wikilinks, but that would be an option as well. See more on [editing](https://quartz.jzhao.xyz/notes/editing/).\n## Metatag with Front Matter\nHugo is picky when it comes to metadata for files. Make sure that your title is double-quoted and that you have a title defined at the top of your file like so. You can also add tags here as well.\n```yaml\n---\ntitle: \"What is a Glossary?\"\ntags:\n- example-tag\n- here i can add more we keep it lower case\nurl: \"term/my-other-domain\"\naliases:\n- Digital Garden\n- Second Brain\n---\n\nRest of your content here.\n```\n\n- `url`: this is not needed, only if the default link (name of the note) is not sufficient\n\t- all spaces will be replaced with `-` (dash).\n- `aliases`: Are like tags, you can add multiple and they will be linkable same as a adding a new term would be.\n\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/granularity":{"title":"What is Granularity","content":"Declaring the¬†granularity (or _grain_)¬†is the pivotal step in [Dimensional Modeling](term/dimensional%20modeling.md). The grain¬†establishes exactly what a single fact table row represents. The grain declaration becomes a binding contract on the design. The grain must be declared before choosing [[dimensions]] or [[facts]] because every candidate dimension or fact must be consistent with the grain. This consistency enforces uniformity on all dimensional designs which is critical to [Business Intelligence](term/business%20intelligence.md) application performance and ease of use.\n\nFor example, in the **transformation layer**, you must balance low and high granularity. What level do you aggregate and store (e.g., [rollups](term/rollup.md) hourly data to daily to save storage), or what valuable dimensions to add. With each dimension and its column added, rows will [explode](https://www.ibm.com/docs/en/ida/9.1.1?topic=phase-step-identify-measures#c_dm_design_cycle_4__c_dm_4_step7) exponentially, and we can‚Äôt persist each of these representations to the filesystem.\n\nA [Semantic Layer](term/semantic%20layer.md) is much more flexible and makes the most sense on top of [transformed data](term/data%20transformation.md) in a [Data Warehouse](term/data%20warehouse.md). Avoid extensive reshuffles or reprocesses of large amounts of data. Think of [OLAP](term/olap%20(online%20analytical%20processing).md) cubes where you can dice-and-slice ad-hoc on significant amounts of data without storing them ahead of time\n\nRead more on [Kimball Dimensional Modeling Techniques](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/grain/). Also related is [Rollup](term/rollup.md).\n\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/idempotency":{"title":"What is Idempotency?","content":"Idempotency is the property of a particular operation that can be applied multiple times without changing the resulting outcome by being given the same inputs. It is used in¬†[Functional Programming](term/functional%20programming.md)¬†and was the foundation for¬†[Functional Data Engineering](term/functional%20data%20engineering.md).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/imperative":{"title":"What is imperative?","content":"\nAn¬†**imperative**¬†pipeline tells¬†_how_¬†to proceed at each step in a procedural manner. In contrast, a¬†**[declarative](term/declarative.md)**¬†data pipeline does not tell the order it needs to be executed but instead allows each step/task to find the best time and way to run. \n\nThe *how* should be taken care of by the tool, framework, or platform running on. For example, update an asset when upstream data has changed. Both approaches result in the same output. However, the declarative approach benefits from¬†**leveraging compile-time query planners**¬†and¬†**considering runtime statistics**¬†to choose the best way to compute and find patterns to reduce the amount of transformed data.\n\nRead more on [Data Orchestration Trends: The Shift From Data Pipelines to Data Products](https://airbyte.com/blog/data-orchestration-trends).\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/in-memory-format":{"title":"What is an In-Memory Format?","content":"\nIn-memory formats are optimized to:\n- hit fast instruction sets \n- be cache friendly \n- be parallelizable\n\nFormats:\n- [Apache Arrow](term/apache%20arrow.md) \n- [Apache Spark](Apache%20Spark) [[DataFrames]]\n- [NumPy](term/numpy.md)\n- [Pandas](term/pandas.md)\n\nThe opposed to in-memory formats are [Data Lake File Formats](Data%20Lake%20File%20Formats) which save space, be cross-language and serve as long-term storage. More on the Data AI Summit talk about [From bits to Data Frames](https://microsites.databricks.com/sites/default/files/2022-07/Sound-Data-Engineering-in-Rust_From-Bits%20to-DataFrames.pdf) by Jorge C Leitao.\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/incremental-synchronization":{"title":"What is Incremental Synchronization?","content":"Incremental synchronization is a process which efficiently copies data to a destination system by periodically executing queries on a source system for records that have been updated or inserted since the previous sync operation. Only those records that have been recently inserted or updated will be sent to the destination, which is much more efficient than copying an entire data set on each sync operation. Incremental synchronization makes use of a cursor field such as `updated_at` (or whatever you wish to call the field) to determine which records should be propagated, and only records with an `updated_at` value that is newer than the `updated_at` value of the most recent record sent in the previous sync should be replicated.\n\nHowever, without special consideration, records that have been deleted in the source system will not be propagated to the destination as they will never appear in the results from such a query. This may be addressed by¬†[Soft Deletes](term/soft%20delete.md)¬†or by making use of¬†[CDC¬†replication](https://airbyte.com/blog/change-data-capture-definition-methods-and-benefits).\n\nRead more on [Incremental data synchronization between Postgres databases](https://airbyte.com/tutorials/incremental-data-synchronization) or see related [Full Refresh Synchronization](term/full%20refresh%20synchronization.md).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/jinja-template":{"title":"What is a Jinja Template?","content":"\nJinja is a fast, expressive, extensible templating engine. Special placeholders in the template allow writing code similar to [Python](term/python.md) syntax. Then the template is passed data to render the final document.\n\nMost popularized by [[dbt]].  Read more on the [Jinja Documentation](https://jinja.palletsprojects.com/).\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/key-performance-indicator-kpi":{"title":"What is Key Performance Indicator (KPI)?","content":"A performance indicator or key performance indicator (KPI) is a type of performance measurement. KPIs evaluate the success of an organization or of a particular activity (such as projects, programs, products, and other initiatives) in which it engages. \n\nSee more on [What is a Metric](term/metric.md), as it's a synonym with a KPI.\n\n\n\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/kubernetes":{"title":"What is Kubernetes?","content":"It‚Äôs a platform that allows you to run and orchestrate container workloads.¬†[**Kubernetes**](https://stackoverflow.blog/2020/05/29/why-kubernetes-getting-so-popular/)¬†**has become the de-facto standard**¬†for your cloud-native apps to (auto-)¬†[scale-out](https://stackoverflow.com/a/11715598/5246670)¬†and deploy your open-source zoo fast, cloud-provider-independent. No lock-in here. Kubernetes is the¬†**move from infrastructure as code**¬†towards¬†**infrastructure as data**, specifically as¬†[YAML](term/yaml.md). With Kubernetes, developers can quickly write applications that run across multiple operating environments. Costs can be reduced by scaling down.","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/lambda-architecture":{"title":"What is a Lambda Architecture?","content":"Lambda architecture¬†is a¬†data-processing¬†architecture designed to handle massive quantities of data by taking advantage of both¬†batch¬†and¬†stream-processing¬†methods. This approach to architecture attempts to balance¬†latency,¬†throughput, and¬†fault tolerance using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data. The two view outputs may be joined before the presentation. The rise of lambda architecture is correlated with the growth of¬†big data, real-time analytics, and the drive to mitigate the latencies of¬†[MapReduce](term/map%20reduce.md).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/machine-learning":{"title":"What is Machine Learning?","content":"Machine learning (ML) is a type of artificial intelligence (AI) that allows software applications to become more accurate at predicting outcomes without being explicitly programmed to do so. Machine learning¬†[algorithms](https://www.techtarget.com/whatis/definition/algorithm)¬†use historical data as input to predict new output values.\n\nMore on [Machine Learning  | Tech Target](https://www.techtarget.com/searchenterpriseai/definition/machine-learning-ML).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/map-reduce":{"title":"What is MapReduce?","content":"MapReduce is a programming paradigm that enables massive scalability across hundreds or thousands of servers in a Hadoop cluster. As the processing component, MapReduce is the heart of [Apache Hadoop](term/apache%20hadoop.md). The term \"MapReduce\" refers to two separate and distinct tasks that Apache Hadoop programs perform.","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/master-data-management-mdm":{"title":"What is Master Data Management (MDM)?","content":"Master data management is a method to centralize master data. It's the bridge between the business that maintain the data and know them best and the data folks, and it's a tool of choice. It helps with uniformity, accuracy, stewardship, semantic consistency, and accountability of mostly enterprise master [Data Assets](term/data%20asset.md).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/maxime-beauchemin":{"title":"Maxime Beauchemin","content":"Creator of [[Apache Airflow]] and [Apache Superset](term/apache%20superset.md).\n\nStarted as [Business Intelligence](term/business%20intelligence.md) Engineer and is working now at [[Preset]] (Superset as a Service).\n\nStarter of [idempotency](term/idempotency.md) and [functional data engineering](term/functional%20data%20engineering.md).\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/metric":{"title":"What is a Metric?","content":"\nA Metric, also called [KPI](term/key%20performance%20indicator%20(kpi).md) or (calculated) measure, are terms that serve as the building blocks for how business performance is both measured and defined, as knowledge of how to define an organization's KPIs. It is fundamental to have a common understanding of them. Metrics usually surface as business reports and dashboards with direct access to the entire organization.\n\nFor example, think of operational metrics that represent your company's performance and service level or financial metrics that describe its financial health. Today these metrics are primarily defined in a lengthy [SQL](term/sql.md) statement inside the [BI tools](term/business%20intelligence%20tools.md).¬†\n\nCalculated measures are part of metrics and apply to specific [dimensions](term/dimensions.md) traditionally mapped inside a [Bus Matrix](term/bus%20matrix.md). Dimensions are the categorical buckets that can be used to segment, filter, group, slice, and dice‚Äîsuch as sales amount, region, city, product, color, and distribution channel. Dimensions (and facts) are also known from the concept of [Dimensional Modeling](term/dimensional%20modeling.md). \n\nSee more on [Semantic Layer](term/semantic%20layer.md) or [Metrics Layer](term/metrics%20layer.md).\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/metrics-layer":{"title":"What is a Metrics Layer?","content":"\u003e [!info] Similarities to a Semantic Layer\n\u003e \n\u003e The metrics layer is one component of a [Semantic Layer](term/semantic%20layer.md). A limited metrics layer is usually built into a [BI tool](term/business%20intelligence%20tools.md), translating its [metrics](term/metric.md) to only that BI tool. \n\nA metrics layer, sometimes also called a metrics store, includes a specification of metrics such as [measures](term/metric.md) and [dimensions](term/dimensions.md). Additionally, it can contain model parsing from files (mostly¬†[YAML](term/yaml.md)) and APIs to create and execute metric logic; some include a cache layer. A metrics layer encourages us to enforce the¬†[DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)¬†(Do not repeat yourself) principle by defining metrics once before populating them to any BI tools used or integrating them into internal applications or processes.\n\nRead more on [Semantic Layer](term/semantic%20layer.md) or [The Rise of the Semantic Layer](https://airbyte.com/blog/the-rise-of-the-semantic-layer-metrics-on-the-fly).\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/modern-data-stack":{"title":"What is the Modern Data Stack?","content":"The Modern Data Stack (MDS) is a heap of open-source tools to achieve end-to-end analytics from ingestion to [transformation](term/data%20transformation.md) to ML over to a columnar data warehouse or lake solution with an analytics BI dashboard backend. This stack is extendable like lego blocks. Usually, it consists of **[data integration](term/data%20integration.md), a transformation tool, an [Orchestrator](term/data%20orchestrator.md), and a [Business Intelligence Tool](term/business%20intelligence%20tools.md)**. With growing data, you might add [Data Quality](term/data%20quality.md) and observability tools, [Data Catalog](term/data%20catalog.md), [Semantic Layer](term/semantic%20layer.md), and more.\n\nIn a way, it is [unbundling](https://blog.fal.ai/the-unbundling-of-airflow-2/) the data stack as Gorkem says:\n\u003e Products start small, in time, add adjacent verticals and functionality to their offerings, and become a platform. Once these¬†**platforms**¬†become big enough, people begin to figure out how to serve better-neglected verticals or abstract out functionality to break it down into purpose-built chunks, and the unbundling starts.\n\nThe goal of an MDS is to get data insight with the best suitable tools for each part. It's noteworthy that it's a relatively new term.\n\n\u003e [!note] New Terms popping up\n\u003e\n\u003e There is already a new term [ngods (new generation open-source data stack)](https://blog.devgenius.io/modern-data-stack-demo-5d75dcdfba50). Or *DataStack 2.0* in Dagster's recent [blog post](https://dagster.io/blog/evolution-iq-case-study).\n\n## The Future of MDS\nIf we look a little in the future, Barr Moses illustrates in her article [What's In Store For The Future Of The Modern Data Stack?](https://www.montecarlodata.com/blog-the-future-of-the-modern-data-stack/) more features such as data sharing, universal [Data Governance](term/data%20governance.md), [Data Lake](term/data%20lake.md), and [Data Warehouse](term/data%20warehouse.md) equalized, and a newer evolution of predictive analysis:\n![](images/future-modern-data-stack.png)","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/normalization":{"title":"Airbyte Normalization","content":"Normalization¬†is the process of structuring data from the source into a format appropriate for consumption in the destination. For example, when writing data from a nested, dynamically typed source like a JSON API to a relational destination like Postgres,¬†normalization¬†is the process that un-nests JSON from the source into a relational table format that uses the appropriate column types in the destination.\n\nRead more on our [docs](https://docs.airbyte.com/cloud/core-concepts#normalization) or what [Database Normalization](term/database%20normalization.md) means in general.\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/notebooks":{"title":"What are Notebooks?","content":"\nData notebooks popularized by Jupyter notebooks are the centralized IDE inside a browser for doing collaborative work.\n\n1. Notebooks that are popularized and in heavy use today.\n\t- [Jupyter Notebook](https://jupyter.org/) and [JupyterHub](https://jupyter.org/hub)\n\t\t- Automation on top of Jupyter notebooks: [Naas](https://github.com/jupyter-naas/awesome-notebooks)\n\t- [Zeppelin](https://zeppelin.apache.org/)\n\t- [Databricks Notebook](https://docs.databricks.com/notebooks/index.html)\n2. Newer versions of Jupyter notebooks with more integrated features and an integrated cloud\n\t- [HEX](https://hex.tech/)\n\t- [Deepnote](https://deepnote.com/)\n\t- [Count.co](https://count.co)\n\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/olap-online-analytical-processing":{"title":"What is OLAP (Online Analytical Processing)?","content":"\nOLAP is an acronym for **Online Analytical Processing**. OLAP performs multidimensional analysis of business data and provides the capability for complex calculations, trend analysis, and sophisticated [data modeling](term/data%20modeling.md). An **OLAP cube** is a multidimensional database that is optimized for [data warehouse](term/data%20warehouse.md) and online analytical processing (OLAP) applications. \n\nAn OLAP cube is a method of storing data in a multidimensional form, generally for reporting purposes. In OLAP cubes, data ([Measures](term/metric.md)) are categorized by [dimensions](term/dimensions.md). \n\nIn order to manage and perform processes with an OLAP cube, Microsoft developed a query language, known as [multidimensional expressions (MDX)](https://learn.microsoft.com/en-us/analysis-services/multidimensional-models/mdx/), in the late 1990s.  Many other vendors of multidimensional databases have adopted MDX for querying data, but with this specific language, management of the cube requires personnel with the skill set.\n\nThe opposite of OLAP is [OLTP](term/oltp%20(online%20transactional%20processing).md). Read more on [Wikipedia](https://en.wikipedia.org/wiki/Online_analytical_processing).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/oltp-online-transactional-processing":{"title":"What is OLTP (Online Transactional Processing)=","content":"In¬†online transaction processing¬†(**OLTP**), information systems typically facilitate and manage **transaction-oriented** applications. It's the opposite of [OLAP (Online Analytical Processing)](term/olap%20(online%20analytical%20processing).md).\n\nRead more on [Wikipedia](https://en.wikipedia.org/wiki/Online_transaction_processing).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/open-data-stack":{"title":"What is the Open Data Stack?","content":"\nThe open data stack is a better term for the [Modern Data Stack](term/modern%20data%20stack.md) but focuses on solutions built on open-source and open standards covering the [Data Engineering Lifecycle](term/data%20engineering%20lifecycle.md). The open data stack is maintained by everyone using it. Companies can reuse existing battle-tested solutions and build on them instead of reinventing the wheel by re-implementing critical components for each component of the data stack.\n\nThe *open* piece is so important and often overlooked because it‚Äôs what makes the #opendatastack more embeddable with tools from the open data stack such as [Airbyte](term/airbyte.md), [dbt](dbt), [Dagster](Dagster), [Superset](term/apache%20superset), and so forth. Letting you integrate them into your services, unlike closed-source services.\n\n\u003e [!example] See a reference project building the open-data-stack\n\u003e\n\u003e This is the start of the open data stack in action. Check out the GitHub repo [Open-Data-Stack](https://github.com/airbytehq/open-data-stack/).\n\nAlternative names that came up beside the Modern Data Stack are [ngods (new generation open-source data stack)](https://blog.devgenius.io/modern-data-stack-demo-5d75dcdfba50), [DataStack 2.0](https://dagster.io/blog/evolution-iq-case-study), [DAD Stack](https://www.reddit.com/r/dataengineering/comments/11fhmqu/comment/jajkydk/?context=3), or more as a joke on Twitter, the boring data stack.\n\nSee more on the topic of [The Open (aka *Modern*) Data Stack Distilled into Four Core Tools](https://airbyte.com/blog/modern-open-data-stack-four-core-tools).\n\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/orc":{"title":"What is ORC?","content":"The¬†**Optimized Row Columnar**¬†(ORC)¬†[Data Lake File Format](term/data%20lake%20file%20format.md)¬†provides a highly efficient way to store Hive data. It was designed to overcome the limitations of the other Hive file formats. Using ORC files improves performance when Hive is reading, writing, and processing data.\n\nRead more about how to build a Data Lake on top of it on our¬†[Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/other-glossaries":{"title":"Other Data Glossaries","content":"Other helpful data glossaries:\n- [Seconda Glossary](https://www.secoda.co/glossary) \n- [Prisma's Data Guide](https://www.prisma.io/dataguide/)\n- [Reddit Data Engineering Wiki](https://dataengineering.wiki/)","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/pandas":{"title":"What is Pandas?","content":"Pandas is a software library written for the [Python](term/python.md) programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. It is free software released under the three-clause BSD license. \n\n## What is a Pandas DataFrame\nTwo-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nThe data structure also contains labeled axes (rows and columns). Arithmetic operations align on both row and column labels. It can be thought of as a dict-like container for Series objects. The primary Pandas data structure.\n\nSee more on [Pandas Documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n\nAnother DataFrame with the same API is [Koalas](https://github.com/databricks/koalas), created by Databricks, optimized for more extensive data sets, and [Apache Spark](term/apache%20spark.md).\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/partial-success":{"title":"Partial Success","content":"A¬†Partial Success¬†indicates that some records were successfully committed to the destination during a sync, even when the overall sync status was reported as a failure.\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/programming-languages":{"title":"What are Data Engineering Programming Languages?","content":"2022 marks JavaScript‚Äôs tenth year in a row as the most commonly used programming language according to [Stack Overflow Developer Survey 2022](https://survey.stackoverflow.co/2022/#section-most-popular-technologies-programming-scripting-and-markup-languages). Further, they say: People learning to code are more likely than Professional Developers to report using Python (58% vs 44%), C++ (35% vs 20%), and C (32% vs 17%).\n\nProgramming Languages (so far):\n- [SQL](term/sql.md)\n- [Python](term/python.md)\n\nSee also [Functional Programming](term/functional%20programming.md) or [Functional Data Engineering](term/functional%20data%20engineering.md).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/push-down":{"title":"What is a Push-Down?","content":"Query pushdown aims to execute as much work as possible in the source databases. \n\nPush-downs or query pushdowns push transformation logic to the source database. This reduces to store data physically and transfers them over the network. \n\nFor example, a [semantic layer](term/semantic%20layer.md) or [data virtualization](term/data%20virtualization.md) translates the transformation logic into [SQL](term/sql.md) queries and sends the SQL queries to the database. The source database runs the SQL queries to process the transformations.\n\nPushdown optimization increases mapping performance when the source database can process transformation logic faster than the semantic layer itself. \n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/python":{"title":"What is Python?","content":"Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured, object-oriented, and [Functional Programming](term/functional%20programming.md).\n\nPython is the de facto standard for [Data Engineering](term/data%20engineering.md) next to [SQL](term/sql.md). If you want to learn Python, see the Freecodecamp Python Course in under 300 hours:\n{{\u003c youtube vMl4YUch7x4 \u003e}}\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/raw-tables":{"title":"Raw Tables","content":"Airbyte spits out tables with the prefix¬†`_airbyte_raw_`. This is your replicated data, but the prefix indicates that it's not normalized. If you select basic [normalization](term/normalization.md), Airbyte will create renamed versions without the prefix.","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/reverse-etl":{"title":"What is Reverse ETL?","content":"Reverse ETL is the flip side of the [ETL](term/etl.md)/[ELT](term/elt.md).¬†**With Reverse ETL, the data warehouse becomes the source rather than the destination**. Data is taken from the warehouse, transformed to match the destination's data formatting requirements, and loaded into an application ‚Äì for example, a CRM like Salesforce ‚Äì to enable action.\n\nIn a way, the Reverse ETL concept is not new to data engineers, who have been enabling data movement warehouses to business applications for a long time. As [Maxime Beauchemin](term/maxime%20beauchemin.md) mentions in¬†[his article](https://preset.io/blog/reshaping-data-engineering/), Reverse ETL ‚Äúappears to be a modern new means of addressing a subset of what was formerly known as¬† [Master Data Management (MDM)](term/master%20data%20management%20(mdm).md).‚Äù\n\nRead more about in [Reverse ETL Explained](https://airbyte.com/blog/reverse-etl#so-what-is-a-reverse-etl).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/rollup":{"title":"What is a Rollup?","content":"Rollup is¬†a form of summarization or pre-aggregation. Rolling up data can dramatically reduce the size of data to be stored and reduce row counts by potential orders of magnitude.\n\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/rust":{"title":"What is Rust?","content":"Former Mozilla employee Graydon Hoare initially created¬†[Rust](https://digital-garden.erdrix.konpyutaika.com/term/rust)¬†as a personal project. The first stable release, Rust 1.0 was released on May 15, 2015. Rust is a¬†[**multi-paradigm programming language**](https://en.wikipedia.org/wiki/Comparison_of_multi-paradigm_programming_languages)¬†that supports imperative procedural, concurrent actor, object-oriented and pure¬†[functional](https://glossary.airbyte.com/term/functional-programming)¬†styles, supporting generic programming and metaprogramming statically and dynamically.\n\n\u003e The goal of Rust is to be a good programming language for creating highly¬†**concurrent, safe, and performant systems**.¬†[Learning Rust](https://learning-rust.github.io/docs/a1.why_rust.html)\n\nFind more comparisons to [python](term/python.md) and how Rust will take over [data engineering](term/data%20engineering.md) on [Will Rust Take over Data Engineering?](https://airbyte.com/blog/rust-for-data-engineering).\n\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/schema-evolution":{"title":"What is Schema Evolution?","content":"Automatic Schema Evolution is a crucial feature in¬†[Data Lake Table Format](term/data%20lake%20table%20format.md)s as changing formats is still a pain in today's data engineer work. Schema Evolution means adding new columns without breaking anything or even enlarging some types. You can even rename or reorder columns, although that might break backward compatibilities. Still, we can change one table, and the table format takes care of switching it on all distributed files. Best of all does not require e rewrite of your table and underlying files.\n\nSee also [ACID Transactions](term/acid%20transactions.md).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/semantic-layer":{"title":"What is a Semantic Layer","content":"\n\u003e A semantic layer (sometimes also called Headless BI) calculates complex business [metrics](term/metric.md) at *query time*. It sits between your data sources/transformation layer and your analytics tools. You define a metric's aggregations (daily, weekly, monthly, and quarterly) and dimensions (region, customer, product). Examples of metrics could be \"monthly active users\", \"weekly revenue\", \"number of paying customers\", and so on.\n\nYou can think of a semantic layer as a translation layer between any data presentation layer ([business intelligence](term/business%20intelligence.md), [notebooks](term/notebooks.md), data apps) and the data sources. A translation layer includes many features, such as integrating data sources, modeling the metrics, and integrating with the data consumers by translating metrics into [SQL](term/sql.md), REST, or GraphQL.\n\nBecause everyone has different definitions of ‚Äúactive‚Äù users or ‚Äúpaying‚Äù customers, the semantic layer allows you to define these discrepancies once company-wide. Instead of having three different versions each presentation tool e.g. BI tool would show a different number than your Jupyter notebook or data app. And what if the metric changes to a new definition, with a semantic layer you change only one time. This powerful feature empowers domain experts and data practitioners to get a common understanding of business metrics.\n\nA sub-layer of the semantic layer is the [Metrics Layer](term/metrics%20layer.md). \n\nRead more on [The Rise of the Semantic Layer](https://airbyte.com/blog/the-rise-of-the-semantic-layer-metrics-on-the-fly) or other fascinating reads on the topic:\n-   [Down the Semantic Rabbit Hole](https://jpmonteiro.substack.com/p/down-the-semantic-rabbit-hole)\n-   [The Missing Piece of the Modern Data Stack](https://benn.substack.com/p/metrics-layer)¬†\n-   [Deep Dive: What The Heck Is the Metrics Layer](https://pedram.substack.com/p/what-is-the-metrics-layer)\n-   Follow-up: [Deep dive: What the heck is the Semantic Layer](https://cube.dev/blog/what-the-heck-is-the-headless-bi)\n-   [The Great Data Debate by Atlan](https://atlan.com/great-data-debate/)\n-   [The Metrics Layer has Growing up to do](https://prakasha.substack.com/p/the-metrics-layer-has-growing-up)\n-   [The Universal Semantic Layer, More Important Than Ever](https://www.atscale.com/blog/what-is-a-universal-semantic-layer-why-would-you-want-one/)\n-   [Demystifying the Metrics Store and Semantic Layer](https://thenewstack.io/demystifying-the-metrics-store-and-semantic-layer/)\n-   Semantic Superiority series: [Part 1](https://davidsj.substack.com/p/semantic-superiority-part-1), [Part 2](https://davidsj.substack.com/p/semantic-superiority-part-2), [Part 3](https://davidsj.substack.com/p/semantic-superiority-part-3), [Part 4](https://davidsj.substack.com/p/semantic-superiority-part-4), and [Part 5](https://davidsj.substack.com/p/semantic-superiority-part-5)\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/semantic-warehouse":{"title":"What is a Semantic Warehouse","content":"It incorporates best practices espoused by¬†[Bill Inmon](Bill%20Inmon)¬†for robust, scalable warehouse design built for the cloud as an abstraction of the [[Modern Data Stack]] with [[term/ata modeling]] at its core. \n\n![](images/semantic-warehouse.png)\nIllustrating the Semantic Warehouse from [Chad Sanderson on LinkedIn](https://www.linkedin.com/posts/chad-sanderson_im-very-happy-to-unveil-the-semantic-warehouse-activity-6958091220157964288-JSXj/)\n\nChad Sanders first introduced the term in this [LinkedIn post](https://www.linkedin.com/posts/chad-sanderson_im-very-happy-to-unveil-the-semantic-warehouse-activity-6958091220157964288-JSXj/). Some defining features:\n- Data as a product and capturing the natural world through events instead of batch processing with a clear defined schema\n- [Data Contract](term/data%20contract.md) as a foundation to introduce contracts to its underlying source tables.\n- Collaborative, peer-reviewed data modeling.\n- Centralized metrics with a [Metrics/Locical Layer](term/metrics%20layer.md) allow collaborative data modeling between the business and the (data) engineers and abstract away the complexity of the data stack.\n- Built-in incentives with semantics and modeling are required to generate good [Data Products](term/data%20product.md).\n\nThe semantic warehouse tries to solve the following problems:\n1. The [Modern Data Stack](term/modern%20data%20stack.md) (MDS) is a good set of tools for building things, but they do not help ensure that what is being built is high quality.  \n2. Most data architectures and data foundations are not scalable. The first version of data infrastructure (typically set up by engineers or junior data devs) is never refactored because it is tough to do so  \n3. Producers do not (but should) own data quality. Data Engineers should not be middle-men caught in the cross-fire of consumers  \n4. Semantics and context are missing. Data devs spend days to weeks just trying to understand what data we have, what it means, how it maps to services, and whether data can be trusted  \n5. Data modeling was not a first-class citizen. Modeling was challenging to do (because of #4) and, in some cases, impossible, thanks to data simply being missing.  \n6. Our [Data Warehouse](term/data%20warehouse.md) did not reflect the real world. Instead, it was a dumping ground for production services and 3rd party APIs.  \n7. A lack of interoperability due to tools not 'speaking the same language.' We have multiple products which each require their distinct modeling environment and no shared understanding of business concepts  \n8. [Data Governance](term/data%20governance.md) is critical, but businesses will reject it if it becomes a bottleneck. We cannot scale our data team horizontally with the complexity  \n  \nSee also [[Metrics Layer|Semantic Layer]] and [[Data Contract]].","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/semi-structured-data":{"title":"What is semi-structured data?","content":"\nSemi-structured data is data that lacks a rigid structure and that does not conform directly to a data model, but that has tags, metadata, or elements that describe the data. Examples of semi-structured data are JSON or XML files. Semi-structured data often contains enough information that it can be relatively easily converted into structured data.¬†\n\nJSON data embedded inside of a string, is an example of semi-structured data. The string contains all the information required to understand the structure of the data, but is still for the moment just a string -- it hasn't been structured yet. The Raw JSON stored by Airbyte during ELT is an example of semi-structured data. This looks as follows:  \n\n|               |  **\\_airbyte_data**|\n|---------| -----------|\n|Record 1| \\\"{'id': 1, 'name': 'Mary X'}\\\" |\n|Record 2| \\\"{'id': 2, 'name': 'John D'}\\\"|\n\n## Semi-structured vs structured data\nIn contrast to semi-structured data,  [structured data](term/structured%20data.md) refers to data that has been formatted into a well-defined schema. An example would be data that is stored with precisely defined columns in a relational database or excel spreadsheet. Examples of structured fields could be age, name, phone number, credit card numbers or address. \n\n## Structuring of semi-structured data\n\nIt is often relatively straightforward to convert semi-structured data into structured data. Converting semi-structured data into structured data is often done during the [data transformation](term/data%20transformation.md) stage in an [ETL](term/etl.md) or [ELT](term/elt.md) process.¬† \n\nFor example, if normalization is enabled then Airbyte will automatically convert the JSON stored in the `_airbyte_data` field in the table above,  into a table that looks as follows:  \n\n|               |  **id** | **name** |\n|---------| -----------|---- |\n|Record 1| 1 | \"Mary X\" |\n|Record 2|2| \"John D\" |\n  \n## A real-world example of converting semi-structured to structured data\n\nIf the semi-structured JSON data were stored in Postgres, then it could be converted¬† into structured data by making use of [JSON Functions and Operators]([https://www.postgresql.org/docs/9.4/functions-json.html](https://www.postgresql.org/docs/9.4/functions-json.html)). A real-world implementation of this is discussed the tutorial: [Explore Airbyte's full refresh data synchronization](https://airbyte.com/tutorials/full-data-synchronization)","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/slowly-changing-dimension-scd":{"title":"What is Slowly Changing Dimension?","content":"A Slowly Changing Dimension (SCD) is¬†**a dimension that stores and manages both current and historical data over time in a¬†[Data Warehouse](term/data%20warehouse.md)**. It is considered and implemented as one of the most critical ETL tasks in tracking the history of dimension records.","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/soft-delete":{"title":"What is a Soft Delete?","content":"In order to propagate records that have been deleted when using¬†[Incremental Synchronization](term/incremental%20synchronization.md)¬†modes, records in a database may include a field that indicates that a record should be treated as if it has been removed. This is necessary because incremental synchronization does not replicate documents that are fully deleted from a source system.\n\nFor example, a boolean flag such as `is_deleted` could be used to indicate that a record should be treated as if it has been deleted. All queries would need to be written so as to exclude records/documents where `is_deleted` is set, and periodically executed background jobs can be used to remove all documents where `is_deleted`¬†is set.\n\n‚Äç\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/software-defined-assets":{"title":"What is a Software-Defined Asset?","content":"The software-defined asset was first [introduced](https://dagster.io/blog/software-defined-assets) by [Dagster](term/Dagster.md) with the following definition:\n\u003e A new, [declarative](term/declarative.md) approach to managing data and orchestrating its maintenance. \n\u003e Declarative data management starts with using code to define the data assets that you want to exist. These asset definitions, version-controlled through git and inspectable via tooling, allow anyone in your organization to understand your canonical set of data assets, enable you to reproduce them at any time, and offer a foundation for asset-based orchestration.\n\nThe key to software-defined assets is to declare a data asset/product pre-runtime. The SW-defined function in Dagster is like a microservice or the code that defines the asset in a  [functional way](term/functional%20data%20engineering.md) (that can live independently). With the declarative approach, we have more information defined as code helping the [orchestrator](term/data%20orchestrator.md) to figure out the lineage, how to run, etc. \n\nThe best thing, you get the actual data lineage of your physical assets, not an arbitrary lineage of tasks (that is interesting for engineers but not for data consumers).\n\nIn the future, more code will be written with SW-defined assets as it reduces the need for writing lots of boilerplate as it's declarative and describes what the asset is supposed to do and include rather than how that is handled and run by Dagster. See more on [Data Orchestration Trends](https://airbyte.com/blog/data-orchestration-trends), where the trends included in this shift from an [imperative](term/imperative.md) pipeline with ops, jobs, graphs, etc., to Assets with Software-defined assets are explained.\n\nMuch has been announced on the [Dagster Community Day](https://www.youtube.com/live/An78xLxM9zQ?feature=share), where Nick said, the founder of Dagster: \"Think of an iPhone: It feels like this one device, but there is a lot of complexity, and heterogeneous. The same is true for orchestration which could be the future to bundle the [Open Data Stack](term/open%20data%20stack.md) into one coherent data stack. An alternative would be a vertical integration with one of the prominent vendors.\n\n\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/sql":{"title":"What is SQL?","content":"SQL is¬†**a standardized language used to interact with relational [[databases]]**. It stands for structured query language (SQL) and defines a standard [programming language](term/programming%20languages.md) utilized to extract, organize, manage, and manipulate data stored in relational databases.\n\nHere are different levels you can go into ([Source](https://twitter.com/largedatabank/status/1559651463919452161)):\n![](images/sql-levels-explained.png)\n\nSee more on [SQL-Levels Explained](https://github.com/airbytehq/SQL-Levels-Explained).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/storage-layer-object-store":{"title":"What is a Storage Layer / Object Store?","content":"A storage layer or object storage are services from the three big cloud providers, AWS S3, Azure Blob Storage, and Google Cloud Storage. The web user interface is easy to use.¬†**Its features are very basic, where, in fact, these object stores store distributed files exceptionally well.**¬†They are also highly configurable, with solid security and reliability built-in.\n\nYou can build on with  [Data Lake File Format](term/data%20lake%20file%20format.md) or [Data Lake Table Format](term/data%20lake%20table%20format.md). Read more on our¬†[Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/structured-data":{"title":"What is structured data?","content":"\nStructured data refers to data that has been formatted into a well-defined schema. An example would be data that is stored with precisely defined columns in a relational database or excel spreadsheet. Examples of structured fields could be age, name, phone number, credit card numbers or address. Storing data in a structured format allows it to be easily understood and queried by machines and with tools such as SQL.\n\n## Example of structure data\n\nBelow is an example of structured data as it would appear in a database:\n\n|         |  **age**| **name**| **phone**| \n|---------|-----|------|-----|\n|Record 1| 29 | Bob | 123-456 |\n|Record 2| 30 | Sue | 789-123 | \n\nIt may seem that all data is structured, but this is not always the case -- data can be unstructured, or semi-structured. The differences are best understood by example, and are discussed in the following sections. \n\n## Structured data vs. unstructured data\n\nStructured data can be contrasted with [unstructured data](term/unstructured%20data.md), which does not conform to a data model and has no easily identifiable structure. Unstructured data cannot be easily used by programs, and is difficult to analyze. Examples of unstructured data could be the contents of an email, contents of a word document, data from social media, photos, videos, survey results, etc.¬†  \n\nAn simple example of unstructured data is a string that contains interesting information inside of it, but that has not been formatted into a well defined schema. An example is given below:\n\n|               |  **UnstructuredString**|\n|---------| -----------|\n|Record 1| \"Bob is 29\" |\n|Record 2| \"Mary just turned 30\"|\n\n## Structuring of unstructured data\n\nConverting unstructured data into structured data can be done during the [data transformation](term/data%20transformation.md) stage in an [ETL](term/etl.md) or [ELT](term/elt.md) process.¬† \n\nFor example, in order to efficiently make use of the unstructured data given in the previous example, it may desirable to transform it into structured data such as the following:\n\n|               |  **name** | **age** |\n|---------| -----------|---- |\n|Record 1| \"Bob\" | 29 |\n|Record 2| \"Mary\"| 30 |\n\nStoring the data in a structured manner makes it much more efficient to query the data. For example, after structuring the data it is possible to easily and efficiently execute the following query on the structured data:\n  \n``` SQL\nSELECT * FROM X where Age=29\n```\n\nA query such as this would be expensive and/or more difficult to execute on unstructured data.\n\n## Structured data vs. semi-structured data\n\nStructured data can also be contrasted with [semi-structured data](term/semi-structured%20data.md), which lacks a rigid structure and does not conform directly to a data model. However, semi-structured data has tags and elements that describe the data. \n\nExamples of semi-structured data are JSON or XML files. Semi-structured data often contains enough information that it can be relatively easily converted into structured data.¬†\n\nJSON data embedded inside of a string, is an example of semi-structured data. The string contains all the information required to understand the structure of the data, but is still for the moment just a string -- it hasn't been structured yet. The Raw JSON stored by Airbyte during ELT is an example of semi-structured data. This looks as follows:  \n\n|               |  **\\_airbyte_data**|\n|---------| -----------|\n|Record 1| \\\"{'id': 1, 'name': 'Mary X'}\\\" |\n|Record 2| \\\"{'id': 2, 'name': 'John D'}\\\"|\n\n## Structuring of semi-structured data\n\nIt is often relatively straightforward to convert semi-structured data into structured data. For example, if normalization is enabled then Airbyte will automatically convert the JSON stored in the `_airbyte_data` field in the table above, into a table that looks as follows:  \n\n|               |  **id** | **name** |\n|---------| -----------|---- |\n|Record 1| 1 | \"Mary X\" |\n|Record 2|2| \"John D\" |\n  \n## A real-world example of converting semi-structured to structured data\n\nIf the semi-structured JSON data were stored in Postgres, then it could be converted¬† into structured data by making use of [JSON Functions and Operators]([https://www.postgresql.org/docs/9.4/functions-json.html](https://www.postgresql.org/docs/9.4/functions-json.html)). A real-world implementation of this is discussed the tutorial: [Explore Airbyte's full refresh data synchronization](https://airbyte.com/tutorials/full-data-synchronization)\n","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/sync-run":{"title":"What is a sync run?","content":"Airbyte replication can be thought of as a loop which periodically requests records from a data source and sends them to a destination. Each iteration of this loop is referred to as a sync run, which is discussed in more detail in [How we scale workflow orchestration with Temporal](https://airbyte.com/blog/scale-workflow-orchestration-with-temporal#triggering-a-sync-run).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/temporal":{"title":"Temporal","content":"\u003e [!info] Info\n\u003e \n\u003e This is only relevant for individuals who want to learn about or contribute to our underlying platform.\n\n[Temporal](https://temporal.io/)¬†is a development kit that lets you create workflows, parallelize them, and handle failures/retries gracefully. We use it to reliably schedule each step of the ELT process, and a Temporal service is always deployed with each Airbyte installation.\n\nRead more on [How we Scale Workflow Orchestration with Temporal](https://airbyte.com/blog/scale-workflow-orchestration-with-temporal) at Airbyte.","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/time-travel":{"title":"What is Time Travel?","content":"With time travel, the¬†[Data Lake Table Format](term/data%20lake%20table%20format.md)¬†versions the big data you store in your¬†[Data Lake](term/data%20lake.md). You can access any historical version of that data, simplifying data management with easy-to-audit, rollback data in case of accidental bad writes or deletes, and reproduce experiments and reports. Time travel enables reproducible queries as you can query two different versions simultaneously.\n\nRead more about how to build a Data Lake on top of it on our¬†[Data Lake and Lakehouse Guide](https://airbyte.com/blog/data-lake-lakehouse-guide-powered-by-table-formats-delta-lake-iceberg-hudi).","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/unstructured-data":{"title":"What is unstructured data?","content":"\nUnstructured data is data that does not conform to a data model and has no easily identifiable structure. Unstructured data cannot be easily used by programs, and is difficult to analyze. Examples of unstructured data could be the contents of an email, contents of a word document, data from social media, photos, videos, survey results, etc.\n\n## An example of unstructured data\n\nAn simple example of unstructured data is a string that contains interesting information inside of it, but that has not been formatted into a well defined schema. An example is given below:\n\n|               |  **UnstructuredString**|\n|---------| -----------|\n|Record 1| \"Bob is 29\" |\n|Record 2| \"Mary just turned 30\"|\n\n## Unstructured vs structured data\n\nIn contrast with unstructured data, [structured data](term/structured%20data.md) refers to data that has been formatted into a well-defined schema. An example would be data that is stored with precisely defined columns in a relational database or excel spreadsheet. Examples of structured fields could be age, name, phone number, credit card numbers or address. Storing data in a structured format allows it to be easily understood and queried by machines and with tools such as¬† SQL.\n\n## Structuring of unstructured data\n\nExtracting structured data from unstructured data is often done during the [data transformation](term/data%20transformation.md) stage in an [ETL](term/etl.md) or [ELT](term/elt.md) process.¬† \n\nFor example, in order to efficiently make use of the unstructured data given in the previous example, it may desirable to transform it into structured data such as the following:\n\n|               |  **name** | **age** |\n|---------| -----------|---- |\n|Record 1| \"Bob\" | 29 |\n|Record 2| \"Mary\"| 30 |\n\nStoring the data in a structured manner makes it much more efficient to query the data. For example, after structuring the example data it is possible to easily and efficientl execute queries by name or by age. \n\n\n  ","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null},"/term/yaml":{"title":"What is YAML?","content":"YAML is a data serialization language often used to write configuration files. Depending on whom you ask, YAML stands for yet another markup language, or YAML isn‚Äôt markup language (a recursive acronym), which emphasizes that YAML is for data, not documents.","lastmodified":"2023-07-25T13:01:03.637830787Z","tags":null}}