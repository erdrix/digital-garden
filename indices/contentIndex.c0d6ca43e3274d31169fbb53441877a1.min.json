{"/":{"title":"Digital Garden ü™¥","content":"\n# Welcome in my Digital Garden ü™¥\nI'm happy to welcome you to my garden!\n\nI love learning, testing and discovering new things, related to the world of data that is dear to me, or not. But it was quite frustrating because, over time, things would slip out of my mind replaced by other discoveries, not lost forever, but much harder to retrieve and share!\n\nAnd then I read Jacky's [Networked Thought](https://jzhao.xyz/posts/networked-thought) page sharing his [Quartz](https://github.com/jackyzha0/quartz) project, it gave me the tool I'd been missing, being able to document, create my garden full of seeds to grow and share with anyone interested üåª.\n\n\n## My garden ü•ï\nI'm just at the beginning of my garden, the land is still uncultivated, and I don't know where I will put the vegetable patch, how I will organize my plots, which trees and flowers I will be able to plant, and how they will grow! So, we will start by drawing inspiration from the neighbors.\n\n### #Seeds\n\nI tend to generally bookmark things for later then revisit them when I have time. For [ideas list](thoughts/bag%20of%20seeds.md), writing, and all sorts of reading. Even when reading books, I don‚Äôt like to take complex notes right away will only bookmark or highlight phrases. I will eventually come back to the bookmarks a second time to generate insights and actual thoughts. It feels like this weeds out unnecessary noise and provides a natural chance for spaced repetition.\n\nThese are the seeds that form the basis of ideas and thoughts.\n\n### #Saplings\n\nSaplings are single nodes or thoughts.\n\n### #Fruits\n\nOf course, a knowledge index isn‚Äôt much use if it doesn‚Äôt inform future thinking and output. Fruits are derivative or ‚Äônew‚Äô pieces of content.\n\nIt‚Äôs the act of creating ‚Äônewer‚Äô work from saplings, mostly longer form #posts, [projects](thoughts/projects.md), etc. At this stage, thoughts and ideas have matured enough to be able to share and collaborate.\n\n\n## Let's take a digital coffee ‚òï\n\n\u003e [!info]\n\u003e Feel free to join our [Slack channel](https://join.slack.com/t/konpytika/shared_invite/zt-14md072lv-Jr8mqYoeUrqzfZF~YGUpXA) for a chat ü§ó\n","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/private/doctolib/daily_log/2023-07-26":{"title":"26-07-2023","content":"\n##  Discussion with Pierre\n\n- Redshift serverless -\u003e Will go\n- Data Catalog -\u003e Data Hub innovation day\n- Ansible automation -\u003e [Github Actions](thoughts/github%20actions.md) vs AWX vs AWS SSM\n\n## Log and metric management vector\n\n- ECS discussion: \n\t- Integration Vault \u003e Secret Manager \u003e Environment variable\n\t- Log driver approach: Splunk, Firelens\n- Challenge Vector vs other approaches with Thomas.\n\n## DBT\n\n- Finish docker image for base\n\n## Cost allocation\n\n- Discussion and PR review","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/private/doctolib/daily_log/2023-07-27":{"title":"27-07-2023","content":"\n##  DBT\n- Add ECR for base image\n- Add ECR for DBT DTMS project\n- Deep dive dataform\n- Automate CI for base image\n\n## Cost allocation\n- PRs review\n\n## Meets\n- 121 Josselin\n- Review\n\n## Fast logging\n- Secret integration avec Vault + Secret manager","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/private/doctolib/daily_log/2023-07-28":{"title":"28-07-2023","content":"\n## Data architecture comitee\n### Topics\n- Vector ‚Üí Scoping logging + monitoring\n\n### Decisions\n- ECS integration with Vault via Terraform‚Üí Ok to deal integration that way\n\n### Actions\n- Move from ECS to EKS\n- `Vector.toml` ‚Üí add an entrypoint in the Dockerfile to manage the config via environment variable.\n\n## Data Catalog\n\n\n## DBT\n- Generates .py + manifest.json\n- Issues with CI:\n\t- No explicite message because github action not allowed\n\t- AWS assume role unused.\n","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/private/doctolib/daily_log/2023-07-31":{"title":"31-07-2023","content":"\n‚Üí Setup new mac\n\n## DBT\n\n- Start merge some of the DBT PRs\n- Build image DBT DTMs\n- Support multiple platforms\n\n## CI\n\n- Assume role","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/private/doctolib/daily_log/2023-08-01":{"title":"01-08-2023","content":"\n\n## DBT\n- Sync DBT with Mathieu \u0026 Lucas\n- Github action docker build support multiple platforms\n- ECR terraform module support empty list of readers or writers\n- Move from CI to multiple-stage docker image for python model and manifest.json\n- Scoping source to sensors\n\n## CI\n- Take back ownership on CI image\n- Add buildx to support multi-stage build with redshift connection\n\n## Meets\n- Ops sharing\n","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/private/doctolib/daily_log/2023-08-02":{"title":"02-08-2023","content":"\n## CI\n- Test new Image on production\n\n## DBT\n- Sync DBT with Business scale:\n\t- TADA for AE on DBT usage (concrete UC with model import etc.)\n\t- Workshop with people by Vincent to evangelize usage\n\t- Work on Airflow integration + Concrete UC to validate\n\n","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/private/doctolib/daily_log/2023-08-04":{"title":"04-08-2023","content":"\n## \n","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/thoughts/Apache-NiFi":{"title":"Apache NiFi","content":"","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/thoughts/Kubernetes":{"title":"Kubernetes","content":"","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/thoughts/Kubernetes-CRD":{"title":"Kubernetes Custom Resource Definition","content":"","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/thoughts/Kubernetes-network-policies":{"title":"Network policies","content":"\n[Kubernetes](thoughts/Kubernetes.md) Network policies are an application-centric construct which allow you to specify how a pod is allowed to communicate with various network ‚Äúentities‚Äù (endpoints, services) over the network.\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: test-network-policy\n  namespace: default\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n    - Ingress\n    - Egress\n  ingress:\n    - from:\n        - ipBlock:\n            cidr: 172.17.0.0/16\n            except:\n              - 172.17.1.0/24\n        - namespaceSelector:\n            matchLabels:\n              project: myproject\n        - podSelector:\n            matchLabels:\n              role: frontend\n      ports:\n        - protocol: TCP\n          port: 6379\n  egress:\n    - to:\n        - ipBlock:\n            cidr: 10.0.0.0/24\n      ports:\n        - protocol: TCP\n          port: 5978\n\n```\n\n## Links\n- [Network Policies | Kubernetes](https://kubernetes.io/docs/concepts/services-networking/network-policies/)\n","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/thoughts/NAT-Gateway":{"title":"NAT Gateway","content":"\nA NAT gateway is a Network Address Translation (NAT) service. It lets certain resources create outbound connections to the internet or to other [VPC](thoughts/aws%20vpc.md) networks. Cloud NAT supports address translation for established inbound response packets only. It does not allow unsolicited inbound connection\n\n### Public NAT\n\nPublic NAT lets resources that do not have public IP addresses communicate with the internet. These VMs use a set of shared public IP addresses to connect to the internet. Public NAT does not rely on proxy VMs. Instead, a Public NAT gateway allocates a set of external IP addresses and source ports to each VM that uses the gateway to create outbound connections to the internet.\n\n### Private NAT\n\nPrivate NAT enables¬†_private-to-private_¬†translations across Google Cloud networks. Inter-VPC NAT, a Private NAT offering, lets you create a Private NAT gateway that works in conjunction with Network Connectivity Center to perform NAT between Virtual Private Cloud networks. The Private NAT gateway uses a NAT IP address from a Private NAT subnet to NAT traffic between resources that are attached to a Network Connectivity Center hub.\n\nAssume that the resources in your VPC network need to communicate with the resources in a VPC network that is owned by a different business entity. However, the VPC network of that business entity contains subnets whose IP addresses overlap with the IP addresses of your VPC network. In this scenario, you create a Private NAT gateway that routes traffic between the subnets in your VPC network to the non-overlapping subnets of that business entity.","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/thoughts/NiFiKop":{"title":"NiFiKop","content":"\nIt is is an open-source [Kubernetes operator](thoughts/kubernetes%20operator.md) that makes it¬†easy¬†to run [Apache NiFi](thoughts/Apache%20NiFi.md) on [Kubernetes](thoughts/Kubernetes.md).","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/thoughts/aws-vpc":{"title":"AWS VPC","content":"\nWith Amazon Virtual Private Cloud (Amazon VPC), you can launch AWS resources in a logically isolated virtual network that you've defined. This virtual network closely resembles a traditional network that you'd operate in your own data center, with the benefits of using the scalable infrastructure of AWS.\n\nThe following diagram shows an example VPC. The VPC has one subnet in each of the Availability Zones in the Region, EC2 instances in each subnet, and an internet gateway to allow communication between the resources in your VPC and the internet.\n\n![](thoughts/images/aws_vpc.png)","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/thoughts/aws-vpc-cni":{"title":"AWS VPC CNI","content":"[Container Network Inteface Plugin](thoughts/container%20network%20inteface%20plugin.md)¬†using [elastic network interfaces](thoughts/elastic%20network%20interfaces.md)¬†on AWS.\n#aws\n\n## Overlay network\nThe first one is the overlay network used for communication and encapsulating the packets over the network. In this case, the encapsulating is taking place at the hardware level using #aws [ENI](thoughts/elastic%20network%20interfaces.md) inside a [VPC](thoughts/aws%20vpc.md).\n\n![](thoughts/images/pod_networking_amazon_vpc_cni.png)\n\nThe plugin support two modes when it comes to manage IP addresses management for pods:\n\n- [Prefix mode](https://aws.github.io/aws-eks-best-practices/networking/prefix-mode/index_linux/ \"https://aws.github.io/aws-eks-best-practices/networking/prefix-mode/index_linux/\"): allows [Kubernetes](thoughts/Kubernetes.md) Pods to have the same IP address as they do on the [VPC](thoughts/aws%20vpc.md) network. More specifically, all containers inside the Pod share a network namespace, and they can communicate with each-other using local ports.\n- [Secondary IP mode](https://aws.github.io/aws-eks-best-practices/networking/vpc-cni/#overview \"https://aws.github.io/aws-eks-best-practices/networking/vpc-cni/#overview\"): when a worker node is provisioned, it has a default [ENI](thoughts/elastic%20network%20interfaces.md), called the primary [ENI](thoughts/elastic%20network%20interfaces.md), attached to it. The [CNI](thoughts/container%20network%20inteface%20plugin.md) allocates a warm pool of [ENIs](thoughts/elastic%20network%20interfaces.md) and secondary IP addresses from the subnet attached to the node‚Äôs primary [ENI](thoughts/elastic%20network%20interfaces.md). By default, IPAMD (a long-running node-local IP Address Management daemon and is responsible for) attempts to allocate an additional [ENI](thoughts/elastic%20network%20interfaces.md) to the node. The IPAMD allocates additional [ENI](thoughts/elastic%20network%20interfaces.md) when a single Pod is scheduled and assigned a secondary IP address from the primary [ENI](thoughts/elastic%20network%20interfaces.md). This \"warm\" [ENI](thoughts/elastic%20network%20interfaces.md) enables faster Pod networking. As the pool of secondary IP addresses runs out, the [CNI](thoughts/container%20network%20inteface%20plugin.md) adds another [ENI](thoughts/elastic%20network%20interfaces.md) to assign more.\n\n## Flow traffic control\nIn #aws a [Security Group (SG)](thoughts/security%20group.md) acts as a virtual firewall for EC2 instances to control inbound and outbound traffic. By default the Amazon [VPC](thoughts/aws%20vpc.md) CNI will use [SGs](thoughts/security%20group.md) associated with the primary[ENI](thoughts/elastic%20network%20interfaces.md)  on the node. Which means that every Pod on a node shares the same security groups as the node it runs on.\n![](thoughts/images/aws_vpc_cni_node_security_group.png)\nWith [Security Group for Pods](https://aws.github.io/aws-eks-best-practices/networking/sgpp/ \"https://aws.github.io/aws-eks-best-practices/networking/sgpp/\"), multiple types of security rules, such as Pod-to-Pod and Pod-to-External AWS services, can be defined in a single place with EC2 security groups and applied to workloads with Kubernetes native APIs.\n\n![](thoughts/images/aws_security_group_for_pods.png)\nTo enable the Security Groups for pod we have to set `ENABLE_POD_ENI = true` for VPC CNI. Once done, for each node in the cluster the add-on adds a label with the value `vpc.amazonaws.com/has-trunk-attached=true`. The [VPC Resource Controller](https://github.com/aws/amazon-vpc-resource-controller-k8s \"https://github.com/aws/amazon-vpc-resource-controller-k8s\") running on the control plane creates and attaches a trunk interface called ‚Äúaws-k8s-trunk-eni‚Äù to the node. It acts as a standard network interface attached to the instance.\n\nThe controller also creates branch interfaces named \"aws-k8s-branch-eni\" and associates them with the trunk interface. Pods are assigned a security group using the [SecurityGroupPolicy](https://github.com/aws/amazon-vpc-resource-controller-k8s/blob/master/config/crd/bases/vpcresources.k8s.aws_securitygrouppolicies.yaml \"https://github.com/aws/amazon-vpc-resource-controller-k8s/blob/master/config/crd/bases/vpcresources.k8s.aws_securitygrouppolicies.yaml\") custom resource and are associated with a branch interface. Since security groups are specified with network interfaces, we are now able to [schedule Pods requiring specific security groups](https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html#sg-pods-example-deployment \"https://docs.aws.amazon.com/eks/latest/userguide/security-groups-for-pods.html#sg-pods-example-deployment\") on these additional network interfaces.\n\n![](thoughts/images/aws_security_group_for_pods_branch_eni.png)\n\n\u003e [!warning] \n\u003e \n\u003e Branch interface capacity is _additive_ to existing instance type limits for secondary IP addresses. Pods that use security groups are not accounted for in the max-pods formula and when you use security group for pods you need to consider [raising the max-pods](https://docs.aws.amazon.com/eks/latest/userguide/cni-increase-ip-addresses.html \"https://docs.aws.amazon.com/eks/latest/userguide/cni-increase-ip-addresses.html\") value or be ok with running fewer pods than the node can actually support.\n\n#### Recommandation\n\nSome highlights on recommandations using this feature:\n\n\u003e[!info]  \n\u003e [Disable TCP Early Demux for Liveness Probe](https://aws.github.io/aws-eks-best-practices/networking/sgpp/#disable-tcp-early-demux-for-liveness-probe \"https://aws.github.io/aws-eks-best-practices/networking/sgpp/#disable-tcp-early-demux-for-liveness-probe\")\n\u003e\n\u003eIf are you using liveness or readiness probes, you also need to disable TCP early demux, so that the kubelet can connect to Pods on branch network interfaces via TCP\n\n\u003e [!info]\n\u003e [Supporting Kubernetes Network Policy](https://aws.github.io/aws-eks-best-practices/networking/sgpp/#enforcing-mode-use-standard-mode-in-the-following-situations \"https://aws.github.io/aws-eks-best-practices/networking/sgpp/#enforcing-mode-use-standard-mode-in-the-following-situations\")\n\u003e \n\u003e We recommend using standard enforcing mode when using network policy with Pods that have associated security groups.\n\u003e\n\u003eWe strongly recommend to utilize security groups for Pods to limit network-level access to AWS services that are not part of a cluster. Consider [Kubernetes network policies](thoughts/Kubernetes%20network%20policies.md) to restrict network traffic between Pods inside a cluster, often known as East/West traffic.\n\n\u003e [!info] \n\u003e [Deploy pods with Security Groups to Private Subnets](https://aws.github.io/aws-eks-best-practices/networking/sgpp/#deploy-pods-with-security-groups-to-private-subnets \"https://aws.github.io/aws-eks-best-practices/networking/sgpp/#deploy-pods-with-security-groups-to-private-subnets\")\n\u003e \n\u003e Pods that are assigned security groups must be run on nodes that are deployed on to private subnets. Note that Pods with assigned security groups deployed to public subnets will not able to access the internet.\n\n## Setup\nWhen you create a new EKS cluster the Amazon VPC CNI is now installed by default as a managed add-on. But on our cluster they have been installed in self-managed mode.\n\nThere is two different approaches to install it:\n- [Update the self-managed add-on](https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html#vpc-add-on-self-managed-update \"https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html#vpc-add-on-self-managed-update\")\n- [Switch to the Amazon EKS add-on](https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html#vpc-add-on-create \"https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html#vpc-add-on-create\")\n\n|**Pros**|**Cons**|\n|---|---|\n|VPC CNI uses Native VPC networking, which results in high performance and is also highly scalable|Need to add another CNI plugin to manage network policies|\n|Support to configure POD security groups, which allows or denies access to POD both from external and internal|No solution to control traffic flow based on fqdn|\n|Support of secondary CIDR IP ranges, so that PODs can have more IPs from secondary subnet||\n\n## Links","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/thoughts/aws-vpc-cni-x-cilium":{"title":"Amazon VPC CNI x Cilium","content":"\nIn this hybrid mode, the [Amazon VPC CNI](thoughts/aws%20vpc%20cni.md) is responsible for setting up the virtual network devices as well as for IP address management (IPAM) via [ENIs](thoughts/elastic%20network%20interfaces.md). After the initial networking is setup for a given pod, the Cilium CNI plugin is called to attach [eBPF](thoughts/eBPF.md) programs to the network devices set up by the [Amazon VPC CNI](thoughts/aws%20vpc%20cni.md) plugin in order to enforce [Kubernetes network policies](thoughts/Kubernetes%20network%20policies.md), perform load-balancing and provide encryption.\n\nThe steps to setup CNI chaining with AWS CNI:\n![](thoughts/images/cilium_cni_chaining_amazon_vpc_cni.png)\n\n- [Setting up Cilium with helm](https://docs.cilium.io/en/stable/installation/cni-chaining-aws-cni/#setting-up-a-cluster-on-aws \"https://docs.cilium.io/en/stable/installation/cni-chaining-aws-cni/#setting-up-a-cluster-on-aws\")\n- [Restart existing pods](https://docs.cilium.io/en/stable/installation/cni-chaining-aws-cni/#restart-existing-pods \"https://docs.cilium.io/en/stable/installation/cni-chaining-aws-cni/#restart-existing-pods\")\n- [Validate the installation](https://docs.cilium.io/en/stable/installation/cni-chaining-aws-cni/#validate-the-installation \"https://docs.cilium.io/en/stable/installation/cni-chaining-aws-cni/#validate-the-installation\")\n- [Enabling security groups for pods](https://docs.cilium.io/en/stable/installation/cni-chaining-aws-cni/#validate-the-installation \"https://docs.cilium.io/en/stable/installation/cni-chaining-aws-cni/#validate-the-installation\")\n\n\u003e [!warning]\n\u003e Warning\n\u003e \n\u003e Some advanced [Cilium](thoughts/cilium.md) features may be limited when chaining with other CNI plugins, such as:\n\u003e- [Layer 7 Policy](https://docs.cilium.io/en/stable/security/policy/language/#l7-policy \"https://docs.cilium.io/en/stable/security/policy/language/#l7-policy\") (see [GitHub issue 12454](https://github.com/cilium/cilium/issues/12454 \"https://github.com/cilium/cilium/issues/12454\"))\n\u003e- [IPsec Transparent Encryption](https://docs.cilium.io/en/stable/security/network/encryption-ipsec/#encryption-ipsec \"https://docs.cilium.io/en/stable/security/network/encryption-ipsec/#encryption-ipsec\") (see [GitHub issue 15596](https://github.com/cilium/cilium/issues/15596 \"https://github.com/cilium/cilium/issues/15596\"))\n\n","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/thoughts/bag-of-seeds":{"title":"Bag of seeds üå±","content":"\n","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/thoughts/bounded-context":{"title":"Bounded context","content":"\nThe delimited applicability of a particular model [that] gives team members a clear and shared understanding of what has to be consistent an what can develop independently\n\n## Links\n- [BoundedContext](https://martinfowler.com/bliki/BoundedContext.html)\n","lastmodified":"2023-09-06T10:14:15.370662007Z","tags":null},"/thoughts/cilium":{"title":"Cilium","content":"\n[Cilium](https://cilium.io/get-started/ \"https://cilium.io/get-started/\") is an open source project to provide networking, security and observability for [Kubernetes](thoughts/Kubernetes.md) cluster an other container orchestration platforms. At the foundation of Cilium is a new Linux kernel technology called [eBPF](thoughts/eBPF.md), which enabled the dynamic insertion of powerful security, visibility, and networking control logic into the Linux kernel.\n\n## Global architecture\n![](thoughts/images/cilium_global_architecture.png)\n\n### Cilium\n- **Agent:** runs on each node in the cluster. It accepts configuration via [Kubernetes](thoughts/Kubernetes.md) or APIs that describes networking, service load-balancing, [Kubernetes network policies](thoughts/Kubernetes%20network%20policies.md), and visibility \u0026 monitoring requirements. It listens for events from orchestration systems such as [Kubernetes](thoughts/Kubernetes.md) to learn when containers or workloads are started and stopped. It manages the [eBPF](thoughts/eBPF.md) programs which the Linux kernel uses to control all network access in / out of those containers.\n- **Operator:** is responsible for managing duties in the cluster which should logically be handled once for the entire cluster, rather than once for each node in the cluster. The Cilium [kubernetes operator](thoughts/kubernetes%20operator.md) is not in the critical path for any forwarding or network policy decision.\n- **[CNI plugin](thoughts/container%20network%20inteface%20plugin.md):** is invoked by Kubernetes when a pod is scheduled or terminated on a node. It interacts with the Cilium API of the node to trigger the necessary datapath configuration to provide networking, load-balancing and [Kubernetes network policies](thoughts/Kubernetes%20network%20policies.md) for the pod.\n\n### Data Store\nCilium requires a data store to propagate state between agents. It supports the following data stores:\n\n- **Kubernetes CRDs (Default):** The default choice to store any data and propagate state is to use [Kubernetes Custom Resource Definition (CRD)](thoughts/Kubernetes%20CRD.md). CRDs are offered by  [Kubernetes](thoughts/Kubernetes.md) for cluster components to represent configurations and state via  [Kubernetes](thoughts/Kubernetes.md) resources.\n- **Key-Value Store** **(etcd):** A key-value store can optionally be used as an optimization to improve the scalability of a cluster as change notifications and storage requirements are more efficient with direct key-value store usage\n\n### Observability with Hubble\n- **Server:** runs on each node and retrieves the eBPF-based visibility from Cilium. It is embedded into the Cilium agent in order to achieve high performance and low-overhead. It offers a gRPC service to retrieve flows and Prometheus metrics.\n- **Relay:** standalone component which is aware of all running Hubble servers and offers cluster-wide visibility by connecting to their respective gRPC APIs and providing an API that represents all servers in the cluster.\n- **Graphical UI:** utilizes relay-based visibility to provide a graphical service dependency and connectivity map.\n\n## eBPF datapath: Linux Kernel\n[eBPF](thoughts/eBPF.md) is used to provide high-performance networking, multi-cluster and multi-cloud capabilities, advanced load balancing, transparent encryption, extensive network security capabilities, transparent observability, and much more.\n\nHere is an example how the life of a packet, with [eBPF](thoughts/eBPF.md).\n\n![](thoughts/images/cilium_ebpf_packer_life.png)\n- **Network endpoint (lxc)**: Defines in Kubernetes as [Cilium Endpoint](https://docs.cilium.io/en/stable/network/kubernetes/ciliumendpoint/ \"https://docs.cilium.io/en/stable/network/kubernetes/ciliumendpoint/\") CR. It defines the network interface couple (one in the pod, another one on the host) that link the pod network to the host network.\n\u003e [!note] \n\u003e\n\u003e Even if an endpoint is running, it doesn't mean it's ready. [Here](https://docs.cilium.io/en/stable/security/policy/lifecycle/ \"https://docs.cilium.io/en/stable/security/policy/lifecycle/\") is the graph states and their meanings.\n- **Barckley Packet Filter BPF**: a technology used in certain computer operating systems for programs that need to, among other things, analyze network traffic. It provides a raw interface to data link layers, permitting raw link-layer packets to be sent and received.\n\u003e [!note] \n\u003e \n\u003e I highly recommend anyone working with Cilium to watch this video: [Episode 51: Life of a Packet with Cilium](https://www.youtube.com/watch?v=0BKU6avwS98\u0026t=494s). It explains the basics of this scheme, and gives the commands and methodology for gaining visibility of the network flow, which is very useful for debugging.\n\n## Flow traffic control\nThe configuration of the Cilium agent and the Cilium [network policy](thoughts/Kubernetes%20network%20policies.md) determines whether an endpoint accepts traffic from a source or not. The agent can be put into the following three policy enforcement modes:\n- **default:** endpoints will start without any restrictions and as soon as a rule restricts their ability to receive traffic on ingress or to transmit traffic on egress, then the endpoint goes into whitelisting mode and all traffic must be explicitly allowed.\n- **always:** policy enforcement is enabled on all endpoints even if no rules select specific endpoints.\n- **never:** policy enforcement is disabled on all endpoints, even if rules do select specific endpoints.\n\n### Layer 3 policy\nCan be specified using the following methods:\n- [Labels Based](https://docs.cilium.io/en/stable/security/policy/language/#labels-based \"https://docs.cilium.io/en/stable/security/policy/language/#labels-based\"): used to describe the relationship if both endpoints are managed by Cilium and are thus assigned labels.\n- [Services based](https://docs.cilium.io/en/stable/security/policy/language/#services-based \"https://docs.cilium.io/en/stable/security/policy/language/#services-based\"): Intermediate form between Labels and CIDR and makes use of the services concept in the orchestration system.\n- [Entities Based](https://docs.cilium.io/en/stable/security/policy/language/#entities-based \"https://docs.cilium.io/en/stable/security/policy/language/#entities-based\"): Entities are used to describe remote peers which can be categorized without knowing their IP addresses.\n- [IP/CIDR based](https://docs.cilium.io/en/stable/security/policy/language/#cidr-based \"https://docs.cilium.io/en/stable/security/policy/language/#cidr-based\"): used to describe the relationship to or from external services if the remote peer is not an endpoint.\n- [DNS based](https://docs.cilium.io/en/stable/security/policy/language/#dns-based \"https://docs.cilium.io/en/stable/security/policy/language/#dns-based\"): Selects remote, non-cluster, peers using DNS names converted to IPs via DNS lookups. It shares all limitations of the [IP/CIDR based](https://docs.cilium.io/en/stable/security/policy/language/#cidr-based \"https://docs.cilium.io/en/stable/security/policy/language/#cidr-based\") rules above. DNS information is acquired by routing DNS traffic via a proxy, or polling for listed DNS targets. DNS TTLs are respected.\n\n### Layer 4 policy\nLayer 4 policy can be specified in addition to layer 3 policies or independently. It restricts the ability of an endpoint to emit and/or receive packets on a particular port using a particular protocol.\n\n### Layer 7 policy\nLayer 7 policy rules are embedded into Layer 4 rules and can be specified for ingress and egress.\n- **path:** extended POSIX regex matched against the path of a request\n- **method:** extended POSIX regex matched against the method of a request, e.g. `GET`, `POST`, `PUT`, `PATCH`, `DELETE`, ‚Ä¶\n- **host:** extended POSIX regex matched against the host header of a request, e.g. `foo.com`\n- **headers:** list of HTTP headers which must be present in the request\n\n\u003e [!warning] \n\u003e Warning \n\u003e \n\u003e DNS Proxy intercepts egress DNS traffic and records IPs seen in the responses. This interception is, itself, a [separate policy rule governing the DNS requests](https://docs.cilium.io/en/stable/security/policy/language/#dns-proxy \"https://docs.cilium.io/en/stable/security/policy/language/#dns-proxy\"), and must be specified separately\n\n\u003e [!note]\n\u003e Note\n\u003e \n\u003e Unlike layer 3 and layer 4 policies, violation of layer 7 rules does not result in packet drops. Instead, if possible, an application protocol specific access denied message is crafted and returned, e.g. an _HTTP 403 access denied_ is sent back for HTTP requests which violate the policy, or a _DNS REFUSED_ response for DNS requests.\n\n\u003e [!note]\n\u003e Note\n\u003e \n\u003e Layer 7 rules are not currently supported in [Host Policies](https://docs.cilium.io/en/stable/security/policy/language/#hostpolicies \"https://docs.cilium.io/en/stable/security/policy/language/#hostpolicies\"), i.e., policies that use [Node Selector](https://docs.cilium.io/en/stable/security/policy/intro/#nodeselector \"https://docs.cilium.io/en/stable/security/policy/intro/#nodeselector\").\n\n### Egress gateway\n\n[Egress Gateway ‚Äî Cilium 1.14.0 documentation](https://docs.cilium.io/en/stable/network/egress-gateway/\n\nEgress gateway feature routes all IPv4 connections originating from pods and destined to specific cluster-external CIDRs through particular nodes, from now on called ‚Äúgateway nodes‚Äù. When enabled, matching packets that leave the cluster are masqueraded with selected, predictable IPs associated with the gateway nodes.\n\n\u003e [!info]\n\u003e Info\n\u003e \n\u003e As an example, this feature can be used in combination with legacy firewalls to allow traffic to legacy infrastructure only from specific pods within a given namespace. The pods typically have ever-changing IP addresses, and even if masquerading was to be used as a way to mitigate this, the IP addresses of nodes can also change frequently over time.\n\n## Installation concerns\n\nThere is two installation modes for Cilium:\n\n- [Using the CLI](https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/ \"https://docs.cilium.io/en/stable/gettingstarted/k8s-install-default/\")\n- [Using an helm chart](https://docs.cilium.io/en/stable/installation/k8s-install-helm/ \"https://docs.cilium.io/en/stable/installation/k8s-install-helm/\")\n\nFor both configurations, the steps are as follows:\n- Patch managed node groups taints\n- Install cilium with helm chart\n- Restart unmanaged pods\n- Validate installation\n\nSome steps, such as patching the [aws vpc cni](thoughts/aws%20vpc%20cni.md), are not explicit in the CLI documentation and may be hidden, so we need to compare the two implementations to see if any differences exist and what they imply. Same for [ENI integration as a requirement or not](https://docs.cilium.io/en/stable/network/concepts/routing/#aws-eni \"https://docs.cilium.io/en/stable/network/concepts/routing/#aws-eni\").\n\nFor the Helm chart, this means that some steps are manual. We can check out a way to manage our own Helm chart that includes Cilium's and add a Job to perform these steps.\n\nFor the observability part with Hubble, this is the same as for Cilium, we can install with the [CLI or an Helm chart](https://docs.cilium.io/en/stable/gettingstarted/hubble_setup/ \"https://docs.cilium.io/en/stable/gettingstarted/hubble_setup/\"), with some configuration:\n- [TLS certificates](https://docs.cilium.io/en/stable/gettingstarted/hubble-configuration/#tls-certificates \"https://docs.cilium.io/en/stable/gettingstarted/hubble-configuration/#tls-certificates\") (with [cert-manager](https://docs.cilium.io/en/stable/gettingstarted/hubble-configuration/#auto-generated-certificates-via-cert-manager \"https://docs.cilium.io/en/stable/gettingstarted/hubble-configuration/#auto-generated-certificates-via-cert-manager\")? or [auto-generated via Helm](https://docs.cilium.io/en/stable/gettingstarted/hubble-configuration/#auto-generated-certificates-via-helm \"https://docs.cilium.io/en/stable/gettingstarted/hubble-configuration/#auto-generated-certificates-via-helm\"))\n- [Enable UI](https://docs.cilium.io/en/stable/gettingstarted/hubble/#enable-the-hubble-ui \"https://docs.cilium.io/en/stable/gettingstarted/hubble/#enable-the-hubble-ui\")\n\n### Specific Configurations\n#### Tunnel mode\n- **Overlay:** Encapsulation-based virtual network spanning all hosts. Currently [VXLAN](https://en.wikipedia.org/wiki/Virtual_Extensible_LAN \"https://en.wikipedia.org/wiki/Virtual_Extensible_LAN\") (default) and [Geneve](https://www.redhat.com/en/blog/what-geneve \"https://www.redhat.com/en/blog/what-geneve\") are baked in but all encapsulation formats supported by Linux can be enabled.\n- **Native routing:** (see integration with Amazon VPC CNI section)\n\n#### Bandwith manager\n[Bandwidth Manager ‚Äî Cilium 1.14.0 documentation](https://docs.cilium.io/en/stable/network/kubernetes/bandwidth-manager/)\n\n#### Kube-proxy replacement\n[eCHO Episode 53: Life of a Packet in Cilium Continued - YouTube](https://www.youtube.com/watch?v=SGfMEpjq07Q\u0026list=PLDg_GiBbAx-mY3VFLPbLHcxo6wUjejAOC\u0026index=47)\n[Kubernetes Without kube-proxy ‚Äî Cilium 1.9.18 documentation](https://docs.cilium.io/en/v1.9/gettingstarted/kubeproxy-free/#kube-proxy-hybrid-modes)\n\n|**Pros**|**Cons**|\n|---|---|\n|One of the most, if not the most advanced CNI plugin|Not integrated with Amazon VPC CNI as it ([pods with Security Group will not be impacted by the network policies](https://aws.github.io/aws-eks-best-practices/networking/sgpp/#enforcing-mode-use-standard-mode-in-the-following-situations \"https://aws.github.io/aws-eks-best-practices/networking/sgpp/#enforcing-mode-use-standard-mode-in-the-following-situations\") as they are on a branch ENI)|\n|Open source|Network skills will be needed to understand what's going on and to be able to debug.|\n|Manage toFQDN egress endpoints||\n|User friendly observability with Hubble||\n","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/container-network-inteface-plugin":{"title":"Container Network Interface plugin","content":"\n[Kubernetes](thoughts/Kubernetes.md) does not implement such things as mulit-host networking (i.e. Pod-to-Pod communication across nodes). The Container Network Interface (CNI) is a framework for dynamically configuring network resources. \n\nThe plugin specification defines an interface for configuring network, provisioning IP addreses, and maintaining connectivity with multiple hosts:\n- Pods have their own IP addresses\n- Nodes manages pod subnets and allocates pod IPs locally\n- The communication/access from one pod to another pod is through CNI (L2 bridge)\n- The CNI plugin creates interface, used as interface between container runtime and network, and to configure network routes.\n- The interface communicates to another node through an overlay network ([VXLAN](https://en.wikipedia.org/wiki/Virtual_Extensible_LAN \"https://en.wikipedia.org/wiki/Virtual_Extensible_LAN\"), [BGP](https://en.wikipedia.org/wiki/Border_Gateway_Protocol \"https://en.wikipedia.org/wiki/Border_Gateway_Protocol\") etc.) \n\n![Pod networking and communication](thoughts/images/pod_networking_communication.png)\n\n\n## Links\n\n\n","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Analytical-Data":{"title":"Analtical Data","content":"\n## In Nutshel\nAnalytical data is the temporal, historic, and often aggregated view of the facts of the business over time. It is modeled to provide retrospective or future-perspective insights.\n\n\u003e [!summary] \n\u003e Summary\n\u003e  \n\u003e This is used to *optimize* the business and user experience. This the data that fuels organization's AI and analytics aspirations.\n","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Data-Contract":{"title":"Data Contract","content":"\n## In Nutshell\nIt's an API-like formal **agreement** between the [Data Product](thoughts/data/Data%20Product.md) and the [data product](thoughts/data/Data%20Product.md) consumer. It specifies the guarantees about a provided data set and expectations concerning [Data Product](thoughts/data/Data%20Product.md) usage. The goal is to generate a well-modeled, high-quality, trusted data.\n\n## Specification\n### Promises\n\u003e [!info]\n\u003e Info\n\u003e \n\u003e It declares the intent of the port. Promises are not guarantee of the outcome but the [Data Product](thoughts/data/Data%20Product.md) will behave accordingly to them to realize its intent.\n\u003e \n\u003e The more a [Data Product](thoughts/data/Data%20Product.md) keeps its promises over time the more trustworthy it is.\n\nWe find:\n- [Service-level objectives](thoughts/data/Service-level%20objectives.md): such as latency, availability\n- Deprecation policy\n\n### Expectations\n\u003e [!info]\n\u003e Info\n\u003e  \n\u003e It declares how it wants the port to be used. They are a way to explicitly state what promises the data product would like consumers to make regarding how they will use the port\n\nWe find:\n- [Data Product](thoughts/data/Data%20Product.md) consume, including team, and responsible contact.\n- [Semantic model](thoughts/data/Semantic%20model.md): the meaning\n- [Syntax model](thoughts/data/Syntax%20model.md): the structure of the exchanged data\n- Purpose of data usage\n- Terms, such as query intervals and data processing volumes\n- Start data\n- End data\n- Notice period, to cancel the contract\n\n\n### Contracts\n\u003e [!info]\n\u003e Info\n\u003e  \n\u003e ¬†A contract is an explicit agreement between the data product and its consumers. It is used to group all the promises and expectations that if not respected can generate penalties like monetary sanctions or interruption of service\n\nIt is the format used to state of the agreement, including:\n- [Data Product](thoughts/data/Data%20Product.md) provider, including team, owner, and the [output ports](thoughts/data/output%20ports.md) to access.\n- Level of data access required\n- Service-level agreement\n- Costs\n\n\u003e [!important]\n\u003e Important\n\u003e  \n\u003e Data contracts are the link between [Data Products](thoughts/data/Data%20Product.md). They refer to a specific version of an output port. Become active when a consumer requests data access and the provider accepts the request.\n\u003e \n\u003e Data contracts can be canceled by providers or consumers with a defined notice period (e.g when the cost become too high, or when the provider needs to implement a breaking change).\n\u003e \n\u003e Data contracts provide stability, trust, and quality within the [Data Mesh](thoughts/data/Data%20Mesh.md). They can be used to visualize data flow across [Data Products](thoughts/data/Data%20Product.md).\n\n## Specification\n\n- [data-contract-template by Paypal](https://github.com/paypal/data-contract-template/tree/main/docs)\n- [Data Contract - Open Data Mesh](https://dpds.opendatamesh.org/concepts/data-contract/)\n- [Data Contract Specification | The Data Contract Specification Repository](https://datacontract-specification.com/)\n\n\n## One step further\nData contracts can also be used for automation: As soon as a data contract has been concluded, permissions for the [output port](thoughts/data/output%20ports.md) can be set up automatically in the [Self-Serve Data Platform](thoughts/data/Self-Serve%20Data%20Platform.md) on an event basis. When the contract is terminated, the permissions are automatically deleted again.\n\n## Sources\n- [Data Mesh Architecture](https://www.datamesh-architecture.com/#data-contract)\n- [Data Contract - Open Data Mesh](https://dpds.opendatamesh.org/concepts/data-contract/)\n- [Data Contracts 101 - How They Work, Why They're Important](https://www.montecarlodata.com/blog-data-contracts-explained/)\n- [What is a Data Contract?](https://glossary.airbyte.com/term/data-contract/)\n- [The next generation of Data Platforms is the Data Mesh | by Jean-Georges Perrin | The PayPal Technology Blog](https://medium.com/paypal-tech/the-next-generation-of-data-platforms-is-the-data-mesh-b7df4b825522)\n","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Data-Mesh":{"title":"Data Mesh","content":"## Data culture before Data Mesh\nData collection, experimentation, and intelligence were outsourced to a separate data team. The data team was under sheer pressure. Domains did not have trust in the data, or often couldn‚Äôt find the data they needed. \n\nThe domains themselves had taken no responsibility and interest in making data readily available, reliable, and usable.\n\n## In Nutshell\nData mesh is a decentralized sociotechnical approach to share, access, and manage analytical data in complex and large-scale environments - within or across organizations.\n\n### The shift\nData mesh introduces multidimensional technical and organizational shifts from earlier analytical data management approaches.\n\n![Data mesh dimensions of change](thoughts/data/img/data_mesh_dimensions_of_change.png)\n\n### The principles\n- [Domain ownership](thoughts/data/Domain%20ownership.md)\n- [Data Product](thoughts/data/Data%20Product.md)\n- [Self-Serve Data Platform](thoughts/data/Self-Serve%20Data%20Platform.md)\n- [Federated Computational Governance](thoughts/data/Federated%20Computational%20Governance.md)\n\nThe interplay of the principles:\n\n![](thoughts/data/img/data_mesh_principles_interplay.png)\n\n## At a glance\n\n![](thoughts/data/img/data_mesh_principles_operating_model.png)\n\n## The Data\n\nData mesh focuses on [Analytical Data](thoughts/data/Analytical%20Data.md), against the [Operational Data](thoughts/data/Operational%20Data.md).\n\n## Why data mesh ?\n### The inflection point\n![](thoughts/data/img/data_mesh_macro_drivers_for_it_creation.png)\n\n### After the inflection point\n#### Respond Gracefully to Change in a Complex Business\nThe principle of Data mesh carries out the same decomposition for [Analytical Data](thoughts/data/Analytical%20Data.md), resulting in the [Domain ownership](thoughts/data/Domain%20ownership.md) of data.\n[Domain ownership](thoughts/data/Domain%20ownership.md) results in a distributed architecture, where the data artifacts - datasets, code, metadata, and [computational policies](thoughts/data/computational%20policies.md) - are maintained by their corresponding domains\n![](thoughts/data/img/data_mesh_aligning_business_tech_data_to_manage_complexity.png)\n#### Reduce Accidental Complexity of Pipelines and Copying Data\nData Mesh addresses this problem by creating a new architectural unit that encapsulates a domain-oriented [data semantic](thoughts/data/Semantic%20model.md) while also providing multiple modes of access to the data suitable for different use cases and users. This architectural unit is called #dataquantum .\n\n#### Sustain Agility in the Face of Growth\n##### Remove Centralized and monolithic Bottlenecks\nA centralized data team managing a monolithic data lake or warehouse limits agility, particularly as the number of sources to onboard or number of use cases to serve grow.\n\nEach [Data Product](thoughts/data/Data%20Product.md) provides versioned interfaces that allow peer-to-peer consumption of data. The data from multiple [Data Products](thoughts/data/Data%20Product.md) can be composed and aggregated into new higher-order  [Data Products](thoughts/data/Data%20Product.md).\n![](thoughts/data/img/data_mesh_removes_centralized_architecture_bottlenecks.png)\n##### Reduce Coordination of Data Pipelines\nIn case where a new use case requires access to a new [Data Product](thoughts/data/Data%20Product.md) outside of the domain, the consumer can make progress utilizing the [standard contracts](thoughts/data/Data%20Contract.md) of the new [Data Product](thoughts/data/Data%20Product.md), mocks, stub, or synthetic data interfaces until the [Data Product](thoughts/data/Data%20Product.md) becomes available. This is the beauty of [Data Contract](thoughts/data/Data%20Contract.md), as they ease the coordination between consumer and provider during development.\n\n![](thoughts/data/img/reduce_architectural_coordination_of_pipelines.png)\n\n##### Reduce Coordination of Data Gouvernance\nData mesh reduces governance coordination friction through two functions:\n- Automating and embedding [policies as code](thoughts/data/computational%20policies.md) in each [Data Product](thoughts/data/Data%20Product.md)\n- Delegating central responsibilities of governance to [Federated Computational Governance](thoughts/data/Federated%20Computational%20Governance.md).\n![](thoughts/data/img/reduce_sync_of_data_governance.png)\n\n##### Enable Autonomy\nEmpirical studies show that teams' freedom in decision making to fulfill their mission can lead to better team performance. On the other hand, too much autonomy can result in inconsistencies, duplicated efforts, and team isolation.\n\nData Mesh gives domain teams autonomy to build and maintain their [Data Products](thoughts/data/Data%20Product.md), while it places a domain-agnostic [Data Platform](thoughts/data/Self-Serve%20Data%20Platform.md) in place for teams to do so in a consistent and cost-effective way. \n\nThe principle of [Self-Serve Data Platform](thoughts/data/Self-Serve%20Data%20Platform.md) essentially makes it feasible for domain teams to manage the life cycle of their [Data Products](thoughts/data/Data%20Product.md) with autonomy and utilize the skillsets of their generalist developper to do so.\n\n#### Increase the Ratio of Value from Data to Investment\nData mesh looks critically at the existing technology landscape and reimagines the technologies solutions as a data-product-developer (or user)-centric platform. It intends to remove the need for data specialists and enable generalist experts to develop data products.\n\n##### Embed Product Thinking Everywhere\nData mesh shifts the measure of success of the [Platform](thoughts/data/Self-Serve%20Data%20Platform.md) from the number of capabilities to the impact of its capabilities on improving the experience of [Data Product](thoughts/data/Data%20Product.md) development and reducing the lead time to deliver, or discover and user, a [Data Product](thoughts/data/Data%20Product.md).\n\nProduct thinking leads to reduced effort and cost, hidden in the everyday experience of [Data Product](thoughts/data/Data%20Product.md) users and developers.\n\n### Before the Inflection Point\n#### First Generation: Data Warehouse Architecture\nData warehousing architecture today is influenced by early concept such as facts and dimensions formulated in the 1960s. Data is:\n- Extracted from many operational databases and sources\n- Transformed into a universal schema - represented in a multidimensional and time-variant tabular format.\n- Loaded into the warehouse tables\n- Accessed through SQL-like queries\n- Mainly serving [Data analysts](thoughts/data/Data%20analyst.md) for reporting and analytical visualization use cases\n\nThe data warehouse approach has been refined to Data Marts with the common distinction that a data marts serves a single department in an organization, while a data warehouse serves the larger organization integrating across multiple departments.\n\nOver time, they include thousands of ETL jobs, tables, and reports that only specialized group can understand and maintain.\n\n![](thoughts/data/img/analytical_data_architecture_warehouse.png)\n#### Second Generation: Data Lake Architecture\nData lake architecture, similar to a data warehouse, assumes that data gets extracted from operational systems and loaded into a central repository often in the format of an object store - storage of any type of data.\n\nOnce the data becomes available in the lake, the architecture gets extended with elaborate transformation pipelines to model the higher value data and store it in lakeshore marts or feature stores at the edge of the lake.\nNotable characteristics of a data lake architecture include:\n- Data is extracted from many operational databases and sources\n- Data represents as much as possible of the original content and structure\n- Data is minimally transformed to fit the popular storage formats (eg. Parquet, Avro, etc.)\n- Data - as close as possible to the source schema - is loaded to scalable object storage.\n- Data is accessed through the object storage interface - read as file or data frames, a two dimensional array-like structure\n- [Data scientists](thoughts/data/Data%20scientist.md) mainly access the lake storage for analytical and machine learning model training.\n- Downstream from the lake, lakeshore marts are created as fit-for-purpose  data marts\n- Lakeshore marts are used by applications and analytics use cases\n- Downstream from the lake, feature stores are created as fit-for-purpose columnar data modeled and stored for machine learning training.\n![](thoughts/data/img/analytical_data_architecture_data_lake.png)\nIt creates complex and unwieldy pipelines of batch or streaming jobs operated by a central team of hyper-specialized data engineers. It deteriorates over time. Its unmanaged datasets, which are often untrusted and inaccessible, provide little value. The data lineage and dependencies are obscured and hard to track.\n\n#### Third Generation: Multimodal Cloud Architecture\nMix of previous generations, with a few modern twists, as those data architectures:\n- Support streaming for near real-time data availability with architecture such as [Kappa Architecture - Where Every Thing Is A Stream](http://milinda.pathirage.org/kappa-architecture.com/)\n- Attempt to unify batch and stream processing for data transformation with frameworks such as [Apache Beam | Technology Radar | Thoughtworks](https://www.thoughtworks.com/radar/languages-and-frameworks/apache-beam).\n- Fully embrace cloud-based managed services and use modern cloud-native implementations with isolated compute and storage. They leverage the elasticity of the cloud for cost optimization\n- Converge the warehouse and lake into one technology, either extending the data warehouse to include embedded ML training, or alternatively building data warehouse integrity, transactionally, and querying systems into data lake solutions.\nThese third-generation data platforms are addressing some of the gaps of the previous generations such as real-time data analytics, while also reducing the cost of managing big data structure.\n\n#### Characteristics of Analytical Data Architecture\nDespite the undeniable innovation there are fundamental assumptions that have remained unchallenged for the last few decades and must be closely evaluated:\n- Data must be centralized to be useful - managed by a centralized organization, with an intention to have an enterpris-wide taxonomy\n- Data management architecture, technology, and organization monolithic.\n- The enabling technologies drive the paradigm - architecture and organziation.\n\n##### Mononlithic\n###### Monolithic architecture\n![](thoughts/data/img/view_of_the_monolithic_data_platform.png)\nMonolithic architecture goal is to:\n- *Ingest data* from all corners of the enterprise and beyond, ranging from operational and transactional systems in domains that run the business to external data providers that augment the knowledge of the enterprise.\n- *Cleanse, enrich, and transform* the data into trustworthy data that can address the needs of diverse set of customers.\n- *Serve* datasets to a variety of consumers with a diverse set of needs.\n\nThis architecture falls short as the solution scales. The organization complexity and proliferation of sources and use cases create tension and friction on the architecture and organization structure:\n\n\u003e [!danger]\n\u003e Ubiquitous data and source proliferation\n\u003e \n\u003e The ability to consume it all and harmonize it in one place, logically under the control of a decentralized platform and team diminishes.\n\n\u003e[!danger]\n\u003eOrganizations' innovation agenda and use case proliferation\n\u003e\n\u003eThe need for rapid experimentation by organizations introduces a larger number of use cases that consume data from the platform. This implies an ever-growing number of transformations to create data - aggregates, projections, and slices - that can satisfy the test and learn cycle of innovation\n\n\u003e [!danger]\n\u003e Organizational complexity\n\u003e \n\u003e When we add volatility and continuous change of the data landscape to the mix, the monolithic architecture becomes a synchronization and prioritization hell.\n\n###### Monolithic technology\nFor example Data Warehousing technologies such as Snowflake, Google BigQuery, and Synapse all have monolithic logical architecture.\n\nData lake technologies such as object storage and pipeline orchestration tools can be deployed in a distributed fashion. However, by default, they lead to the creation of monolithic lake architectures. \n\u003e[!example]\n\u003eExample\n\u003e\n\u003eData processing pipelines DAG definitions and orchestration lack constructs such as interfaces and contracts that abstract pipeline jobs dependencies and complexity, leading to a [Big Ball of Mud monolithic](http://laputan.org/mud/) architecture  with tightly coupled labyrinthic pipelines, where it si difficult to isolate change or failure to a single step in the process\n\n###### Monolithic organization\n![](thoughts/data/img/siloed_hyper_specialized_data_team.png)\n\n##### Centralized Data Ownership\nIt's an accepted convention that the monolithic Data Platform hosts and owns the data that belongs to different domains.\n![](thoughts/data/img/centralized_data_ownership_with_no_clear_domain_boundaties.png)\n\n##### Technology Oriented\nThe proposed solution architecture decomposes and then integrates the components of the architecture based on their technical function and the solution supporting the function.\n\n\u003e [!example]\n\u003e Example\n\u003e \n\u003e-  *Ingestion*: function supported by Cloud Pub/sub\n\u003e - *Publishing data*: to Cloud Sotage\n\u003e - *Serves data*: through BigQuery\n\nThis approach leads to a technically partitioned architecture and consequently an activity-oriented team decomposition.\n![](thoughts/data/img/example_of_modern_analytical_architecture_influenced_by_a_technology_oriented_decomposition.png)\n###### Technically partitioned architecture\nOne of the limitation of data management solutions today is how we have attempted to manage its unwieldy complexity, how we have decomposed an ever-growing monolithic data platform and team to smaller partitions. We have chosen the path of least resistance, a *technical partitioning*.\n\n\u003e [!summary] \n\u003e Technical partitioning\n\u003e \n\u003e It's a decomposition that is close to the implementation concerns than business domains concerns. Architects and leaders of monolithic data platforms have decomposed monolithic solutions based on data pipeline architecture into its technical functions such as *ingestion, cleansing, aggregation, enrichment, and serving*.\n\u003e The top-level functional decomposition leads to synchronization overhead and slow response to data changes. \n\u003e \n\u003e An alternative is a *top-level domain-oriented paritioning*, where these technical functions are embedded in every domain, where the change to the domain can be managed locally without top-level synchronization\n\n![](thoughts/data/img/top_level_technical_partitioning_of_monolithic_data_architecture.png)\n\n###### Activity-oriented team decomposition\nOne of the motivations behind breaking a system down into its architectural components is to create independent teams that can each build and operate an architectural component.\n\nThough this model provides some level of scale, by assigning teams to different activities of the flow, it has an inherent limitation that does not scale what matters: delivery of outcome - in this case, delivery of new quality and trustworthy data. Delivering an outcome demands synchronization between teams and aligning changes to the activities.\n\n![](thoughts/data/img/architecture_and_team_decomposition_is_orthogonal_to_the_axis_of_change__outcome.png)\n\n\u003e [!important] \n\u003e Note\n\u003e \n\u003e In recent years, there has been a move toward decomposing the central data team into domain-oriented data teams. While this is an improvement, it does not solve the issue of long-terms data ownership and data quality. The data team remains far from the actual domains, out of sync with domain system changes and their needs. This is an antipattern.","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Data-Product":{"title":"Data Product","content":"## In Nutshell\nWith [Domain ownership](thoughts/data/Domain%20ownership.md), the domain-oriented data is shared as a **product** directly with data users ([Data analyst](thoughts/data/Data%20analyst.md), [Data scientist](thoughts/data/Data%20scientist.md)).\n\nData as a product adheres to a set of usability charecteristics:\n- [Data Product - Discoverable](thoughts/data/Data%20Product%20-%20Discoverable.md)\n- [Data Product - Addressable](thoughts/data/Data%20Product%20-%20Addressable.md)\n- [Data Product - Understandable](thoughts/data/Data%20Product%20-%20Understandable.md)\n- [Data Product - Trustworthy \u0026 truthful](thoughts/data/Data%20Product%20-%20Trustworthy%20\u0026%20truthful.md)\n- [Data Product - Natively accessible](thoughts/data/Data%20Product%20-%20Natively%20accessible.md)\n- [Data Product - Interoperable](thoughts/data/Data%20Product%20-%20Interoperable.md)\n- [Data Product - Valuable on its own](thoughts/data/Data%20Product%20-%20Valuable%20on%20its%20own.md)\n- [Data Product - Secure](thoughts/data/Data%20Product%20-%20Secure.md)\n\nA data product provides a set of explicitly defined and easy to use [data sharing contracts](thoughts/data/Data%20Contract.md).\n\n\u003e [!important]\n\u003e Important\n\u003e \n\u003e Each data product is autonomous, and its life cycle and model are managed independently of others\n\n\u003e [!danger] \n\u003e data quantum\n\u003e \n\u003e Data as a product introduces a new unit of logical architecture called #dataquantum, controlling and encapsulating all the structural components needed to share data as a product:\n\u003e - data\n\u003e - metadata\n\u003e - code\n\u003e - policy\n\u003e - declaration of infrastructure dependencies\n\n## Motivation\n- Remove the possibility of creating domain-oriented data silos by changing relationship of teams with data. \n- Creates a data-driven innovation culture, by streamlining the experience of discovering and using high-quality data, peer-to-peer, without friction.\n- Create resilience to change with built-time and run-time isolation between data products and explicitly defined [data sharing contracts](thoughts/data/Data%20Contract.md) so that changing one does not destabilize others.\n\n## Specification\n\nTreating data as a product requires to introduce new roles to domains such as [Domain data product owner](thoughts/data/Domain%20ownership.md) and data product developer who have responsibility for:\n- creating\n- serving\n- evangelizing\ndata products, while maintaining the specific objective measures of data accessibility, quality, usability, and availability over the lifetime of the data products.\n\n### Baseline Usability Attributes of a Data Product\n\n![](thoughts/data/img/data_product_baseline_usability_attributes.png)\n\n### Applying Product Thinking to Data\n\nOperational technology must be treating like a product. Applying product management techniques to internal platforms, which accelerates the ability of internal developers to build and host solutions on top of internal platform (e.g. Spotify Backstage)\n\n- [Data Product Specification | The Data Product Specification](https://dataproduct-specification.com/)\n- [Data Mesh Architecture: Designing Data Products](https://www.datamesh-architecture.com/data-product-canvas)\n\n### Inspiration from software development\n- [Backstage by Spotify | Supercharged developer portals](https://backstage.spotify.com/)\n- [Square API Reference](https://developer.squareup.com/reference/square)\n\n### Transition to Data as a Product\n\n\n\u003e[!important]\n\u003eReframe upstream data from ingestion to consumption\n\u003e\n\u003eThe subtle difference is that the upstream data is already cleansed, processed and served ready for consumption\n\n\n\u003e[!important]\n\u003eReframe extraction to \n\u003e\n\u003epublish serve or share\n\n\u003e [!warning]\n\u003e Data as a Product, Not a Mere Asset\n\u003e \n\u003e Change the metaphor to data as a product, and a change of perspective that comes with that: measuring success through: \n\u003e - **adoption of data**\n\u003e - **number of users**\n\u003e - **satisfaction using the data**\n\n\u003e [!tip] \n\u003e Trust-But-Verify Data culture\n\u003e \n\u003e These practices include introducing a role for long-term ownership of a data product, accountable for the integrity, quality, availability, and other usability characteristics of the Data Product; introducing the concept of a Data Product that only shares data but also explicitly shares a set of objective measures such as timeless, retention and accuracy; and creating a data product development process that automates testing of the data product.\n\n","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Data-Product-Addressable":{"title":"Data Product - Addressable","content":"\nThe addressing system accommodates the following ever-changing aspects of a [Data Product](thoughts/data/Data%20Product.md), among others, while retaining access to the [Data Product](thoughts/data/Data%20Product.md) through its long-lasting **unique address**:\n- [Semantic model](thoughts/data/Semantic%20model.md) and [Syntax model](thoughts/data/Syntax%20model.md) changes (schema evolution)\n- **Continuous release of new data over time (window):** Partitioning strategy and grouping of data tuples associated with a particular time (or time window)\n- **Newly supported modes of access to data**: New ways of serializing, presenting, and querying the data\n- **Changing runtime behavioral information**: For example, [Service-level objectives](thoughts/data/Service-level%20objectives.md), access log, debug, logs","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Data-Product-Discoverable":{"title":"Data Product - Discoverable","content":"\nThe [Data Product](thoughts/data/Data%20Product.md) itself **intentionally** provides discoverability information. It relies on individuals [Data Products](thoughts/data/Data%20Product.md) information at different points of their life cycle - build, deploy, and run - in a standard way.\n\nIt [Data Product](thoughts/data/Data%20Product.md) continuously shares its:\n- source of origin\n- owners\n- runtime information such as:\n\t- timeless\n\t- quality metrics\n\t- sample datasets\n- Information contributed by their consumes such as the top use cases and applications enabled by their data.\n","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Data-Product-Interoperable":{"title":"Data Product - Interoperability","content":"\nHere are few things [Data Product](thoughts/data/Data%20Product.md) need to standardize to facilitate interoperability and composability:\n- `Field type`: A common explicitly defined type system\n- [Polysemes](thoughts/data/polysemes.md) identifiers\n- `Data product global address`: A unique global address allocated to each data product, ideally with a uniform scheme for ease of establishing connections to different [Data Product](thoughts/data/Data%20Product.md)\n- `Common metadata fields`: Sucs as representation of time when data occurs and when data is recorded\n- `Schema linking`: Ability to link and reuse schema - types - defined by other  [Data Products](thoughts/data/Data%20Product.md)\n- `Data linking`: Ability to ling or map to data in other  [Data Products](thoughts/data/Data%20Product.md)\n- `Schema stability`: Approach to evolving schemas that respects backward compatibility\n","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Data-Product-Natively-accessible":{"title":"Data Product - Natively accessible","content":"\n![](thoughts/data/img/data_product_users_spectrum.png)\n\nThere is a direct link between the usability of a [Data Product](thoughts/data/Data%20Product.md) and how easily a particular data user can access it with their native tools. Hence, a [Data Product](thoughts/data/Data%20Product.md) needs to make it possible for various data users to access and read its data in their native mode of access. \n\n\u003e [!hint] \n\u003e Hint\n\u003e \n\u003e This can be implemented as a polyglot storage of data or by building multiple read adapters\n\n","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Data-Product-Secure":{"title":"Data Product - Secure","content":"\nIn a distributed architecture like [Data Mesh](thoughts/data/Data%20Mesh.md), the access control is validated by the [Data Product](thoughts/data/Data%20Product.md), right in the flow of data, access, read, or write.\n\nAccess control policies can be defined centrally but enforced at runtime by each individual [Data Product](thoughts/data/Data%20Product.md). \n\n[Data Product](thoughts/data/Data%20Product.md) follow the practice of [Security policy as code | Technology Radar | Thoughtworks](https://www.thoughtworks.com/radar/techniques/security-policy-as-code). This means to write security policies in a way that they can be versioned, automtically tested, deployed and observed, and computationally evaluated and enforced:\n- `Access control`\n- `Encryption`\n- `Confidentiality levels`\n- `Data retention`\n- `Regulations and agreements`\n","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Data-Product-Trustworthy-truthful":{"title":"Data Product - Trustworthy \u0026 truthful","content":"A piece of closing the gap is to guarantee and communicate [Data Products'](thoughts/data/Data%20Product.md) [Service-level objectives](thoughts/data/Service-level%20objectives.md) (SLOs) - objective measures that remove uncertainty surrounding the data.\n\n[Data Mesh](thoughts/data/Data%20Mesh.md) introduces a fundamental shift that the owners of the [Data Product](thoughts/data/Data%20Product.md) must communicate and guarantee an acceptable level of quality and trustworthiness - specific to their domain - as an intrinsic characteristic of the [Data Product](thoughts/data/Data%20Product.md).","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Data-Product-Understandable":{"title":"Data Product - Understandable","content":"\nEach [Data Product](thoughts/data/Data%20Product.md) provides [semantically](thoughts/data/Semantic%20model.md) coherent data: data with a specific meaning. A data user needs to understand this meaning; what kind of entities the [Data Product](thoughts/data/Data%20Product.md) encapsulates, what the relationships among the entities are, and their adjacent [Data Products](thoughts/data/Data%20Product.md).\n\nIn addition to understanding the [semantics](thoughts/data/Semantic%20model.md), data users need to understand how exactly the data is presented to them: \n- How is it serialized, and how can they access and query the data syntactically? \n- What kind of queries can they execute or how can they read the data objects?\nThey need to understand the schema of the underlying [Syntax of data](thoughts/data/Syntax%20model.md). \n\n\u003e [!info]\n\u003e  Information\n\u003e  \n\u003e  Sample datasets and exemple consumer codes ideally accompany this information\n\nAdditionally, dynamic and computational documents like Notebooks are great companions to tell the story of a [Data Product](thoughts/data/Data%20Product.md).","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Data-Product-Valuable-on-its-own":{"title":"Data Product - Valuable on its own","content":"\nA [Data Product](thoughts/data/Data%20Product.md) should carry a dataset that is valuable and meaningful on its own - without being joined and correlated with other [Data Products](thoughts/data/Data%20Product.md).\n\nIf a [Data Product](thoughts/data/Data%20Product.md) on its own serves no value on its own, it should not exist.\n\n\u003e [!warning]\n\u003e In Data Warehouse\n\u003e \n\u003e There are glue (aka facts) tables that optimize correlation between entities. These are identity tables that map identifiers of one kind of entity to another. Such identity tables are not meaningful or valuable on their own - without being joined to other tables. They are simply mechanical implementation to facilitate joins.\n\n\n\u003e [!warning]\n\u003e In Data Mesh\n\u003e \n\u003e There are not mechanical [Data Product](thoughts/data/Data%20Product.md) that solely exist to enable the machines to correlate information across the mesh. Machine optimizations such as indices or fact tables must be automatically created by the [Platform](thoughts/data/Self-Serve%20Data%20Platform.md) and hidden from the product products.\n","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Data-analyst":{"title":"Data Analyst","content":"","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Data-scientist":{"title":"Data Scientist","content":"","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Domain-ownership":{"title":"Data Ownership","content":"\n## In Nutshell\n\nDecentralize the ownership of analytical data to business domains closest to the data - either the source of the data or its main consumers.\n\n## Motivations\n\nArchitecturally and organizationally align business, technology, and analytical data:\n- The ability to scale out data sharing aligned with the axes of organizational growth.\n- Optimization for continuous change by localizing change to the business domains.\n- Enabling agility by reducing cross-team synchronizations and removing centralized bottlenecks of data teams, wharehouses, and lake architecture.\n\n## Strategy\n\nTo find the axis of data decomposition, data mesh follows the seams of organizational units. It follows the lines of division of responsability aligned with the business. \n\nThe traditional architectures mirror an organizational structure that centralizes the responsability of sharing analytical data to a single data team.\n\nData mesh gives the data sharing responsability to each of the business domain: This the principle of domain ownership.\n\n\u003e [!summary]-\n\u003e In short\n\u003e \n\u003e Product ownership: long-term ownership of responsabilites to:\n\u003e - create\n\u003e - model\n\u003e - maintain\n\u003e - evolve\n\u003e - share\n\u003e\n\u003edata as a product to meet the needs of data users\n\u003e \u003e [!warning]\n\u003e \u003e Warning\n\u003e \u003e \n\u003e \u003e Should not be confused with data sovereignty, control of data by whom it is collected from. The ultimate sovereignty of data remains with the users, customers, or other organizations whose data is being captured and managed. The organizations act as a data product owners while the individuals remain the data owners.\n\u003e \u003e \u003e [!info]  \n\u003e \u003e \u003e Self-sovereign data\n\u003e \u003e \u003e \n\u003e \u003e \u003e individuals having full control and authority over their personal data.\n\n\n### New challenges\n\nIt leads to a distributed logical data architecture. \n\n\u003e [!note]- \n\u003e Distributed architecture\n\u003e \n\u003e They are scalable but are more complex to manage. It requires new ways of handling data interoperability and connectivity between domains\n\n### Applying DDD's strategy to Data\n\nMove away from the previously used modes of modeling and ownership:\n\n\u003e [!hint]- \n\u003e Organization-level central modeling\n\u003e \n\u003e Centralized modeling leads to organization bottlenecks for change.\n\n\u003e [!hint]- Silos of internal models with limited integration\n\u003e \n\u003e This mode introduces cumbersome interteam communications. This is similar to data silos in different application, connected via brittle extract, transform, load procedures (ETL)\n\n\u003e [!hint]- \n\u003e No intentional modeling\n\u003e \n\u003e Similar to a data lake, dumping raw data into blob storage\n\nInstead DDD's strategic design embraces modeling based on multiple modes each contextualized to a particular domain, called a [bounded context](thoughts/bounded%20context.md).\n\nAdditionally, DDD introduces `context mapping`, which explicitly defines the relationship between [bounded contexts](thoughts/bounded%20context.md), the independent models.\n\n[Data Mesh](thoughts/data/Data%20Mesh.md) adopt the boudary of [bounded context](thoughts/bounded%20context.md) to individual [Data Products](thoughts/data/Data%20Product.md) - data, its models, and its ownership.\n\n![](thoughts/data/img/data_ownership_decomposing.png)\n\n### Domain Data archetypes\n\nThere are three archetypes of domain-oriented data:\n- [source-aligned domain data](thoughts/data/source-aligned%20domain%20data.md)\n- [aggregate domain data](thoughts/data/aggregate%20domain%20data.md)\n- [consumer-aligned domain data](thoughts/data/consumer-aligned%20domain%20data.md)\n\n![](thoughts/data/img/domain_data_archetype.png)\n\n### Define multiple connected models\n\nThe could be multiple models of the same concept in different domains and that is OK. \n\n\u003e [!example] \n\u003e Example\n\u003e \n\u003e The artist representation in the `payment` includes payment attributes, which is very different from the `artist` model in the `recommandation` domain\n\nThe mesh should allow mapping of same concept (aka. [polysemes](thoughts/data/polysemes.md)) from one domain to another to be able to link them. There are multiple ways to achieve this, including a unique identification scheme, a single ID used by all domains that include the same concept: \n\n### Embrace the most relevant Domain Data: Don't expect a Single source of truth\n\nThis is a wonderful idea: prevent multiple copies of out-of-date data and the sprawl of untrustworthy data. But in reality it's proved costly, an impediment to scale an speed or simply achievable.\n\n[Data Mesh](thoughts/data/Data%20Mesh.md) does not enforce the idea of one source of truth. However, it places multiple practices in place that reduces the likelihood of multiple copies of out-of-date data.\n\nLong-term domain-oriented ownership with accountability to share discoverable, high-quality, and usable data - in multiple modes for analysts and scientists - reduces the need for copying and keeping stale data around.\n\nData can be read from one domain and transformed an stored by another domain.\n\n### Hide the Data Pipelines as Domain's internal implementation\n\nIn [Data Mesh](thoughts/data/Data%20Mesh.md) a data pipeline is simply an internal implementation of the Data domain and is handled internally within the domain.\n\n\u003e [!example] \n\u003e Example \n\u003e \n\u003e The [source-aligned domain](thoughts/data/source-aligned%20domain%20data.md) need to include the cleansing, deduplicating, and enriching of their domain events so that they can be consumed by other domains, without replication of cleansing downstream.\n\nThe aggregation stages of a centralized pipeline move into the implementation details of [aggregate domain data](thoughts/data/aggregate%20domain%20data.md) or [consumer-aligned domain data](thoughts/data/consumer-aligned%20domain%20data.md).\n\n\u003e [!important] \n\u003e Important\n\u003e \n\u003e One might argue that this model leads to duplicated effort in each domain to create their own data processing pipeline implementation, technology stack, and tooling. [Data Mesh](thoughts/data/Data%20Mesh.md) adresses this concern with the [Self-Serve Data Platform](thoughts/data/Self-Serve%20Data%20Platform.md)","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Federated-Computational-Governance":{"title":"Federated Computational Governance","content":"## In Nutshell\nIt embraces constant change to the data landscape.\nIt delegates the responsibility of modeling and quality of the data to individuals [domains](thoughts/data/Domain%20ownership.md), and heavily automates the computational instructions that assure data is:\n- secure\n- compliant\n- of quality\n- usable\n\nRisk is managed early in the life cycle of data, and throughout, in an automated fashion. It embeds the [computational policies](thoughts/data/computational%20policies.md) in each and every [domains](thoughts/data/Domain%20ownership.md) and [Data Product](thoughts/data/Data%20Product.md). [Data Mesh](thoughts/data/Data%20Mesh.md) calls this model of governance a *federated computational governance*.\n\n## Motivations\nData Governance teams and processes have noble objectives: ensuring the availability of:\n- safe\n- high quality\n- consistent\n- compliant\n- privacy-respecting\n- usable\ndata across an organization with managed risk. However, traditionally, our approach in achieving them has been a point of friction. \n\nGovernance has relied heavily on:\n- manual interventions\n- complex central processes of data validation and certification\n- establishing global canonical modeling of data with minimal support for change\n- often engaged too late after the fact.\n\nIn a [Data Mesh](thoughts/data/Data%20Mesh.md) organization we need:\n- Ability to get higher-order value from aggregation and correlation of independent yet interoperable data products\n- Countering the undesirable consequences of domain-oriented decentralizations\n- Reducing the overhead of manual synchronization between domains and the governance function\n\n## Specification\nFederated and computational governance is a decision-making model led by the federation of [Domain](thoughts/data/Domain%20ownership.md) [Data Product](thoughts/data/Data%20Product.md) owners and [Data Platform](thoughts/data/Self-Serve%20Data%20Platform.md) product owners, with autonomy and domain-local decision-making power, while creating and adhering to a set of global rules - rules applied to all [Data Products](thoughts/data/Data%20Product.md) and their interfaces. \n\nThe global rules are informed and enabled by global specializations such as legal and security to ensure [trustworthy](thoughts/data/Data%20Product%20-%20Trustworthy%20\u0026%20truthful.md), [secure](thoughts/data/Data%20Product%20-%20Secure.md), and [interoperable](thoughts/data/Data%20Product%20-%20Interoperable.md) system.\n\n\u003e[!important]\n\u003eImportant\n\u003e\n\u003eIt addresses what is considered one of the most common mistakes of today's governance: being an IT initiative with an organizational model that parallels business and is not embedded within the business\n\n### Components\n![](thoughts/data/img/data_mesh_federated_computational_governance_components.png)\n#### Apply systems thinking to Data Mesh Governance\n\n\u003e [!info]\n\u003e Systems thinking\n\u003e \n\u003e Is the discipline of \"seing the whole\", shifting our focus \"from parts to the organization of parts, recognizing interaction of the parts are not  static and constant, but dynamic processes\".\n\nData Mesh Governance must see the mesh more than the sum of its parts and as a collection of interconnected systems - [Data Product](thoughts/data/Data%20Product.md),  [Data Product](thoughts/data/Data%20Product.md) providers,  [Data Product](thoughts/data/Data%20Product.md) consumers, and \n[platform teams and services](thoughts/data/Self-Serve%20Data%20Platform.md).\n\n![](thoughts/data/img/data_mesh_characteristics_when_applying_systems_thinking.png)\n\n##### Maintain dynamic equilibrium between domain autonomy and global interoperability\n\nOne of the main objectives of [Data Mesh](thoughts/data/Data%20Mesh.md) governance, is to balance the state of the mesh between decentralization of domains and their autonomy with global interoperability and just-enough mesh-level harmony.\n\n\u003e [!note]\n\u003e Note\n\u003e \n\u003e While the governance model respectes the local control and autonomy of each domain, accountable for the:\n\u003e - quality and integrity\n\u003e - to share\n\u003e their data with the rest of the ecosystem\n\u003e It must be balance that with a global-level:\n\u003e - security\n\u003e - legal conformance\n\u003e - interoperability standards\n\u003e - and other mesh-level policies\n\u003e applied to all [Data Product](thoughts/data/Data%20Product.md)\n\n\u003e [!example]\n\u003e Examples\n\u003e \n\u003e Governance model needs to balance the equilibrium between allowing each domain to define the model and schema of their own [Data Product](thoughts/data/Data%20Product.md) with autonomy - while making sure the [Data Product](thoughts/data/Data%20Product.md) model is standardized enough to be joined and stitched with other [Domain's data product](thoughts/data/Domain%20ownership.md)\n\u003e \n\u003e Allowing each [Domain data product owner](thoughts/data/Domain%20ownership.md) to be accountable for the [security](thoughts/data/Data%20Product%20-%20Secure.md) of their [Data Product](thoughts/data/Data%20Product.md) while making sure all  [Data Products](thoughts/data/Data%20Product.md) are consistently and reliably secure\n\n##### Introduce feedback loop\n\n\u003e [!example]-\n\u003e *Preventing  [Data Product](thoughts/data/Data%20Product.md) duplication and redundant work*\n\u003e \n\u003eBasically controlling the chaos that may arise from each [Domain team](thoughts/data/Domain%20ownership.md) making independent decisions and creating duplicate  [Data Products](thoughts/data/Data%20Product.md) \n\u003e \n\u003e **Traditionally:** Governance control structures that qualify and certify that the data is not a duplicate before it can be used. Despite best intentions, this creates a **bottleneck** that simply doesn't scale out in a complex system.\n\u003e \n\u003e **Data mesh:** introduce two feedback loop to get the same outcome without creating bottlenecks. Every [Data Product](thoughts/data/Data%20Product.md) on the mesh, with the help of the [platform](thoughts/data/Self-Serve%20Data%20Platform.md), is equipped with self-registration, observability, and  [discoverability](thoughts/data/Data%20Product%20-%20Discoverable.md) capabilities. Hence, all [Data Products](thoughts/data/Data%20Product.md) are known to the users of the mesh from the moment of creation throughout their lifetime until they retire.\n\u003e Now imagine the kind of observability and discovery information that each [Data Products](thoughts/data/Data%20Product.md) supplies - their [semantic](thoughts/data/Semantic%20model.md), [syntax](thoughts/data/Syntax%20model.md), the user's satisfaction rating, quality metrics, timeless, completeness, and retention measures.\n\u003e The mesh can use this information to identify duplicate [Data Products](thoughts/data/Data%20Product.md) and provide additional insights such as identifying groups of [Data Products](thoughts/data/Data%20Product.md)  that serve similar information, and comparison of these [Data Products](thoughts/data/Data%20Product.md) based on their satisfaction rating, number of users, completeness, etc.\n\u003e\n\u003e \u003e [!hint]- \n\u003e \u003e negative or balancing feedback loop\n\u003e \u003e \n\u003e \u003e The  [platform - Search and Discovery feature](thoughts/data/Platform%20Feature%20-%20Search%20and%20Discovery.md) can give lower visibility to the duplicate [Data Products](thoughts/data/Data%20Product.md) that don't have high ratings =\u003e gradually degraded on the mesh and hence less used.\n\u003e \u003e  [Platform](thoughts/data/Self-Serve%20Data%20Platform.md) can inform the [Data Product](thoughts/data/Data%20Product.md) owners of the state of their [Data Products](thoughts/data/Data%20Product.md)  in favor of others.\n\u003e \n\u003e \u003e[!hint]-\n\u003e \u003ePositive feedback loop\n\u003e \u003e\n\u003e \u003eUses the same information to promote high-quality and highly usable [Data Products](thoughts/data/Data%20Product.md) with happy users. This mesh discovery function gives higher search ranking to the [Data Products](thoughts/data/Data%20Product.md).\n\n![](thoughts/data/img/feedback_loops_to_maintain_a_state_of%20dynamic_equilibrium.png)\n##### Introduce leverage points\nThe more successful a [Data Product](thoughts/data/Data%20Product.md) is, the more opportunity for usage it gets, and as result it becomes more successful. If the feedback loop is left unchecked, over time we might see undesired side effects, successful [Data Products](thoughts/data/Data%20Product.md) becomes:\n- bloated,\n- slow,\n- fragile to change\n- a god-like dependency for many other downstream [Data Products](thoughts/data/Data%20Product.md).\n- slow down progress of the mesh as a whole\nThis where parameters and measurements can be used as a short-term and quick leverage points and change behavior fast.\n\nIn this case, the mesh governance can place upper bounds on the *lead time to change a [Data Product](thoughts/data/Data%20Product.md)* to detect [Data Products](thoughts/data/Data%20Product.md) that are hard to change, or detect an uptick in change *fail ratio* of more fragile products, and that combined with the number of users and downstream dependencies can identify bloated [Data Products](thoughts/data/Data%20Product.md) that need to change.\n\nAnother type of leverage point is the *goal of the system*: Everything in the system follows a goal that is articulated clearly, repeated. measured, and insisted upon.\n\n\u003e [!danger]\n\u003e  Goal in the wrong direction\n\u003e  \n\u003e  `The number of data products`: Overproduction of data products early on leads to a higher cost of exploration, and in fact it becomes an inhibitor to an exploration of what works best.\n\n\u003e [!check] \n\u003e Goal in the right direction\n\u003e \n\u003e `rate of new data products`\n\n##### Utilize automation and the distributed architecture\n\nThe execution of governance must fit in a distributed architecture, operating in a peer-to-peer mode, instead of relying in a single point of control.\n\n#### Apply federation to the governance model\nThe [Domains](thoughts/data/Domain%20ownership.md) control and own their [Data Products](thoughts/data/Data%20Product.md). They control how their [Data Products](thoughts/data/Data%20Product.md) are modeled and served. The domains select what [Service-level objectives](thoughts/data/Service-level%20objectives.md) their [Data Products](thoughts/data/Data%20Product.md) guarantee and ultimately they are responsible for the satisfaction of their [Data Products](thoughts/data/Data%20Product.md)' consumers?\n\nDespite the autonomy of the domains, there are a set of standards and global policies that all domains must adhere to as a prerequisite to be a member of the mesh - a functioning ecosystem.\n\nFor the federated group to manage its operations, it defines the following operating elements:\n![](thoughts/data/img/federated_computational_governance_operating_model.png)\n##### Federated team\nThe governance is composed of cross-functional team of representatives from domains, as well as [Platform experts](thoughts/data/Self-Serve%20Data%20Platform.md) and subject matters experts from security, compliance, legal, etc.\nIt take the responsibility of:\n- Deciding what policies must be implemented by all [Data Products](thoughts/data/Data%20Product.md).\n- How the platform must support these policies computationally\n- How [Data Products](thoughts/data/Data%20Product.md) adopt the policies\n###### Domain representatives\n[Data Products](thoughts/data/Data%20Product.md) owners are the long-term owners of the domain's [Data Products](thoughts/data/Data%20Product.md). \nEarly buy-in and contribution of domains to define the global policies is crucial in adoption of them\n\nTheir role is different from what is traditionally known as *data stewards* as the main accountability and responsibility lives within a business-alignes-tech domain.\n\nTheir role as a member of the governance is contributing to the definition of plicies that govern the [Data Products](thoughts/data/Data%20Product.md) on the mesh.\n\n###### Data platform representatives\nNearly all governance decisions rely on platform automation. The automation may be in the form of enablement, monitoring, or recovery\n\n\u003e [!example]\n\u003e Example\n\u003e \n\u003e The platform can automate de-identification of personally identifiable information (PII) during write/read of data, or implement standardized APIs for access each [Data Products](thoughts/data/Data%20Product.md) discoverability information\n\n###### Subject matters experts\nLegal teams often are the source of the latest data privacy regulations\n\n###### Facilitators and managers\nBringing a group of people from different disciplines and scope of influence with somewhat competing priorities is no easy task. It requires allocated management and administrative roles to facilitate and support the process of governance under the federated and computational model.\n\n##### Guiding values\n\n\u003e [!hint]\n\u003e Localize decisions and responsibility close to the source\n\u003e \n\u003e Data mesh gives the ownership of the decision and accountability for execution to the people who have the most relevant knowledge and scope of influence.\n\n\u003e [!hint]\n\u003e Identify cross-cutting concerns that need a global standard\n\u003e \n\u003e Mesh, by default, assumes that decisions should be made at the most local level, there are often cross-cutting aspects of [Data Products](thoughts/data/Data%20Product.md) that require global standards. Cross cutting concerns include ensuring confidentiality compliance with regulations, access control, and security.\n\u003e \u003e [!example]\n\u003e \u003e Example\n\u003e \u003e \n\u003e \u003e The execution of \"the right to be forgotten\" according to GDPR regulation is a cross-cutting concern applied to all [Data Products](thoughts/data/Data%20Product.md). How and when such an administrative function is triggered, and how each [Data Products](thoughts/data/Data%20Product.md) assures the execution of this function, is a global concern as it applies equally to all [Data Products](thoughts/data/Data%20Product.md)\n\n\u003e [!hint]\n\u003e Globalize decisions that facilitate interoperability\n\u003e \n\u003e To perform the query, the [Data Products](thoughts/data/Data%20Product.md) must standardize on a few things:\n\u003e - Inclusion of the [temporal dimensions](thoughts/data/temporal%20dimensions.md) in the data\n\u003e - Standardization on the data and time representation, e.g. using the [ISO - ISO 8601 ‚Äî Date and time format](https://www.iso.org/iso-8601-date-and-time-format.html)\n\u003e - Standardization of how the time dimension is represented in the query, e.g. [ISO/IEC 9075-1:2011 - Information technology ‚Äî Database languages ‚Äî SQL ‚Äî Part 1: Framework (SQL/Framework)](https://www.iso.org/standard/53681.html). Defining a consistent way for encoding and querying temporal data allows users to run time-sensitive queries and data processing across disparate [Data Products](thoughts/data/Data%20Product.md)\n\n\u003e [!hint]\n\u003e Identify consistent experiences that need a global standard\n\u003e \n\u003e To run an experiment, the [Data scientist](thoughts/data/Data%20scientist.md) first uses the [Platform Feature - Search and Discovery](thoughts/data/Platform%20Feature%20-%20Search%20and%20Discovery.md) to local the [Data Product](thoughts/data/Data%20Product.md). \n\u003e Then, he deep dives into each of the [Data Products](thoughts/data/Data%20Product.md) to learn about them further, reading documentation, schemas, and other metadata.\n\u003e \n\u003e To provide a consistent experience of understanding [Data Products](thoughts/data/Data%20Product.md) across the mesh, the decision how [Data Products](thoughts/data/Data%20Product.md) encode and share their [semantic](thoughts/data/Semantic%20model.md) and [syntax schema](thoughts/data/Syntax%20model.md) becomes a global concern. This decision will be automated by the [platform](thoughts/data/Self-Serve%20Data%20Platform.md) to provide a set of tools to create, verify, and share [Data Products](thoughts/data/Data%20Product.md) schemas.\n\n\u003e [!hint]\n\u003e Execute decision locally\n\u003e \n\u003e Consider the global decision about how access to [Data Products](thoughts/data/Data%20Product.md) is granted, revoked, and verified. While the decision on how we configure and apply access control is made globally, the configuration of the access control policies is encoded for each [Data Product](thoughts/data/Data%20Product.md) and evaluated and executed at the time of access to a [Data Product](thoughts/data/Data%20Product.md).\n\n##### Policies\nThe output of a system of governance can be distilled to the definition of a set of guides or rules: [computational policies](thoughts/data/computational%20policies.md).\n\n##### Incentives\nThe incentive structure creates two leverage points:\n- **Global incentives**: that encourage building a richly interconnected mesh of [Data Products](thoughts/data/Data%20Product.md) and not silos\n- **Local incentives**: that encourage speed and autonomy of individuals domains.\n\n###### Introduce local incentives\nLocally within domains,  [Data Products](thoughts/data/Data%20Product.md)  owners measure their success based on the satisfaction and growth of their [Data Product](thoughts/data/Data%20Product.md) users.\n\n###### Introduce global incentives\nThe governance is introducing a new set of quality metrics that each [Data Products](thoughts/data/Data%20Product.md) must report through a consistent set of APIs. This a [global policy](thoughts/data/computational%20policies.md) that must apply to all [Data Products](thoughts/data/Data%20Product.md) to achieve a mesh-level observability. However, from the perspective of the domain's product owners conforming to this policy and implementing the data quality reporting APIs, it is somewhat in competition with their domain priorities, building richer and featureful [Data Products](thoughts/data/Data%20Product.md).\n\nTo resolve this conflict and encourage the domains to participate in [global policies](thoughts/data/computational%20policies.md), the incentives of [Data Product](thoughts/data/Data%20Product.md) owners must be augmented. In addition to their local incentives, they need to be rewarded and motivated by the degree of adoption of global policies.\n\n#### Apply computation to the Governance model\nThe [Data Mesh Platform](thoughts/data/Self-Serve%20Data%20Platform.md) is able to embed the execution of these [computational policies](thoughts/data/computational%20policies.md) into each and every [Data Product](thoughts/data/Data%20Product.md) without friction.\n\nIn the operational plane, a similar computational governance has recently been demonstrated by service mesh platforms. \n\n\u003e [!example]\n\u003e Example\n\u003e \n\u003e [Istio](https://istio.io/), an open source implementation of a service mesh, embeds the configuration of traffic routing policies in each endpoint of every single service and executes them locally right at the time of making a request?\n\n##### Standard as code\n\n\u003e [!important]\n\u003e Data product discovery and observability interfaces\n\u003e \n\u003e APIs that expose [discoverability](thoughts/data/Data%20Product%20-%20Discoverable.md) information, documentation, schema, and [Service-level objectives](thoughts/data/Service-level%20objectives.md)\n\n\u003e [!important]\n\u003e Data product data interfaces\n\u003e \n\u003e APIs that expose the data\n\n\u003e [!important]\n\u003e Data and query modeling language\n\u003e \n\u003e [Modeling of semantics](thoughts/data/Semantic%20model.md) and [syntax](thoughts/data/Syntax%20model.md) of data and the query language operating on the data.\n\n\u003e [!important] \n\u003e Lineage modeling\n\u003e \n\u003e Modeling of the traces of data flows and operations across connected [Data Products](thoughts/data/Data%20Product.md).\n\n\u003e [!important]\n\u003e  [Polysemes](thoughts/data/polysemes.md) identification modeling\n\u003e \n\u003e Modeling of identity systems that globally identify and address common business concepts across different [Data Products](thoughts/data/Data%20Product.md)\n\n##### Policies as Code\nAll [Data Products](thoughts/data/Data%20Product.md) must implement [global policies](thoughts/data/computational%20policies.md) such as compliance, access control, access audit, and privacy. \n##### Automated tests\n\n##### Automated monitoring\n\n#### Transition to Federated Computational Governance\n##### Delegate Accountability to Domain\nGovernance of the [Data Mesh](thoughts/data/Data%20Mesh.md) calls for a federated team of [Domain product owners](thoughts/data/Domain%20ownership.md), subject matter experts, and central facilitators. Moving from a central custodianship of the data to a federated model of [Data Product ownership](thoughts/data/Data%20Product.md) requires establishing a new accountability structure led by the domains. This can be achieved by moving some of the existing data stewards into the business-tech-aligned with the new role of [Data Product owner](thoughts/data/Data%20Product.md).\n\n##### Embed Policy execution in Each Data Product\n\n##### Automate Enablement and monitoring over interventions\n[Data Mesh](thoughts/data/Data%20Mesh.md) governance shifts these controls left to the source domain of the data. It favors enabling *doing the right thing* early, through seamless automation, over methods of intervening. This changes the nature of the tools from policing to enabling, from fixing problems late to detecting and recovering the issues as early as possible.\n\n##### Model the Gap\n[Data Mesh](thoughts/data/Data%20Mesh.md) leaves the modeling of data to the domains, the people closest to the data. However, in order to get interoperability and linkage between data across domains, there are data entities in each domain that need to be modeled in a consistent fashion across all domains. Such entities are called [polysemes](thoughts/data/polysemes.md). Standardizing how [polysemes](thoughts/data/polysemes.md) are modeled, identifies, and mapped across domains in a global governance function\n\n##### Measure the Network effect\nThere is no direct and reliable link from the volume to the value. [Data Mesh](thoughts/data/Data%20Mesh.md) introduces a new way for the governance to demonstrate success, based on the usage of data. The stronger the interoperability of the mesh and the trust in the data, the larger the number of interconnections between nodes - consumer and providers - on the mesh.\n\n##### Embrace Changer over Consistency\n[Data Mesh](thoughts/data/Data%20Mesh.md) governance practices must embrace constant change: change from the continuous arrival of fresh data, change of data models, rapide change in use cases and users of the data, new [Data Products](thoughts/data/Data%20Product.md) being created, and old [Data Products](thoughts/data/Data%20Product.md) being retired.\n\n\n## Sources\n- [Data Governance is broken. 90% of governance initiatives are‚Ä¶ | by P Platter | Aug, 2023 | Medium](https://p-platter.medium.com/data-governance-is-broken-c11b592bc391)","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Operational-Data":{"title":"Operational Data","content":"\n## In Nutshel\nOperational data supports running the business and keeps the current state of the business with transactional integrity. This data is captured, stored, and processed by transactions in real time, by OLTP (Online Transaction Processing) systems.\n\nOperational data is referred to as \"data on the inside\". It is the private data of an application or a microservice that perofms CRUD (Create, Update, Delete) operations on it.\n\n\u003e [!summary] \n\u003e Summary\n\u003e \n\u003e This is the historical, integrated, and aggregate view of data created as the byproduct of running the business.\n","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Platform-Feature-Search-and-Discovery":{"title":"Platform Feature - Search and Discovery","content":"","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Self-Serve-Data-Platform":{"title":"Self-Serve Data Platform","content":"\n## In Nutshell\nIt is a new generation of self-serve data platform services that empower domains‚Äô cross-functional teams to share data. The platform services are centered around removing friction from the end-to-end journey of data sharing, from source to consumption. The platform services manage the full life cycle of individual data products.\n\n## Motivations\n- Reduce the total cost of decentralized ownership of data.\n- Abstract data management complexity and reduce the cognitive load of domain teams in managing the end-to-end life cycle of their data products.\n- Mobilize a larger population of developers\n- Automate governance policies to create security and compliance standards for all data products?\n\n## Specification\n\nIt is not that there is any shortage of data and analytics platforms, but we need to make changes to them so they can scale out sharing, accessing, and using analytical data, in a decentralized manner, for a new population of generalist technologists.\n\nThe platform is built and maintained by a dedicated Platform team.\n![](thoughts/data/img/data_domain_agnostic_infrastructure.png)\n\n### Data mesh platform: Compare and contrast\nSmall sample of the existing platform capabilities:\n- `Analytical data storage`: in the form of a lake, warehouse or lakehouse\n- `Data processing frameworks` and computation engines to process data in batch and streaming modes\n- `Data querying languages`, based on two modes of computational data flow programming or algebraic SQL-like statements\n- `Data catalog` solutions to enable data governance as well as discovery of all data across an organization\n- `Pipeline workflow management`, orchestrating complex data pipeline task or ML model deployment workflows.\n![](thoughts/data/img/data_mesh_platofrm_diff_characterisitcs.png)\n\n### Serving Autonomous Domain-Oriented Teams\n\nThe main responsibility of the data mesh platform is to enable existing or new domain engineering teams with the new and embedded responsibilities of building, sharing, and using [Data Product](thoughts/data/Data%20Product.md) end to end:\n- Capturing data from [Operational systems](thoughts/data/Operational%20Data.md) and other sources\n- Transforming and sharing the data as a product with the end to end users\nThe platform must allow teams to do this in an **autonomous** way without any dependence on centralized data teams or intermediates.\n\n\u003e [!note]-\n\u003e Note\n\u003e \n\u003e Many existing vendor technologies are built with an assumption of centralized team, capturing and sharing data for all domains. This have deep technological consequences such as:\n\u003e - Cost is estimated and managed monolithically and not per isolated domain resources\n\u003e - Security and privacy management assumes physical resources are shared under the same account and don't scale to an isolated security context per [Data Product](thoughts/data/Data%20Product.md)\n\u003e - A central pipeline (DAG) orchestration assumes management of all data pipelines centrally - with a centrale pipeline configuration repository and a central monitoring portal.\n\n### Managing Autonomous and interoperable Data Products\n\n[Data Mesh](thoughts/data/Data%20Mesh.md) puts a new construct, a [domain-oriented Data Product](thoughts/data/Data%20Product.md) at the center of its approach. It encodes all the behavior and data needed to provide [discoverable](thoughts/data/Data%20Product%20-%20Discoverable.md), usable, [trustworthy](thoughts/data/Data%20Product%20-%20Trustworthy%20\u0026%20truthful.md), and [secure](thoughts/data/Data%20Product%20-%20Secure.md) data to its and data users.\n\n[Data Products](thoughts/data/Data%20Product.md) share data with each other and are interconnected in a mesh.\n\n### A continuous Platform of Operational and Analytical Capabilities\n\nThe principle of [Domain ownership](thoughts/data/Domain%20ownership.md) demands a platform that enables autonomous domain teams data end to end. Whether the team is building and running an application or sharing data using [Data Products](thoughts/data/Data%20Product.md) for analytical use cases, the team's experience should be connected and seamless.\n\n\u003e [!important] \n\u003e For example\n\u003e \n\u003e Today the computation fabric running data processing pipelines such as Spark are managed on a different clustering architecture, away often disconnected from the computation fabric that runs [Operational services](thoughts/data/Operational%20Data.md), such as [Kubernetes](thoughts/Kubernetes.md). In order to create [Data Products](thoughts/data/Data%20Product.md) that collaborate closely with their corresponding microservice, i.e [source-aligned domain data](thoughts/data/source-aligned%20domain%20data.md), we need a closer integration of the computation fabircs\n\nThe [Operational systems](thoughts/data/Operational%20Data.md) use the [OpenTelemetry](https://opentelemetry.io/) standards for tracing of (API) calls across distributed applications, in a tree-like structure. On the other hands, data processing workloads use [Home | OpenLineage](https://openlineage.io/) to trace the lineage of data across distributed data pipelines.\n\n### Designed for a Generalist Majority\nAnother barrier to the adoption of data platforms today is the level of proprietary specialization that each technology vendor assumes - the jargon and the vendor specific knowledges. This has led to the creation of scarce specialized roles such as data engineers.\n\nThere are a few reasons for this unscalable specialization:\n- lack of (de facto) standards and conventions,\n- lack of incentives for technology interoperability\n- lack of incentive to make products super simple to use\n\n\u003e [!hint]\n\u003e Hint\n\u003e \n\u003e A data mesh platform must break this pattern and start with the definition of a set of open conventions that promote [interoperability](thoughts/data/Data%20Product%20-%20Interoperable.md) between different technologies and reduce the number of proprietary language and experience one specialist must learn to generate from data.\n\nThis should be achieved without compromising on the software engineering practices that result in sustainable solutions. For example, many low-code or no code platforms promise to work with data, but compromise on testing, versioning, modularity, and other techniques. Over time they become unmaintainable.\n\n### Favoring Decentralized Technologies\nAnother common characteristic of existing platforms, is the centralization of control:\n- Centralized pipeline orchestration tools\n- Centralized catalogs\n- Centralized allocation of compute/storage resources\n- etc.\n\nThere are many aspects of infrastructure that need to ne centrally managed to reduce the unnecessary tasks tach each domain team performs in sharing and using data, e.g setting up data processing compute clusters.\n\nThis is where an effective self-serve platform shines, centrally managing underlying resources while allowing independent teams to achieve their outcomes end to end, without tight dependencies to other teams.\n\n### Domain Agnostic\n\nA digital platform is a foundation of self-service APis, tools, services, knowledge and support which are arranged as a compelling internal product. Autonomous delivery teams can make use of the platform to deliver product features at a higher pace, with reduced coordination.\n\n","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Semantic-model":{"title":"Semantic model","content":"\nMachine and human-readable model definition that captures the _domain model_ of the data: How the data product models the domain, what types of entities the data includes, the properties of the entities, the relationships between the entities, etc.  ","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Service-level-objectives":{"title":"Service-level objectives","content":"## SLO list\n\n### Data quality metrics\n\n\u003e [!important]-\n\u003e Accuracy\n\u003e \n\u003e The degree of how closely the data represents the true value of the attribute in the real-world context\n\n\u003e [!important]-\n\u003e Completeness\n\u003e \n\u003e The degree of data representing all properties and instances of the real-world context\n\t\t\n\u003e [!important]-\n\u003e Consistency\n\u003e \n\u003e The degree of data being free of contradictions\n\n\u003e [!important]- \n\u003e Precision\n\u003e \n\u003e The degree of attribute fidelity\n\n### Data maturity metrics\n\n\u003e [!example]-\n\u003e Degree of usage\n\u003e \n\u003e How widely the data is being used \n\n\u003e [!example]- \n\u003e Life cycle\n\u003e \n\u003e Whether the data product under development and actively evolving and being optimized or dormant)\n\n\u003e [!example]-\n\u003e Diveristy\n\u003e \n\u003e Number of modes of access and use cases they supportHow widely the data is being used \n\n\u003e [!example]-\n\u003e Linkage\n\u003e \n\u003e Degree of linking to other data products in the form of reusing types and data\n\n### Temporality metrics\n\n\u003e [!warning]-\n\u003e Epoch (actual and processing)\n\u003e \n\u003e The ealiest actual and processing times for which data is available. It illustrates the data product's duration of data retention\n\n\u003e [!warning]-\n\u003e Processing interval\n\u003e \n\u003e How often the data is processed, if there is such a pattern. In the absence of a specific interval, data products can provide statistics mean, max, and min intervals\n\n\u003e [!warning]-\n\u003e Last processing time\n\u003e \n\u003e Most recent data's processing time\n\n\u003e [!warning]-\n\u003eLast actual time (window)\n\u003e \n\u003e Most recent data's actual time (or window)\n\n\u003e [!warning]-\n\u003e Actual window\n\u003e \n\u003e The window of actual times over which input data is processed and new data is created, if the data is aggregated over windows of time\n\n\u003e [!warning]-\n\u003e Timeliness (or skew)\n\u003e \n\u003e The degree to which the actual time an processing time are separated.\n\n### User-driven metrics\nData users often trust based on the experience of the fellow data users using the data. Hence the [Data Product](thoughts/data/Data%20Product.md) must capture and present the perception and experience of its consumers as a set of quality metrics (e.g. Recognition system such as allocating stars that data users can assign to data products based on their experience).\n\n## Sources\n- Establishing a consistent set for trust metrics:\n\t- [Data on the Web Best Practices: Data Quality Vocabulary](https://www.w3.org/TR/vocab-dqv/)\n\t- [Data Catalog Vocabulary (DCAT) - Version 2](https://www.w3.org/TR/vocab-dcat/)\n","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/Syntax-model":{"title":"Syntax model","content":"\n[Data Product](thoughts/data/Data%20Product.md) encodes its semantic according to the access models supported by its [output ports](thoughts/data/output%20ports.md). This the syntax model. Every [output port](thoughts/data/output%20ports.md) will model have its own syntax model. \n\nFor example:\nA playlist file [output ports](thoughts/data/output%20ports.md) will model the playlists as a nested table, stored as columnar files and defined according to the [JSON Schema](https://json-schema.org/). This modeling is optimized for machine learning training using features (columns across all records).","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/aggregate-domain-data":{"title":"Aggregate domain data","content":"\n## In Nutshel\n\nAnalytical data that is an aggregate of multiple upstream domains.\n\n## Specification\n\nThere is never a one-to-one mapping between a core concept of a business and a source system at an entreprise scale. Hence the might be a lot of [source-aligned data](thoughts/data/source-aligned%20domain%20data.md) that ultimately needs to be aggregated into a more aggregate form concept.\n\n\u003e [!warning] \n\u003e warning\n\u003e \n\u003e It strongly caution us against creating ambitious aggregate domain data - aggregate domain data that attempt to capture all facets of a particular concept, like `listener 360` , and serve many organization-wide data users.\n\u003e In the pas, the implementation of Master Data Management (MDM) has attempted to aggregate all facets of shared data assets in one place and in one model. This a move back to single monolithic schema that doesn't scale.\n\n","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/computational-policies":{"title":"Computational Policies","content":"\n## In Nutshel\n\nOutput of a system of governance that is distilled to the definition of a set of guides or rules, that specify what good looks like and how to ensure it's maintained. \n\n\u003e [!example]\n\u003e Example\n\u003e \n\u003e What secure data looks like and how security is maintained; the same goes for data accessibility, data quality, modeling data, and many other cross-functional characteristics of data shared on the mesh\n\n## Specification\n### Local policies\n\n\u003e [!example] \n\u003e Example\n\u003e \n\u003e The decision around the timeless of a **play event** is best made and maintained by the **player** team. They know best how soon after an event has occurred it can be reliably shared\n\n### Global Policies\nGlobal policies apply consistently to all [Data Products](thoughts/data/Data%20Product.md). They cover the seams, the interconnectivity, and the gap between the  [Data Products](thoughts/data/Data%20Product.md).\n\n\u003e [!example]\n\u003e Example\n\u003e \n\u003e The decision of *data ownership*: which team shall own a new [Data Product](thoughts/data/Data%20Product.md)\n\u003e \n\u003e In this case, the [Federated Computational Governance](thoughts/data/Federated%20Computational%20Governance.md) team can create as set of heuristic to help make such a decision: either an existing source domain is incentivized or another team is empowered to own it as the primary consumers of it, or a new domain must be formed.\n\nIdeally, we would want to minimize global policies to reduce friction. Implementing and updating global policies that affect the mesh is hard. Limiting their number and scope and relentlessly implementing them through automated [Platform](thoughts/data/Platform%20Feature%20-%20Search%20and%20Discovery.md) capabilities are our only options to make them effective\n\n### Policy as Code\nThe [Platform](thoughts/data/Self-Serve%20Data%20Platform.md) is the key enabler in embedding these policies in all [Data Products](thoughts/data/Data%20Product.md). [Data Products](thoughts/data/Data%20Product.md) can define the *policy configurations as code* and test and execute them during their lifecycle.\n\nThe [Platform](thoughts/data/Self-Serve%20Data%20Platform.md) offers the underlying engine that implements the management of policies as code.\n\n\u003e [!example]\n\u003e Compliance\n\u003e \n\u003e All [Data Products](thoughts/data/Data%20Product.md) must protect PII yet enable ML or analytical workloads that inherently require access to such information across a population.\n\u003e To enable such a policy, [Data Products](thoughts/data/Data%20Product.md) can implement techniques such as [Differential privacy | Technology Radar | Thoughtworks](https://www.thoughtworks.com/radar/techniques/differential-privacy), providing access to anonymized data that sustains the statistical characteristics of the population while refraining access to an individual's PII.\n\n\u003e[!example]\n\u003eData privacy and protection\n\u003e\n\u003eStrategies to prevent data from being stolen, lost, or accidentally deleted. Ensure that sensitive data is accessible only to approved parties\n\n\u003e [!example]\n\u003e Data localization\n\u003e \n\u003e Requirements around geolocation of data storage and its processing\n\n\u003e [!example]\n\u003e Data access control and audit\n\u003e \n\u003e Control who can access what elements of the data and keep track of all accesses\n\n\u003e [!example]\n\u003e Data consent\n\u003e \n\u003e Track and control what information the data owners allow to be preserved and shared\n\n\u003e [!example]\n\u003e Data sovereignty\n\u003e \n\u003e Preserving the ownership of data and its control\n\n\u003e [!example]\n\u003e Data retention\n\u003e \n\u003e Managing the availability and storage of data according to the defined retention duration and policy.\n\n","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/consumer-aligned-domain-data":{"title":"Consumer-aligned domain data","content":"\n## In Nutshel\n\nAnalytical data transformed to fit the needs of one or multiple specific use cases. This is also called `fit-for-purpose` domain data.\n\n## Specification\n\nEngineered features to train machine learning models often fall in the category. \n\nThe archetype has a different nature in comparison to [source-aligned domain data](thoughts/data/source-aligned%20domain%20data.md). It structurally goes through more changes and transforms the source domain events to structures and content that fit a particular use case.\n\nThe notion of the consumer here refers to the applications that consume the data, or data users such as data scientists or analysts.\n\n","lastmodified":"2023-09-06T10:14:15.374662053Z","tags":null},"/thoughts/data/output-ports":{"title":"Output ports","content":"","lastmodified":"2023-09-06T10:14:15.402662376Z","tags":null},"/thoughts/data/polysemes":{"title":"Polysemes","content":"\n## In Nutshel\n\nPolysemes are shared concepts across different domains. They point to the same entity, with domain-specific attributes. Polysemes represent shared core concepts in a business such as `artist`, `listener`, and `song`.\n\nA global identification scheme allows mapping a polyseme from one domain to another.","lastmodified":"2023-09-06T10:14:15.402662376Z","tags":null},"/thoughts/data/source-aligned-domain-data":{"title":"Source-aligned domain data","content":"\n## In Nutshel\n\nAnalytical data reflecting the business facts generated by the operational systems. This is also called a `native data product` .\n\nThat domains are responsible for providing the truths of their business domains as source-aligned domain data.\n\n\u003e [!note]\n\u003e Note\n\u003e \n\u003e Source-alignes analytical data is not modeled or accessed directly from the source application transactional database. Exposing analytical data directly from the [Operational Database](thoughts/data/Operational%20Data.md) is an anti-pattern. This is observed in the implementation of ETLs and application of CDC and data virtualization on top of the application's database.\n\n\n## Specification\n\nIt represents the *facts and reality of the business*.\n\n\u003e [!example]-\n\u003e Example\n\u003e \n\u003e Fact of the business such as : \"How users are interacting with the media players\" or \"how users subscribe\", lead to the creation of source-aligned domain data such as **play events, audio play quality streams, and listener profiles**\n\n[Data Mesh](thoughts/data/Data%20Mesh.md) differentiates between the source application's [Operational Database](thoughts/data/Operational%20Data.md)  and its collaboration [Analytical Data](thoughts/data/Analytical%20Data.md) storage. They are tightly integrated and owned by the same [Domain team](thoughts/data/Domain%20ownership.md).\n\n\u003e [!important] \n\u003e Important\n\u003e \n\u003e Source-aligned domain data closely represents the raw data at the point of creation and is not fitted or modeled for a particular consumer.\n\n\n","lastmodified":"2023-09-06T10:14:15.402662376Z","tags":null},"/thoughts/data/temporal-dimensions":{"title":"Temporal dimensions","content":"This includes the timestamp of the real event or state, and the timestamp when the [Data Product](thoughts/data/Data%20Product.md) become aware of the event (process time).\n\n## Standards\n- Date and time representation: [ISO - ISO 8601 ‚Äî Date and time format](https://www.iso.org/iso-8601-date-and-time-format.html)\n- Time dimension representation in the query: [ISO/IEC 9075-1:2011 - Information technology ‚Äî Database languages ‚Äî SQL ‚Äî Part 1: Framework (SQL/Framework)](https://www.iso.org/standard/53681.html)\n","lastmodified":"2023-09-06T10:14:15.402662376Z","tags":null},"/thoughts/dbt":{"title":"DBT","content":"\n","lastmodified":"2023-09-06T10:14:15.402662376Z","tags":null},"/thoughts/driftctl":{"title":"driftctl","content":"\n`driftctl` is CLI tool that measures infrastructure as code coverage, and track infrastructure drift.\n\n## Features\n- Scan cloud provider and map resources with IaC code\n- Analyze diff, and warn about drift and unwanted unmanaged resources\n- Allow user to ignore resources\n- Multiple output formats\n\n## Installation\n\n```console\n# Linux\n# x64\n$ curl -L https://github.com/snyk/driftctl/releases/latest/download/driftctl_linux_amd64 -o driftctl\n\n# x86\n$ curl -L https://github.com/snyk/driftctl/releases/latest/download/driftctl_linux_386 -o driftctl\n\n# macOS\n$ curl -L https://github.com/snyk/driftctl/releases/latest/download/driftctl_darwin_amd64 -o driftctl\n$ curl -L https://github.com/snyk/driftctl/releases/latest/download/driftctl_darwin_arm64 -o driftctl\n```\n\nMake the binary executable:\n```\n$ chmod +x driftctl\n```\n\nOptionally install driftctl to a central location in your¬†`PATH`:\n\n```\n# use any path that suits you, this is just a standard example. Install sudo if needed.\n$ sudo mv driftctl /usr/local/bin/\n```\n\n### Completion\n```\nmkdir -p ~/.oh-my-zsh/completions\ndriftctl completion zsh \u003e ~/.oh-my-zsh/completions/_driftctl\n```\n\n## Usage\n```console\ndriftctl scan \\\n  --from tfstate+tfcloud://$ORGANIZATION_NAME/$WORKSPACE_NAME \\\n  --tfc-endpoint 'https://tfe.doctolib.net/api/v2' \\\n  --to aws+tf \\\n  --disable-telemetry \\\n  --no-version-check\n```\n","lastmodified":"2023-09-06T10:14:15.402662376Z","tags":null},"/thoughts/eBPF":{"title":"eBPF","content":"\nA Linux kernel bytecode interpreter originally introduced to filter network packets, e.g. tcpdump and socket filters. It has since been extended with additional data structures such as hashtable and arrays as well as additional actions to support packet mangling, forwarding, encapsulation, etc\n\n\n","lastmodified":"2023-09-06T10:14:15.402662376Z","tags":null},"/thoughts/elastic-network-interfaces":{"title":"Elastic Network interfaces","content":"\nAn¬†_elastic network interface_¬†is a logical networking component in a [VPC](thoughts/aws%20vpc.md) that represents a virtual network card. It can include the following attributes:\n- A primary private IPv4 address from the IPv4 address range of your [VPC](thoughts/aws%20vpc.md)\n- One or more secondary private IPv4 addresses from the IPv4 address range of your VPC\n- One Elastic IP address (IPv4) per private IPv4 [VPC](thoughts/aws%20vpc.md)\n- One public IPv4 address\n- One or more IPv6 addresses\n- One or more security groups\n- A MAC address\n- A source/destination check flag\n- A description\n\nWe can create and configure network interfaces and attach them to instances in the same Availability Zone. The account might also have¬†_requester-managed_¬†network interfaces, which are created and managed by AWS services to enable us to use other resources and services. We cannot manage these network interfaces ourself.","lastmodified":"2023-09-06T10:14:15.402662376Z","tags":null},"/thoughts/github-actions":{"title":"Github Actions","content":"[how to access private repositories from GA](thoughts/how%20to%20access%20private%20repositories%20from%20GA.md)","lastmodified":"2023-09-06T10:14:15.402662376Z","tags":null},"/thoughts/how-to-access-private-repositories-from-GA":{"title":"How to access private repositories from Github Actions?","content":"\nWhen running in [Github Actions](thoughts/github%20actions.md), the private repositories can not be accessed since the runners is not configured to access the private repositories.\n\n## The solution\n\nSince the private repositories that my app depending on are mine as well, therefore I can add the required configuration into those private repositories with the action [ssh-agent](https://github.com/webfactory/ssh-agent)\n\nThe following steps are required to setup the SSH keys for the runner during runtime to access the private repositories:\n\n- Create SSH key for the private repositories by running¬† `ssh-keygen -C \"git@github.com:erdrix/digital-garden-git.git\"`¬†.\n- Put the public key as deployment key into the private repository.\n- Put the private key as a secret into the main repository.\n- Change your action a bit\n\n```yaml\n# Make sure the @v0.7.0 matches the current version of the action\n- uses: webfactory/ssh-agent@v0.7.0\n  with:\n    ssh-private-key: ${{ secrets.SSH_PRIVATE_KEY_CONTENT }}\n\n- uses: actions/checkout@v3\n  with:\n    submodules: recursive # this is important not to forget\n```\n\n","lastmodified":"2023-09-06T10:14:15.402662376Z","tags":null},"/thoughts/kubernetes-operator":{"title":"Kubernetes Operator","content":"","lastmodified":"2023-09-06T10:14:15.406662422Z","tags":null},"/thoughts/osi-model-layers":{"title":"OSI model layers","content":"\nOSI stands for Open Systems Interconnection. It is a 7-layer architecture with each layer having specific functionality to perform. All these 7 layers work collaboratively to transmit the data from one perso to another across the globe.\n\n![OSI model layers](thoughts/images/osi_model_layers.png)\n\n- **\\[L3\\] Network Layer:** handles addressing, routing, and packet transmission between different networks. It assigns IP addresses, routes data packets, and ensures efficient communication across networks. _For Kubernetes it is translated by the fact that we assign unique IP addresses to pods and enables communication between them within a cluster.\n- **\\[L4\\] Transport Layer:** responsible for managing reliable and efficient communication between devices. It uses protocols like TCP for reliable and ordered data transfer, and UDP for fast, connectionless communication. It handles data segmentation, encapsulation, and ensures reliable delivery when needed. _For Kubernetes it manages communication between pods using protocols and ports to each pod.\n- **\\[L7\\] Application Layer:** focuses on application-level interactions. It handles application-specific protocols, performs tasks like protocol identification, message segmentation, encryption, routing, load balancing, caching, and content delivery. _For Kubernetes it provides service discovery, load balancing, routing, API management._\n\n## Links\n\n\n","lastmodified":"2023-09-06T10:14:15.406662422Z","tags":null},"/thoughts/projects":{"title":"Projects","content":"\n# NiFiKop - A Kubernetes operator for Apache NiFi\n[NiFiKop](thoughts/NiFiKop.md), is an open-source Kubernetes operator that makes it¬†easy¬†to run Apache NiFi on Kubernetes. \n\n[Github](https://github.com/konpyutaika/nifikop), [Documentation](https://konpyutaika.github.io/nifikop/)","lastmodified":"2023-09-06T10:14:15.406662422Z","tags":null},"/thoughts/security-group":{"title":"Security group","content":"\nA security group acts as a firewall that controls the traffic allowed to and from the resources in your [virtual private cloud (VPC)](thoughts/aws%20vpc.md). You can choose the ports and protocols to allow for inbound traffic and for outbound traffic.\n\nFor each security group, you add separate sets of rules for inbound traffic and outbound traffic.","lastmodified":"2023-09-06T10:14:15.406662422Z","tags":null}}